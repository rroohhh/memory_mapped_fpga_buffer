
@INPROCEEDINGS{aamir16dlsneuron,
  author = {Aamir, Syed Ahmed and M{\"u}ller, Paul and Hartel, Andreas and Schemmel, Johannes and Meier, Karlheinz},
  title = {A highly tunable 65-nm {CMOS} {LIF} neuron for a large-scale neuromorphic system},
  booktitle = {Proceedings of IEEE European Solid-State Circuits Conference (ESSCIRC)},
  year = {2016},
  doi = {10.1109/ESSCIRC.2016.7598245},
}

@ARTICLE{aamir2018dls2neuron,
  author={Aamir, Syed Ahmed and Stradmann, Yannik and M\"uller, Paul and Pehle, Christian and Hartel, Andreas and Gr\"ubl, Andreas and Schemmel, Johannes and Meier, Karlheinz},
  journal={IEEE Transactions on Circuits and Systems I: Regular Papers},
  title={An Accelerated {LIF} Neuronal Network Array for a Large-Scale Mixed-Signal Neuromorphic Architecture},
  year={2018},
  volume={65},
  number={12},
  pages={4299-4312},
  abstract={We present an array of leaky integrate-and-fire ({LIF}) neuron circuits designed for the second-generation {BrainScaleS} mixed-signal 65-nm {CMOS} neuromorphic hardware. The neuronal array is embedded in the analog network core of a scaled-down prototype high input count analog neural network with digital learning system chip. Designed as continuous-time circuits, the neurons are highly tunable and reconfigurable elements with accelerated dynamics. Each neuron integrates input current from a multitude of incoming synapses and evokes a digital spike event output. The circuit offers a wide tuning range for synaptic and membrane time constants, as well as for refractory periods to cover a number of computational models. We elucidate our design methodology, underlying circuit design, calibration, and measurement results from individual sub-circuits across multiple dies. The circuit dynamics matches with the behavior of the {LIF} mathematical model. We further demonstrate a winner-take-all network on the prototype chip as a typical element of cortical processing.},
  keywords={CMOS integrated circuits;integrated circuit design;integrated circuit modelling;mixed analogue-digital integrated circuits;neural chips;neural net architecture;continuous-time circuits;accelerated dynamics;input current;digital spike event output;membrane time constants;design methodology;circuit design;circuit dynamics;LIF mathematical model;winner-take-all network;prototype chip;accelerated LIF neuronal network array;mixed-signal neuromorphic architecture;neuronal array;analog network core;prototype high input count analog neural network;digital learning system chip;second-generation BrainScaleS mixed-signal CMOS neuromorphic hardware;leaky integrate-and-fire neuron circuit design;high input count analog neural network;cortical processing;refractory periods;multiple dies;size 65 nm;Neurons;Synapses;Computational modeling;Biological system modeling;Computer architecture;Integrated circuit modeling;Neuromorphics;Analog integrated circuits;neuromorphic;leaky integrate and fire;65nm CMOS;spiking neuron;OTA;opamp;tunable resistor;winner-take-all network},
  doi={10.1109/TCSI.2018.2840718},
  ISSN={1549-8328},
  month=dec,
}

@ARTICLE{aamir2018mixed, crossref={aamir2018dls3neuron}}
@ARTICLE{aamir2018dls3neuron,
  author={Aamir, Syed Ahmed and M\"uller, Paul and Kiene, Gerd and Kriener, Laura and Stradmann, Yannik and Gr\"ubl, Andreas and Schemmel, Johannes and Meier, Karlheinz},
  journal={IEEE Transactions on Biomedical Circuits and Systems},
  title={A Mixed-Signal Structured {AdEx} Neuron for Accelerated Neuromorphic Cores},
  year={2018},
  volume={12},
  number={5},
  pages={1027-1037},
  abstract={Here, we describe a multicompartment neuron circuit based on the adaptive-exponential {I&F} ({AdEx}) model, developed for the second-generation {BrainScaleS} hardware. Based on an existing modular leaky integrate-and-fire ({LIF}) architecture designed in 65-nm {CMOS}, the circuit features exponential spike generation, neuronal adaptation, intercompartmental connections as well as a conductance-based reset. The design reproduces a diverse set of firing patterns observed in cortical pyramidal neurons. Further, it enables the emulation of sodium and calcium spikes, as well as {N-methyl-D-aspartate} plateau potentials known from apical and thin dendrites. We characterize the {AdEx} circuit extensions and exemplify how the interplay between passive and nonlinear active signal processing enhances the computational capabilities of single (but structured) on-chip neurons.},
  keywords={bioelectric potentials;calcium;medical signal processing;neural chips;neural nets;neurophysiology;CMOS;on-chip neurons;nonlinear active signal processing;passive signal processing;AdEx circuit extensions;N-methyl-D-aspartate plateau potentials;calcium spikes;sodium;cortical pyramidal neurons;firing patterns;diverse set;conductance-based reset;intercompartmental connections;neuronal adaptation;circuit features exponential spike generation;existing modular leaky;second-generation BrainScaleS hardware;I&F model;multicompartment neuron circuit;accelerated neuromorphic;mixed-signal structured AdEx neuron;Neurons;Biomembranes;Integrated circuit modeling;Biological system modeling;Synapses;Acceleration;Neuromorphics;65 nm CMOS;Analog integrated circuits;adaptation;AdEx;bursting;dendrites;exponential;integrate-and-fire;leaky;LIF;multi-compartment;neuromorphic;neuron;NMDA;spiking},
  doi={10.1109/TBCAS.2018.2848203},
  ISSN={1932-4545},
  month=oct,
}

@INPROCEEDINGS{berge_iscas07,
  author = {Hans Kristian Otnes Berge and Philipp H{\"a}fliger},
  title = {High-Speed Serial {AER} on {FPGA}},
  booktitle = {ISCAS},
  year = {2007},
  pages = {857-860},
  crossref = {iscas2007},
  institute = {Dept. of Informatics, Oslo Univ.}
}

@ARTICLE{abarbanel03biopysical,
  author = {Abarbanel, Henry D. I. and Leif Gibb, R. Huerta and M. I. Rabinovich},
  title = {Biophysical model of synaptic plasticity dynamics},
  journal = {Biol. Cybern.},
  year = {2003},
  volume = {89},
  pages = {214-226},
  number = {3},
  month = {September},
  abstract = {We discuss a biophysical model of synaptic plasticity that provides
  a unified view of the outcomes of synaptic modification protocols,
  including: (1) prescribed time courses of postsynaptic intracellular
  Ca2+ release, (2) postsynaptic voltage clamping with presentation
  of presynaptic spike trains at various frequencies, (3) direct postsynaptic
  response to presynaptic spike trains at various frequencies, and
  (4) LTP/LTD as a response to precisely timed presynaptic and postsynaptic
  spikes.},
  file = {abarbanel03biopysical.pdf:abarbanel03biopysical.pdf:PDF},
  keywords = {plasticity spiking},
  owner = {mreuss},
  url = {http://www.springerlink.com/link.asp?id=c8nrd0wexxnmuvvy}
}

@MISC{abbott-temporally,
  author = {L.F. Abbott and Sen Song},
  title = {Temporally Asymmetric Hebbian Learning, Spike Timing and Neuronal
  Response Variability},
  citeseerurl = {citeseer.nj.nec.com/167264.html},
  keywords = {spiking learning}
}

@ARTICLE{Abbott2000, crossref={abbott2000synaptic}}
@ARTICLE{abbott2000synaptic,
  author = {Abbott, Larry F and Nelson, Sacha B},
  title = {Synaptic plasticity: taming the beast},
  journal = {Nature Neuroscience},
  year = {2000},
  volume = {3},
  pages = {1178--1183},
  owner = {simon},
  publisher = {Nature Publishing Group},
  timestamp = {2013.04.25}
}

@BOOK{abeles1991,
  title = {Corticonics: Neural Circuits of the Cerebral Cortex},
  publisher = {Cambridge University Press},
  year = {1991},
  author = {M. Abeles},
  address = {New York}
}

@ARTICLE{abeles2004modeling,
  author = {Abeles, M. and Hayon, G. and Lehmann, D.},
  title = {Modeling compositionality by dynamic binding of synfire chains},
  journal = {Journal of computational neuroscience},
  year = {2004},
  volume = {17},
  pages = {179--201},
  number = {2},
  publisher = {Springer}
}

@inproceedings{abi2019arbor,
  title={Arbor—a morphologically-detailed neural network simulation library for contemporary high-performance computing architectures},
  author={Abi Akar, Nora and Cumming, Ben and Karakasis, Vasileios and K{\"u}sters, Anne and Klijn, Wouter and Peyser, Alexander and Yates, Stuart},
  booktitle={2019 27th euromicro international conference on parallel, distributed and network-based processing (PDP)},
  pages={274--282},
  year=2019,
  organization={IEEE}
}

@MISC{abrahams03boost,
  author = {Abrahams, D. and Grosse-Kunstleve, R.W.},
  title = {Building Hybrid Systems with {Boost.Python}},
  year = {2003},
  institution = {Boost Consulting},
  url = {http://www.boostpro.com/writing/bpl.pdf}
}

@ARTICLE{abuhassan2013,
  year={2013},
  issn={0929-5313},
  journal={Journal of Computational Neuroscience},
  doi={10.1007/s10827-013-0462-8},
  title={Compensating for synaptic loss in Alzheimer’s disease},
  url={http://dx.doi.org/10.1007/s10827-013-0462-8},
  publisher={Springer US},
  keywords={Alzheimer’s disease; Synaptic loss; Synaptic compensation mechanisms; Electroencephalography (EEG); Large-scale network model; Izhikevich neuron model; Message passing interface},
  author={Abuhassan, Kamal and Coyle, Damien and Belatreche, Ammar and Maguire, Liam},
  pages={1-19},
  language={English}
}

@ARTICLE{ackley85boltzmann,
  author = {Ackley, D. H. and Hinton, G. E. and Sejnowski, T. J.},
  title = {A learning algorithm for {B}oltzmann machines},
  journal = {Cognitive Science},
  year = {1985},
  volume = {9},
  pages = {147--169}
}

@BOOK{adobe1999postscript,
  title = {{P}ost{S}cript Language Reference},
  publisher = {Addison Wesley Professional},
  year = {1999},
  author = {{Adobe Systems}},
  edition = {3rd},
  isbn = {0-201-37922-8}
}

@ARTICLE{adrian1926impulses,
  author = {Adrian, E. D.},
  title = {The impulses produced by sensory nerve endings.},
  journal = {Journal of Physiology},
  year = {1926},
  volume = {61},
  pages = {49-72},
  owner = {bkaplan},
  timestamp = {2008.11.28}
}

@TECHREPORT{aeberhard93statistical,
  author = {Aeberhard, Stefan and Coomans, Danny and de Vel, Olivier},
  title = {The Performance of Statistical Pattern Recognition Methods in High
	Dimensional Settings},
  institution = {James Cook University},
  year = {1993},
  number = {4},
  address = {Australia},
  file = {aeberhard93statistical.pdf:aeberhard93statistical.pdf:PDF}
}

@ARTICLE{aertsen96,
  author = {Aertsen, A. and Diesmann, M. and Gewaltig, M. O.},
  title = {{Propagation of synchronous spiking activity in feedforward neural
	networks.}},
  journal = {J Physiol Paris},
  year = {1996},
  volume = {90},
  pages = {243--247},
  number = {3-4},
  abstract = {{'Synfire' activity has been proposed as a model for the experimentally
	observed accurate spike patterns in cortical activity. We investigated
	the structural and dynamical aspects of this theory. To quantify
	the degree of synchrony in neural activity, we introduced the concept
	of 'pulse packets'. This enabled us to derive a novel neural transmission
	function which was used to assess the role of the single neuron dynamics
	and to characterize the stability conditions for propagating synfire
	activity. Thus, we could demonstrate that the cortical network is
	able to sustain synchronous spiking activity using local feedforward
	(synfire) connections. This new approach opens the way for a quantitative
	description of neural network dynamics, and enables us to test the
	synfire hypothesis on physiological data.}},
  address = {Department of Neurobiology and Biophysics, Albert-Ludwigs-University,
	Freiburg, Germany.},
  issn = {0928-4257},
  keywords = {reseau, spike-frequency, spike-timing, synchronisation},
  pmid = {9116676},
  posted-at = {2007-02-20 15:32:20},
  priority = {2},
  url = {http://view.ncbi.nlm.nih.gov/pubmed/9116676}
}

@MANUAL{xilinx_synthesis,
  title = {Physical Synthesis},
  author = {Agah, H.},
  organization = {Xilinx, Inc},
  address = {www.xilinx.com},
  month = {February},
  year = {2001},
  note = {XAPP140},
  groupsearch = {0},
  key = {xilinx_synthesis}
}

@ARTICLE{agmon1998role,
  author = {Agmon-Snir, H. and Carr, C.E. and Rinzel, J.},
  title = {The role of dendrites in auditory coincidence detection},
  journal = {Nature},
  year = {1998},
  volume = {393},
  pages = {268--272},
  number = {6682},
  publisher = {Nature Publishing Group}
}

@inproceedings{aharoni2019massively,
  title ={Massively Multilingual Neural Machine Translation},
  author ={Aharoni, Roee and Johnson, Melvin and Firat, Orhan},
  booktitle ={Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  month = jun,
  year = 2019,
  address ={Minneapolis, Minnesota},
  publisher ={Association for Computational Linguistics},
  doi ={10.18653/v1/N19-1388},
  pages ={3874--3884}
}

@MISC{ams_company,
  author = {{A}ustria {M}ikro {S}ysteme {I}nternational {AG}},
  title = {\emph{austria\textsf{micro}systems} {AG}},
  howpublished = {Schloss Premst{\"a}tten, A-8141 Unterpremst{\"a}tten, Austria, \\\texttt{http://www.austriamicrosystems.com}},
  keywords = {spec}
}

@INPROCEEDINGS{amir2013cognitive,
  title={Cognitive Computing Programming Paradigm: A Corelet Language for Composing Networks of Neurosynaptic Cores},
  author={Amir, Arnon and Datta, Pallab and Risk, William P and Cassidy, Andrew S and Kusnitz, Jeffrey A and Esser, Steve K and Andreopoulos, Alexander and Wong, Theodore M and Flickner, Myron and Alvarez-Icaza, Rodrigo and McQuinn, Emmett and Shaw, Ben and Pass, Norm and Modha, Dharmendra S.},
  booktitle={The 2013 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--10},
  year=2013,
  organization={IEEE}
}

@ARTICLE{AlMaashri2013,
  author = {Al Maashri, Ahmed and Cotter, Matthew and Chandramoorthy, Nandhini
	and DeBole, Michael and Yu, Chi-Li and Narayanan, Vijaykrishnan and
	Chakrabarti, Chaitali},
  title = {Hardware Acceleration for Neuromorphic Vision Algorithms},
  journal = {Journal of Signal Processing Systems},
  year = {2013},
  volume = {70},
  pages = {163-175},
  number = {2},
  doi = {10.1007/s11265-012-0699-x},
  issn = {1939-8018},
  keywords = {Domain-specific acceleration; Power efficiency; Neuromorphic systems},
  language = {English},
  owner = {simon},
  publisher = {Springer US},
  timestamp = {2013.04.21},
  url = {http://dx.doi.org/10.1007/s11265-012-0699-x}
}

@INPROCEEDINGS{alam2008early,
  author = {Alam, S. and Barrett, R. and Bast, M. and Fahey, M.R. and Kuehn,
	J. and McCurdy, C. and Rogers, J. and Roth, P. and Sankaran, R. and
	Vetter, J.S. and others},
  title = {Early evaluation of IBM BlueGene/P},
  booktitle = {Proceedings of the 2008 ACM/IEEE conference on Supercomputing},
  year = {2008},
  pages = {23},
  organization = {IEEE Press},
  owner = {simon},
  timestamp = {2013.01.23}
}

@ARTICLE{albada2018performance,
  author={van Albada, Sacha J. and Rowley, Andrew G. and Senk, Johanna and Hopkins, Michael and Schmidt, Maximilian and Stokes, Alan B. and Lester, David R. and Diesmann, Markus and Furber, Steve B.},
  title={Performance Comparison of the Digital Neuromorphic Hardware SpiNNaker and the Neural Network Simulation Software NEST for a Full-Scale Cortical Microcircuit Model},
  journal={Frontiers in Neuroscience},
  volume=12,
  year=2018,
  url={https://www.frontiersin.org/article/10.3389/fnins.2018.00291},
  doi={10.3389/fnins.2018.00291},
  issn={1662-453X},
  abstract={The digital neuromorphic hardware SpiNNaker has been developed with the aim of enabling large-scale neural network simulations in real time and with low power consumption. Real-time performance is achieved with 1 ms integration time steps, and thus applies to neural networks for which faster time scales of the dynamics can be neglected. By slowing down the simulation, shorter integration time steps and hence faster time scales, which are often biologically relevant, can be incorporated. We here describe the first full-scale simulations of a cortical microcircuit with biological time scales on SpiNNaker. Since about half the synapses onto the neurons arise within the microcircuit, larger cortical circuits have only moderately more synapses per neuron. Therefore, the full-scale microcircuit paves the way for simulating cortical circuits of arbitrary size. With approximately 80, 000 neurons and 0.3 billion synapses, this model is the largest simulated on SpiNNaker to date. The scale-up is enabled by recent developments in the SpiNNaker software stack that allow simulations to be spread across multiple boards. Comparison with simulations using the NEST software on a high-performance cluster shows that both simulators can reach a similar accuracy, despite the fixed-point arithmetic of SpiNNaker, demonstrating the usability of SpiNNaker for computational neuroscience applications with biological time scales and large network size. The runtime and power consumption are also assessed for both simulators on the example of the cortical microcircuit model. To obtain an accuracy similar to that of NEST with 0.1 ms time steps, SpiNNaker requires a slowdown factor of around 20 compared to real time. The runtime for NEST saturates around 3 times real time using hybrid parallelization with MPI and multi-threading. However, achieving this runtime comes at the cost of increased power and energy consumption. The lowest total energy consumption for NEST is reached at around 144 parallel threads and 4.6 times slowdown. At this setting, NEST and SpiNNaker have a comparable energy consumption per synaptic event. Our results widen the application domain of SpiNNaker and help guide its development, showing that further optimizations such as synapse-centric network representation are necessary to enable real-time simulation of large biological neural networks.}
}

@inproceedings{albada2021usage,
	author={van Albada, Sacha J. and Pronold, Jari and van Meegen, Alexander and Diesmann, Markus},
	editor={Amunts, Katrin and Grandinetti, Lucio and Lippert, Thomas and Petkov, Nicolai},
	title={Usage and Scaling of an Open-Source Spiking Multi-Area Model of Monkey Cortex},
	booktitle={Brain-Inspired Computing},
	year=2021,
	publisher={Springer International Publishing},
	address={Cham},
	pages={47--59},
	abstract={We are entering an age of `big' computational neuroscience, in which neural network models are increasing in size and in numbers of underlying data sets. Consolidating the zoo of models into large-scale models simultaneously consistent with a wide range of data is only possible through the effort of large teams, which can be spread across multiple research institutions. To ensure that computational neuroscientists can build on each other's work, it is important to make models publicly available as well-documented code. This chapter describes such an open-source model, which relates the connectivity structure of all vision-related cortical areas of the macaque monkey with their resting-state dynamics. We give a brief overview of how to use the executable model specification, which employs NEST as simulation engine, and show its runtime scaling. The solutions found serve as an example for organizing the workflow of future models from the raw experimental data to the visualization of the results, expose the challenges, and give guidance for the construction of an ICT infrastructure for neuroscience.},
	isbn={978-3-030-82427-3}
}

@MISC{albert10bachelorthesis,
  author = {Albert, Marvin},
  title = {Liquid Computing mit Neuromorpher Hardware},
  howpublished = {Bachelor thesis (German), University of Heidelberg, HD-KIP 10-43},
  year = {2010},
  key = {albert10bachelorthesis},
  keywords = {vision, spikey, stage1, software, pynn, plasticity, liquid computing}
}

@MISC{scherzer13bachelorthesis,
  author = {Scherzer, Anne-Christine},
  title = {Phase-Locking on Neuromorphic Hardware},
  howpublished = {Bachelor thesis (German), University of Heidelberg, HD-KIP 10-43},
  year = {2013},
  key = {scherzer13bachelorthesis},
  keywords = {vision, spikey, stage1, software, pynn, plasticity}
}

@book{alberts94mboc,
	title={Molecular Biology of the Cell, third edition},
	author={Bruce Alberts and Dennis Bray and Julian Lewis and Martin Raff and Keith Roberts and Jamed D. Watsonx },
	isbn={0815316208},
	chapter={10 and 11},
	year={1994},
	publisher={Garland Publishing, Inc}
}

@BOOK{allen02cmos,
  title = {{CMOS} Analog Circuit Design},
  publisher = {Oxford University Press, Inc.},
  year = {2002},
  author = {Allen, P. E. and Holberg, D. R.},
  address = {198 Madison Avenue, New York},
  isbn = {0-19-511644-5}
}

@article{alroy2008dynamics,
  title={Dynamics of origination and extinction in the marine fossil record},
  author={Alroy, John},
  journal={Proceedings of the National Academy of Sciences},
  volume={105},
  number={Supplement 1},
  pages={11536--11542},
  year={2008},
  publisher={National Acad Sciences}
}

@INCOLLECTION{amari77b,
  author = {Amari, S. and Arbib, M.A.},
  title = {Competition and Cooperation in Neural Nets},
  booktitle = {Systems Neuroscience},
  publisher = {New York: Academic Press},
  year = {1977},
  editor = {J. Metzler},
  pdf = {http://birg2.epfl.ch/literature/repository/amari77b.pdf}
}

@ARTICLE{ambros-ingerson2005reconstructions,
  author = {Ambros-Ingerson, J. and Holmes, W. R.},
  title = {Analysis and Comparison of Morphological Reconstructions of Hippocampal Field CA1 Pyramidal Cells},
  journal = {Hippocampus},
  year = {2005},
  volume = {15},
  pages = {302--315}
}

@INPROCEEDINGS{amemori2000,
  author = {Amemori, K. and Ishii, S.},
  title = {Effect of the synaptic time constant on stochastic spiking neurons},
  booktitle = {7th International Conference on Neural Information Processing (ICONIP)},
  year = {2000},
  volume = {1},
  pages = {6--11}
}

@ARTICLE{amit97model,
  author = {D J Amit and N Brunel},
  title = {Model of global spontaneous activity and local structured activity
	during delay periods in the cerebral cortex},
  journal = {Cereb Cortex},
  year = {1997},
  volume = {7},
  pages = {237--52},
  number = {3},
  month = {Jan},
  abstract = {We investigate self-sustaining stable states (attractors) in networks
	of integrate-and-fire neurons. First, we study the stability of spontaneous
	activity in an unstructured network. It is shown that the stochastic
	background activity, of 1-5 spikes/s, is unstable if all neurons
	are excitatory. On the other hand, spontaneous activity becomes self-stabilizing
	in presence of local inhibition, given reasonable values of the parameters
	of the network. Second, in a network sustaining physiological spontaneous
	rates, we study the effect of learning in a local module, expressed
	in synaptic modifications in specific populations of synapses. We
	find that if the average synaptic potentiation (LTP) is too low,
	no stimulus specific activity manifests itself in the delay period.
	Instead, following the presentation and removal of any stimulus there
	is, in the local module, a delay activity in which all neurons selective
	(responding visually) to any of the stimuli presented for learning
	have rates which gradually increase with the amplitude of synaptic
	potentiation. When the average LTP increases beyond a critical value,
	specific local attractors (stable states) appear abruptly against
	the background of the global uniform spontaneous attractor. In this
	case the local module has two available types of collective delay
	activity: if the stimulus is unfamiliar, the activity is spontaneous;
	if it is similar to a learned stimulus, delay activity is selective.
	These new attractors reflect the synaptic structure developed during
	learning. In each of them a small population of neurons have elevated
	rates, which depend on the strength of LTP. The remaining neurons
	of the module have their activity at spontaneous rates. The predictions
	made in this paper could be checked by single unit recordings in
	delayed response experiments.},
  affiliation = {Racah Institute of Physics, Hebrew University, Jerusalem, Israel.}
}

@INPROCEEDINGS{Ananthanarayanan2009,
  author = {Ananthanarayanan, R. and Esser, S.K. and Simon, H.D. and Modha, D.S.},
  title = {The cat is out of the bag: cortical simulations with 10 9 neurons,
	10 13 synapses},
  booktitle = {Proceedings of the Conference on High Performance Computing Networking,
	Storage and Analysis},
  year = {2009},
  pages = {63},
  organization = {ACM},
  owner = {simon},
  timestamp = {2013.01.23}
}

@ARTICLE{anderson2000stimulus,
  author = {Anderson, Jeffrey and Lampl, Ivan and Reichova, Iva and Carandini,
	Matteo and Ferster, David},
  title = {Stimulus dependence of two-state fluctuations of membrane potential
	in cat visual cortex},
  journal = {Nature Neuroscience},
  year = {2000},
  volume = {3},
  pages = {617--621}
}

@ARTICLE{andersson2015subthreshold,
  author={Andersson, Oskar and Chon, Ki H. and S\"{o}rnmo, Leif and Rodrigues, Joachim Neves},
  journal={{IEEE} Transactions on Biomedical Circuits and Systems},
  title={A 290 mV Sub-$V_{\rm T}$ ASIC for Real-Time Atrial Fibrillation Detection},
  year=2015,
  volume=9,
  number=3,
  pages={377--386},
  doi={10.1109/TBCAS.2014.2354054}
}

@article{antic2010, crossref = {antic2010nmda} }
@article {antic2010nmda,
  author = {Antic, Srdjan D. and Zhou, Wen-Liang and Moore, Anna R. and Short, Shaina M. and Ikonomu, Katerina D.},
  title = {The decade of the dendritic {NMDA} spike},
  journal = {Journal of Neuroscience Research},
  volume = {88},
  number = {14},
  publisher = {Wiley Subscription Services, Inc., A Wiley Company},
  issn = {1097-4547},
  url = {http://dx.doi.org/10.1002/jnr.22444},
  doi = {10.1002/jnr.22444},
  pages = {2991--3001},
  keywords = {synaptic integration, plateau potential, summation, regeneration, Up state, neocortex, basal, oblique, glutamate},
  year = {2010}
}

@ARTICLE{angeline94evolutionary,
  author = {Angeline, P. J. and Saunders, G. M. and Pollack, J. B.},
  title = {An evolutionary approach that constructs recurrent neural networks},
  journal = {IEEE Trans. on Neural Networks},
  year = {1994},
  volume = {5},
  pages = {54--65},
  number = {1},
  file = {angeline94evolutionary.pdf:angeline94evolutionary.pdf:PDF}
}

@INPROCEEDINGS{anguita2013public,
  title={A public domain dataset for human activity recognition using smartphones.},
  author={Anguita, Davide and Ghio, Alessandro and Oneto, Luca and Parra, Xavier and Reyes-Ortiz, Jorge Luis},
  booktitle={Esann},
  year={2013}
}


@INPROCEEDINGS{valle_esann01perspectives,
  author = {Anguita, D. and Valle, M.},
  title = {Perspectives on dedicated hardware implementations},
  booktitle = {Proceedings ESANN 2001},
  year = {2001},
  pages = {45-55},
  publisher = {D-Facto},
  groupsearch = {0},
  howpublished = {ISBN 2-930307-01-3},
  key = {valle_perspectives}
}

@MISC{annunziato-analog,
  author = {M. Annunziato and D. Badoni and S. Fusi and A. Salamon},
  title = {Analog {VLSI} implementation of a spikedriven stochastic dynamical
	synapse},
  citeseerurl = {citeseer.nj.nec.com/21461.html},
  keywords = {spiking},
  text = {Annunziato M., Badoni D., Fusi S., Salamon A., Analog VLSI implementation
	of a spikedriven stochastic dynamical synapse, these proceedings}
}

@MANUAL{lvds_standard,
  title = {Electrical Characteristics of Low Voltage Differential Signalling
	({LVDS})},
  author = {ANSI/TIA/EIA-644},
  month = {March},
  year = {1996},
  groupsearch = {0},
  key = {standard_lvds},
  keywords = {spec}
}

@ARTICLE{appleby05ensembleinterpretation,
  author = {Appleby, Peter A. and Elliott, Terry},
  title = {{Synaptic and Temporal Ensemble Interpretation of Spike-Timing-Dependent
	Plasticity}},
  journal = {Neural Comp.},
  year = {2005},
  volume = {17},
  pages = {2316-2336},
  number = {11},
  abstract = {We postulate that a simple, three-state synaptic switch governs changes
	in synaptic strength at individual synapses. Under this switch rule,
	we show that a variety of experimental results on timing-dependent
	plasticity can emerge from temporal and spatial averaging over multiple
	synapses and multiple spike pairings. In particular, we show that
	a critical window for the interaction of pre- and postsynaptic spikes
	emerges as an ensemble property of the collective system, with individual
	synapses exhibiting only a minimal form of spike coincidence detection.
	In addition, we show that a Bienenstock-Cooper-Munro-like, rate-based
	plasticity rule emerges directly from such a model. This demonstrates
	that two apparently separate forms of neuronal plasticity can emerge
	from a much simpler rule governing the plasticity of individual synapses.},
  eprint = {http://neco.mitpress.org/cgi/reprint/17/11/2316.pdf},
  keywords = {plasticity learning},
  url = {http://neco.mitpress.org/cgi/content/abstract/17/11/2316}
}

@MISC{ARMArchv5,
  author = {{ARM Ltd.}},
  title = {ARMv5 Architecture Reference Manual},
  year = {2007},
  owner = {simon},
  timestamp = {2013.04.15},
  url = {http://infocenter.arm.com/help/index.jsp}
}

@inproceedings{arnold2022spiking,
  title = {Spiking Neural Network Equalization for {IM/DD} Optical Communication},
  author = {Elias Arnold and Georg B{\"o}cherer and Eric M{\"u}ller and Philipp Spilger and Johannes Schemmel and Stefano Calabr{\`o} and Maxim Kuschnerov},
  year = 2022,
  month = may,
  booktitle = {Signal Processing in Photonic Communications (SPPCom)},
  note = {accepted},
  eprint  = {2205.04263},
  archivePrefix = {arXiv},
  primaryClass = {eess.SP}
}

@inproceedings{arnold2022spikinghardware,
  title = {Spiking Neural Network Equalization on Neuromorphic Hardware for {IM/DD} Optical Communication},
  author = {Elias Arnold and Georg B{\"o}cherer and Eric M{\"u}ller and Philipp Spilger and Johannes Schemmel and Stefano Calabr{\`o} and Maxim Kuschnerov},
  year = 2022,
  month = jun,
  booktitle = {European Conference on Optical Communication (ECOC)},
  note = {accepted},
  eprint  = {2206.00401},
  archivePrefix = {arXiv},
  primaryClass = {eess.SP}
}

@INPROCEEDINGS{arthur07,
  author = {Arthur, J.V. and Boahen, K.},
  title = {Silicon Neurons that Inhibit to Synchronize},
  booktitle = {Circuits and Systems, 2007. ISCAS 2007. IEEE International Symposium
	on},
  year = {2007},
  pages = {1186 -1186},
  month = {27-30},
  abstract = {We present a network of silicon neurons that achieve robust synchrony
	using mutual inhibition. Synchrony strongly influences neuronal spike
	timing within many brain regions, potentially playing a crucial role
	in computation. Yet it has been largely ignored in neuromorphic systems,
	which use mixed analog and digital circuits to model neurobiology
	in silicon. Our neurons synchronize by using shunting inhibition
	(conductance-based) with a synaptic rise-time. Synaptic rise-time
	promotes synchrony by delaying the effect of inhibition, providing
	an opportune period for neurons to spike together. Shunting inhibition,
	through its voltage dependence, inhibits neurons that are late more
	strongly (delaying the spike further), pushing them into phase (in
	the next cycle). We fabricated a chip with 256 inhibitory neurons
	and 1,024 excitatory neurons in 0.25mum CMOS. We show that synchronized
	inhibitory neurons (population of 256) spike with a period that is
	proportional to the synaptic rise-time. We use these neurons to entrain
	the excitatory neurons, implementing a form of object binding},
  doi = {10.1109/ISCAS.2007.378263},
  keywords = {0.25 micron;CMOS circuits;Si;mixed analog-digital circuits;mutual
	inhibition;neurobiology;neuromorphic systems;neuronal spike timing;silicon
	neurons;CMOS integrated circuits;neural chips;silicon;}
}

@INPROCEEDINGS{arthur04,
  author = {Arthur, J.V. and Boahen, K.},
  title = {Recurrently connected silicon neurons with active dendrites for one-shot
	learning},
  booktitle = {Neural Networks, 2004. Proceedings. 2004 IEEE International Joint
	Conference on},
  year = {2004},
  volume = {3},
  pages = { 1699 - 1704 vol.3},
  month = {july},
  abstract = { We describe a neuromorphic chip designed to model active dendrites,
	recurrent connectivity, and plastic synapses to support one-shot
	learning. Specifically, it is designed to capture neural firing patterns
	(short-term memory), memorize individual patterns (long-term memory),
	and retrieve them when primed (associative recall). It consists of
	a recurrently connected population of excitatory pyramidal cells
	and a recurrently connected population of inhibitory basket cells.
	In addition to their recurrent connections, the excitatory and inhibitory
	populations are reciprocally connected. The model is novel in that
	it utilizes recurrent connections and active dendrites to maintain
	short-term memories as well as to store long-term memories.},
  doi = {10.1109/IJCNN.2004.1380858},
  issn = {1098-7576},
  keywords = { active dendrites; excitatory pyramidal cells; inhibitory basket cells;
	long term memory; neural firing patterns; neuromorphic chip design;
	one shot learning; plastic synapses; recurrently connected population;
	recurrently connected silicon neurons; short term memory; content-addressable
	storage; dendrites; learning (artificial intelligence); neural chips;
	recurrent neural nets;}
}

@MISC{Atmel2012,
  author = {Atmel},
  title = {8-bit Atmel Microcontroller with 64K/128K/256K Bytes In-System Programmable
	Flash},
  month = {October},
  year = {2012},
  note = {revision P},
  owner = {simon},
  timestamp = {2013.04.15},
  url = {http://www.atmel.com/Images/doc2549.pdf}
}

@INPROCEEDINGS{auer_icann02,
  author = {Auer, P. and Burgsteiner, H. and Maass, W.},
  title = {Reducing communication for distributed learning in neural networks},
  booktitle = {Proceedings of the International Conference on Artificial Neural
	Networks {ICANN}'02},
  year = {2002},
  pages = {123-128},
  publisher = {Springer Verlag}
}

@ARTICLE{ex1,
  author = {Author, Ivan Marc},
  title = {Some Related article {I} Wrote},
  journal = {Some Fine Journal},
  year = {1999},
  volume = {99},
  pages = {1-100},
  number = {7},
  month = {January},
  groupsearch = {0}
}

@ARTICLE{aviel03embedding,
  author = {Aviel, Yuval and Mehring, Carsten and Abeles, Moshe and Horn, David},
  title = {On Embedding Synfire Chains in a Balanced Network},
  journal = {Neural Computation},
  year = {2003},
  volume = {15},
  pages = {1321-1340},
  number = {6}
}

@manual{avnet2020ultra96,
  author = {{AVNET}},
  title = {{Ultra96-V2}},
  url = {http://zedboard.org/sites/default/files/product_briefs/5365-pb-ultra96-v2-v10b.pdf},
  year = 2020
}

@ARTICLE{aybay91neuralhardware,
  author = {Isik Aybay and S. Çetinkaya and U. Halici},
  title = {Classification Of Neural Network Hardware},
  citeseerurl = {http://citeseer.ist.psu.edu/402895.html},
  keywords = {vlsi},
  owner = {fieres}
}

@INPROCEEDINGS{azariadi2016ecg,
  author={Azariadi, Dimitra and Tsoutsouras, Vasileios and Xydis, Sotirios and Soudris, Dimitrios},
  booktitle={2016 5th International Conference on Modern Circuits and Systems Technologies ({MOCAST})},
  title={{ECG} signal analysis and arrhythmia detection on IoT wearable medical devices},
  year=2016,
  volume={},
  number={},
  pages={1--4},
  doi={10.1109/MOCAST.2016.7495143}
}

@article{azghadi2014tunable,
	abstract = "Cortical circuits in the brain have long been recognised for their information processing capabilities and have been studied both experimentally and theoretically via spiking neural networks. Neuromorphic engineers are primarily concerned with translating the computational capabilities of biological cortical circuits, using the Spiking Neural Network (SNN) paradigm, into in silico applications that can mimic the behaviour and capabilities of real biological circuits/systems. These capabilities include low power consumption, compactness, and relevant dynamics. In this paper, we propose a new accelerated-time circuit that has several advantages over its previous neuromorphic counterparts in terms of compactness, power consumption, and capability to mimic the outcomes of biological experiments. The presented circuit simulation results demonstrate that, in comparing the new circuit to previous published synaptic plasticity circuits, reduced silicon area and lower energy consumption for processing each spike is achieved. In addition, it can be tuned in order to closely mimic the outcomes of various spike timing- and rate-based synaptic plasticity experiments. The proposed circuit is also investigated and compared to other designs in terms of tolerance to mismatch and process variation. Monte Carlo simulation results show that the proposed design is much more stable than its previous counterparts in terms of vulnerability to transistor mismatch, which is a significant challenge in analog neuromorphic design. All these features make the proposed design an ideal circuit for use in large scale SNNs, which aim at implementing neuromorphic systems with an inherent capability that can adapt to a continuously changing environment, thus leading to systems with significant learning and computational abilities.",
	author = "{Rahimi Azghadi}, Mostafa AND Iannella Nicolangelo AND Al-Sarawi Said AND Abbott Derek",
	doi = "10.1371/journal.pone.0088326",
	journal = "PLOS ONE",
	month = "02",
	number = "2",
	pages = "e88326",
	publisher = "Public Library of Science",
	title = "{Tunable Low Energy, Compact and High Performance Neuromorphic Circuit for Spike-Based Synaptic Plasticity}",
	url = "http://dx.doi.org/10.1371%2Fjournal.pone.0088326",
	volume = "9",
	year = "2014"
}

@article{azghadi2015programmable,
	acmid = "2658998",
	address = "New York, NY, USA",
	articleno = "17",
	author = "Azghadi, Mostafa Rahimi and Moradi, Saber and Fasnacht, Daniel B. and Ozdas, Mehmet Sirin and Indiveri, Giacomo",
	doi = "10.1145/2658998",
	issn = "1550-4832",
	issue_date = "August 2015",
	journal = "J. Emerg. Technol. Comput. Syst.",
	keywords = "AER; STDP; VLSI; asynchronous; emerging technologies; learning; neuromorphic; plasticity; realtime; subthreshold",
	month = sep,
	number = "2",
	numpages = "18",
	pages = "17:1--17:18",
	publisher = "ACM",
	title = "{Programmable Spike-Timing-Dependent Plasticity Learning Circuits in Neuromorphic VLSI Architectures}",
	url = "http://doi.acm.org/10.1145/2658998",
	volume = "12",
	year = "2015"
}

@article{azghadi2020complementary,
  title={Complementary metal-oxide semiconductor and memristive hardware for neuromorphic computing},
  author={Rahimi Azghadi, Mostafa and Chen, Ying-Chen and Eshraghian, Jason K and Chen, Jia and Lin, Chih-Yang and Amirsoleimani, Amirali and Mehonic, Adnan and Kenyon, Anthony J and Fowler, Burt and Lee, Jack C and others},
  journal={Advanced Intelligent Systems},
  volume=2,
  number=5,
  pages={1900189},
  year=2020,
  publisher={Wiley Online Library}
}

@article{azouz1999cellular,
  title={Cellular mechanisms contributing to response variability of cortical neurons in vivo},
  author={Azouz, Rony and Gray, Charles M},
  journal={The Journal of neuroscience},
  volume={19},
  number={6},
  pages={2209--2223},
  year={1999},
  publisher={Soc Neuroscience}
}

@ARTICLE{barazzouk1997field,
  author = {Abdellfattah Ba-Razzouk and Ahmed Ch{\'e}riti and Guy Olivier and Pierre Sicard},
  journal = {IEEE Transactions on Power Electronics},
  title = {Field-oriented control of induction motors using neural-network decouplers},
  year = 1997,
  volume = 12,
  number = 4,
  pages = {752--763},
  doi = {10.1109/63.602571}
}

@INPROCEEDINGS{baeck_icga91survey,
  author = {B\"ack, Thomas and Hoffmeister, F. and Schwefel, H.},
  title = {A survey of evolution strategies},
  booktitle = {Proceedings of the 4th International Conference on Genetic Algorithms},
  year = {1991},
  editor = {Belew, Richard K. and booker, Lashon B.},
  pages = {2--9},
  address = {San Diego, CA},
  month = {July},
  publisher = {Morgan Kaufmann},
  file = {baeck_icga91survey.pdf:baeck_icga91survey.pdf:PDF}
}

@ARTICLE{Backus1978,
  author = {Backus, John},
  title = {Can programming be liberated from the von Neumann style?: a functional
	style and its algebra of programs},
  journal = {Commun. ACM},
  year = {1978},
  volume = {21},
  pages = {613--641},
  number = {8},
  month = aug,
  acmid = {359579},
  address = {New York, NY, USA},
  doi = {10.1145/359576.359579},
  issn = {0001-0782},
  issue_date = {Aug. 1978},
  keywords = {algebra of programs, applicative computing systems, applicative state
	transition systems, combining forms, functional forms, functional
	programming, metacomposition, models of computing systems, program
	correctness, program termination, program transformation, programming
	languages, von Neumann computers, von Neumann languages},
  numpages = {29},
  owner = {sfriedma},
  publisher = {ACM},
  timestamp = {2012.10.23},
  url = {http://doi.acm.org/10.1145/359576.359579}
}

@ARTICLE{baddeley97response,
  author = {Roland Baddeley and L. F. Abbott and Michael C. A. Booth and Frank
	Sengpiel and Toby Freeman and Edward A. Wakeman and Edmund T. Rolls},
  title = {Responses of neurons in primary and inferior temporal visual cortices
	to natural scenes},
  journal = {Proceedings of the Royal Society B},
  year = {1997},
  volume = {264},
  pages = {1775--1783}
}

@ARTICLE{badel2006dependence,
  author = {Badel, Laurent and Gerstner, Wulfram and Richardson, Magnus J. },
  title = {Dependence of the spike-triggered average voltage on membrane response
	properties},
  journal = {Neurocomputing},
  year = {2006},
  volume = {69},
  pages = {1062--1065},
  number = {10-12},
  month = {June},
  abstract = {The spike-triggered average voltage (STV) is an experimentally measurable
	quantity that is determined by both the membrane response properties
	and the statistics of the synaptic drive. Here, the form of the STV
	is modelled for neurons with three distinct types of subthreshold
	dynamics; passive decay, h-current sag, and damped oscillations.
	Analytical expressions for the STV are first obtained in the low-noise
	limit, identifying how the subthreshold dynamics of the cell affect
	its form. A second result is then derived that captures the power-law
	behaviour of the STV near the spike threshold.},
  booktitle = {Computational Neuroscience: Trends in Research 2006},
  doi = {http://dx.doi.org/10.1016/j.neucom.2005.12.046},
  keywords = {dynamics, model, sta}
}

@ARTICLE{bahram2000,
  author = {Bahram, S. and Azami, Z. and Feng, G.},
  title = {Robust vector quantizer design using self-organizing neural networks},
  journal = {SP},
  year = {2000},
  volume = {80},
  pages = {1289-1298},
  number = {7},
  month = {July},
  bibsource = {http://www.visionbib.com/bibliography/image-proc150.html#TT10711}
}

@INPROCEEDINGS{baker_icga87reducing,
  author = {Baker, J. E.},
  title = {Reducing bias and inefficiency in the selection algorithm},
  booktitle = {Proceedings of the 2nd International Conference on Genetic Algorithms},
  year = {1987},
  editor = {J. J. Grefenstette},
  pages = {14--21},
  address = {Hillsdale, New Jersey},
  publisher = {Lawrence Erlbaum}
}

@ARTICLE{baker2001synchronization,
  author = {Baker, S.N. and Spinks, R. and Jackson, A. and Lemon, RN},
  title = {Synchronization in monkey motor cortex during a precision grip task.
	I. Task-dependent modulation in single-unit synchrony},
  journal = {Journal of Neurophysiology},
  year = {2001},
  volume = {85},
  pages = {869--885},
  number = {2},
  publisher = {Am Physiological Soc}
}

@BOOK{barlow89,
  title = {Statistics: A Guide to the Use of Statistical Methods in the Physical
	Sciences (The Manchester Physics Series)},
  publisher = {John Wiley \& Sons Ltd},
  year = {1989},
  author = {Barlow, Roger J.},
  edition = {Reprint},
  month = {July},
  abstract = {The Manchester Physics Series General Editors: D. J. Sandiford; F.
	Mandl; A. C. Phillips Department of Physics and Astronomy, University
	of Manchester Properties of Matter B. H. Flowers and E. Mendoza Optics
	Second Edition F. G. Smith and J. H. Thomson Statistical Physics
	Second Edition F. Mandl Electromagnetism Second Edition I. S. Grant
	and W. R. Phillips Statistics R. J. Barlow Solid State Physics Second
	Edition J. R. Hook and H. E. Hall Quantum Mechanics F. Mandl Particle
	Physics Second Edition B. R. Martin and G. Shaw The Physics of Stars
	Second Edition A.C. Phillips Computing for Scientists R. J. Barlow
	and A. R. Barnett Written by a physicist, Statistics is tailored
	to the needs of physical scientists, containing and explaining all
	they need to know. It concentrates on parameter estimation, especially
	the methods of Least Squares and Maximum Likelihood, but other techniques,
	such as hypothesis testing, Bayesian statistics and non-parametric
	methods are also included. Intended for reasonably numerate scientists
	it contains all the basic formulae, their derivations and applications,
	together with some more advanced ones. Statistics features: * Comprehensive
	coverage of the essential techniques physical scientists are likely
	to need. * A wealth of examples, and problems with their answers.
	* Flexible structure and organisation allows it to be used as a course
	text and a reference. * A review of the basics, so that little prior
	knowledge is required.},
  citeulike-article-id = {3269213},
  howpublished = {Paperback},
  isbn = {0471922951},
  keywords = {book, statistics},
  posted-at = {2008-09-15 15:04:32},
  priority = {2},
  url = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/0471922951}
}

@ARTICLE{bartolozzi07synaptic,
  author = {Bartolozzi, Chiara and Indiveri, Giacomo},
  title = {Synaptic Dynamics in Analog {VLSI}},
  journal = {Neural Comput.},
  year = {2007},
  volume = {19},
  pages = {2581--2603},
  number = {10},
  address = {Cambridge, MA, USA},
  publisher = {MIT Press}
}

@INPROCEEDINGS{bartolozzi2006ultra,
  author = {Bartolozzi, C. and Mitra, S. and Indiveri, G.},
  title = {An ultra low power current-mode filter for neuromorphic systems and
	biomedical signal processing},
  booktitle = {Biomedical Circuits and Systems Conference, 2006. BioCAS 2006. IEEE},
  year = {2006},
  pages = {130--133},
  organization = {IEEE}
}

@ARTICLE{basu10,
  author = {Basu, A. and Brink, S. and Schlottmann, C. and Ramakrishnan, S. and
	Petre, C. and Koziol, S. and Baskaya, F. and Twigg, C.M. and Hasler,
	P.},
  title = {A Floating-Gate-Based Field-Programmable Analog Array},
  journal = {Solid-State Circuits, IEEE Journal of},
  year = {2010},
  volume = {45},
  pages = {1781 -1794},
  number = {9},
  month = {sept. },
  doi = {10.1109/JSSC.2010.2056832},
  issn = {0018-9200},
  keywords = {AM receiver;CAB;CMOS;FG devices;FPAA;SPI digital interface;bandwidth
	57 MHz;circuit elements;computational analog blocks;first-order low-pass
	filter;floating-gate transistors;floating-gate-based field programmable
	analog array;nMOS transistors;pMOS transistors;program mode switch;programmable
	interconnects;routing fabric;size 0.35 mum;speech processor;switch
	matrix;CMOS analogue integrated circuits;field programmable analogue
	arrays;integrated circuit interconnections;low-pass filters;network
	routing;}
}

@MISC{baudy2009,
  author = {Johann Baudy},
  title = {{Linux Kernel 2.6.31 -- net: TX\_RING and packet mmap}},
  howpublished = {{Linux Kernel 2.6.31 -- commit 69e3c75f4d541a6eb151b3ef91f34033cb3ad6e1}},
  year = {2009}
}

@ARTICLE{bauer2019realtime,
  author = {Bauer, Felix Christian and Muir, Dylan Richard and Indiveri, Giacomo},
  title = {Real-Time Ultra-Low Power {ECG} Anomaly Detection Using an Event-Driven Neuromorphic Processor},
  journal = {{IEEE} Transactions on Biomedical Circuits and Systems},
  year = 2019,
  volume = 13,
  number = 6,
  pages = {1575--1582},
  doi = {10.1109/TBCAS.2019.2953001}
}

@ARTICLE{baum88capabil,
  author = {Baum, E. B.},
  title = {On the capabilities of multilayer perceptrons},
  journal = {Journal of Complexity},
  year = {1988},
  volume = {4},
  pages = {193--215}
}

@ARTICLE{baum89size,
  author = {Baum, E. B. and Haussler, D.},
  title = {What Size Nets Gives Valid Generalization?},
  journal = {Neural Computation},
  year = {1989},
  volume = {1},
  pages = {151--160}
}

@ARTICLE{baxter92evolution,
  author = {Baxter, J.},
  title = {The evolution of learning algorithms for artificial neural networks},
  journal = {Complex Systems},
  year = {1992},
  pages = {313--326},
  file = {baxter92evolution.pdf:baxter92evolution.pdf:PDF}
}

@ARTICLE{bean2007action,
  author = {Bean, B.P.},
  title = {The action potential in mammalian central neurons},
  journal = {Nature Reviews Neuroscience},
  year = {2007},
  volume = {8},
  pages = {451--465},
  number = {6},
  publisher = {Nature Publishing Group}
}

@ARTICLE{beaulieu85laminar,
  author = {Beaulieu, C. and Colonnier, M.},
  title = {A laminar analysis of the number of round-asymmetrical and flat-symmetrical
	synapses on spines, dendritic trunks, and cell bodies in area 17
	of the cat},
  journal = {J Comp Neurol},
  year = {1985},
  volume = {231},
  pages = {180--9},
  number = {2},
  month = {Jan},
  abstract = {The number of synapses per unit volume of tissue (NV) has been estimated
	in individual laminae of the binocular and monocular regions of area
	17 in six adult cats by using a method of size-frequency distribution.
	Separate estimates were obtained for RA synapses (containing round
	vesicles associated with asymmetric membrane differentiations) and
	for FS synapses (containing flat vesicles associated with symmetric
	membrane differentiations). For the total cortical thickness, the
	NV of all synapses is not statistically different between binocular
	(286 million per mm13) and monocular (281 million) regions, nor is
	it different between the two regions for any of the laminae. Eighty-four
	percent of synapses are of the RA type. Of those, 79i\% are found
	on dendritic spines, 21\% on dendritic trunks, 0.1\% on somata. FS
	synapses represent 16\% of the total, with 31\% of them on spines,
	62\% on dendritic trunks, and 7\% on somata. The ratio of RA to FS
	synapses is kept relatively constant throughout the layers. A two-way
	analysis of variance shows no difference in the NV of either RA or
	FS synapses in the two regions nor in the NV or RA synapses between
	cats. It does, however, clearly demonstrate (p less than 0.001) interindividual
	differiences for FS synapses. These variations between individual
	cats may be due to differences in age, breed, or environmental factors.
	In contrast to the relative uniformity of the NV of synapses between
	regions, the number of each type under 1 mm2 of cortical surface
	is 33\% higher in the binocular region. This is due mainly to the
	greater thickness of the binocular region.},
  keywords = {Synapses, Variation (Genetics), Dendrites, Visual Cortex, Animals,Cats}
}

@ARTICLE{beaulieu83number,
  author = {Beaulieu, C. and Colonnier, M.},
  title = {The number of neurons in the different laminae of the binocular and
	monocular regions of area 17 in the cat},
  journal = {J Comp Neurol},
  year = {1983},
  volume = {217},
  pages = {337--44},
  number = {3},
  month = {Jul},
  keywords = {Cell Count, Species Specificity, Haplorhini, Neurons, Visual Cortex,
	Animals, Cats}
}

@MISC{becker_diplomathesis,
  author = {Becker, J.},
  title = {Ein {FPGA}-basiertes {T}estsystem f\"ur gemischt analog/digitale
	{ASIC}s},
  howpublished = {Diploma thesis (German), University of Heidelberg, HD-KIP-01-11},
  year = {2001},
  key = {becker_diplomathesis},
  keywords = {vision}
}

@article{bekolay2014nengo,
  title={Nengo: a Python tool for building large-scale functional brain models},
  author={Bekolay, Trevor and Bergstra, James and Hunsberger, Eric and DeWolf, Travis and Stewart, Terrence C and Rasmussen, Daniel and Choo, Xuan and Voelker, Aaron and Eliasmith, Chris},
  journal={Frontiers in Neuroinformatics},
  volume=7,
  pages=48,
  year=2014,
  publisher={Frontiers}
}

@INPROCEEDINGS{beiu97digital,
  author = {Beiu, V.},
  title = {Digitial integrated circuit implementations},
  booktitle = {The Handbook of Neural Computation},
  year = {1997},
  editor = {Fiesler, E. and Beale, R.},
  address = {New York},
  month = {January},
  publisher = {Institute of Physics Publishing and Oxford University Publishing},
  chapter = {E1.4},
  keywords = {vlsi}
}

@ARTICLE{beiu03vlsi,
  author = {Beiu, V.},
  title = {{VLSI} implementations of threshold logic -- a comprehensive survey},
  journal = {IEEE Transactions on Neural Networks},
  year = {2003},
  volume = {14},
  pages = {1217--1243},
  number = {5},
  file = {beiu03vlsi.pdf:beiu03vlsi.pdf:PDF},
  keywords = {vlsi}
}

@TECHREPORT{belew91evolving,
  author = {Belew, R. K. and McInerney, J. and Schraudolph, N. M.},
  title = {Evolving Networks: {U}sing the Genetic Algorithm with Connectionist
	Learning},
  institution = {Computer Science \& Engr. Dept. (C-104), Univ. of California},
  year = {1991},
  number = {CS90-174 (Revised)},
  address = {San Diego, La Jolla, CA 92093, USA},
  month = {February},
  file = {belew91evolving.pdf:belew91evolving.pdf:PDF},
  howpublished = {technical report}
}

@INPROCEEDINGS{Belhadj2008,
  author = {Belhadj, B. and Tomas, J. and Malot, O. and N'Kaoua, G. and Bornat,
	Y. and Renaud, S.},
  title = {FPGA-based architecture for real-time synaptic plasticity computation},
  booktitle = {Electronics, Circuits and Systems, 2008. ICECS 2008. 15th IEEE International
	Conference on},
  year = {2008},
  pages = {93 -96},
  month = {31 2008-sept. 3},
  doi = {10.1109/ICECS.2008.4674799},
  file = {:/home/simon/Dokumente/Visions/Shared/Publications/Papers/2008_Belhadj - FPGA-based architecture for real-time synaptic plasticity computation.pdf:PDF},
  keywords = {FPGA-based architecture;configurable architecture;dedicated plasticity
	processor;neural networks;neuromorphic system;real-time synaptic
	plasticity computation;spike timing-dependent plasticity;field programmable
	gate arrays;neural nets;},
  owner = {simon},
  timestamp = {2012.12.15}
}

@ARTICLE{bellec2020recurrently,
  abstract = {Recurrently connected networks of spiking neurons underlie the astounding information processing capabilities of the brain. Yet in spite of extensive research, how they can learn through synaptic plasticity to carry out complex network computations remains unclear. We argue that two pieces of this puzzle were provided by experimental data from neuroscience. A mathematical result tells us how these pieces need to be combined to enable biologically plausible online network learning through gradient descent, in particular deep reinforcement learning. This learning method--called e-prop--approaches the performance of backpropagation through time (BPTT), the best-known method for training recurrent neural networks in machine learning. In addition, it suggests a method for powerful on-chip learning in energy-efficient spike-based hardware for artificial intelligence.},
  author = {Bellec, Guillaume and Scherr, Franz and Subramoney, Anand and Hajek, Elias and Salaj, Darjan and Legenstein, Robert and Maass, Wolfgang},
  da = {2020/07/17},
  date-added = {2021-09-29 14:27:10 +0000},
  date-modified = {2021-09-29 14:27:10 +0000},
  doi = {10.1038/s41467-020-17236-y},
  id = {Bellec2020},
  isbn = {2041-1723},
  journal = {Nature Communications},
  number = {1},
  pages = {3625},
  title = {A solution to the learning dilemma for recurrent networks of spiking neurons},
  ty = {JOUR},
  url = {https://doi.org/10.1038/s41467-020-17236-y},
  volume = {11},
  year = {2020},
  bdsk-url-1 = {https://doi.org/10.1038/s41467-020-17236-y}
}

@BOOK{bellman61control,
  title = {Adaptive control processes},
  publisher = {Princeton University Press, Princeton, NJ},
  year = {1961},
  author = {R.E. Bellman},
  keywords = {learning},
  owner = {fieres}
}

@article{bengio2013bounding,
  title={Bounding the test log-likelihood of generative models},
  author={Bengio, Yoshua and Yao, Li},
  journal={arXiv preprint arXiv:1311.6184},
  year={2013}
}

@article{benjamin2014neurogrid,
  title={Neurogrid: A mixed-analog-digital multichip system for large-scale neural simulations},
  author={Benjamin, Ben Varkey and Gao, Peiran and McQuinn, Emmett and Choudhary, Swadesh and Chandrasekaran, Anand R and Bussat, Jean-Marie and Alvarez-Icaza, Rodrigo and Arthur, John V and Merolla, Paul A and Boahen, Kwabena},
  journal={Proceedings of the IEEE},
  volume={102},
  number={5},
  pages={699--716},
  year={2014},
  publisher={IEEE}
}

@article{benjamin2021neurogrid,
  title={Neurogrid simulates cortical cell-types, active dendrites, and top-down attention},
  author={Benjamin, Ben Varkey and Steinmetz, Nicholas A and Oza, Nick N and Aguayo, Jose J and Boahen, Kwabena},
  journal={Neuromorphic Computing and Engineering},
  volume=1,
  number=1,
  pages={013001},
  year=2021,
  publisher={IOP Publishing}
}

@book{bishop2006pattern,
  title={Pattern recognition and machine learning},
  author={Bishop, Christopher M and Nasrabadi, Nasser M},
  volume={1},
  year={2006},
  publisher={springer New York}
}

@book{bishop2009pattern,
  title={Pattern recognition and machine learning},
  author={Bishop, Christopher M},
  volume={1},
  year={2009},
  publisher={springer New York}
}

@MISC{rnum_homepage,
  author = {Olaf Trygve Berglihn},
  title = {{RNUM} Website},
  howpublished = {\url{http://rnum.rubyforge.org}},
  year = {2006}
}

@ARTICLE{bergman2008exascale,
    author={Bergman, Keren and Borkar, Shekhar and Campbell, Dan and Carlson, William and Dally, William and Denneau, Monty and Franzon, Paul and Harrod, William and Hill, Kerry and Hiller, Jon and others},
    title={ExaScale Computing Study: Technology Challenges in Achieving Exascale Systems},
    journal={Defense Advanced Research Projects Agency Information Processing Techniques Office (DARPA IPTO), Tech. Rep},
    volume={15},
    year={2008}
}

@Article{berkes2011spontaneous,
  Author         = {Berkes, Pietro and Orb{\'a}n, Gerg{\H{o}} and Lengyel,
                   M{\'a}t{\'e} and Fiser, J{\'o}zsef},
  Title          = {Spontaneous cortical activity reveals hallmarks of an
                   optimal internal model of the environment},
  Journal        = {Science},
  Volume         = {331},
  Number         = {6013},
  Pages          = {83--87},
  publisher      = {American Association for the Advancement of Science},
  year           = 2011
}

@MISC{waf,
    Author = {Thomas Nagy},
    Title = {waf: {T}he meta build system},
    Year = {2005},
    URL = {https://waf.io},
}

@ARTICLE{bertschinger03edgeofchaos,
  author = {Bertschinger, N. and Natschl{\"a}ger, T.},
  title = {Real-Time Computation at the Edge of Chaos in Recurrent Neural Networks},
  journal = {Neural Computation},
  year = {2004},
  volume = {16},
  pages = {1413 - 1436},
  number = {7},
  month = {July},
  file = {bertschinger03edgeofchaos.pdf:bertschinger03edgeofchaos.pdf:PDF}
}

@article{bengio2007scaling,
  title={Scaling learning algorithms towards AI},
  author={Bengio, Yoshua and LeCun, Yann},
  journal={Large-Scale Kernel Machines},
  volume={34},
  pages={1--41},
  year={2007}
}

@INPROCEEDINGS{bhanu_gecco02composite,
  author = {Bhanu, B. and Lin, Yingqiang},
  title = {Learning Composite Operators for Object Detection},
  booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference
	{GECCO} 2002},
  year = {2002},
  editor = {Langdon, W.B. and others},
  pages = {1003--1010},
  month = {July},
  publisher = {Morgan Kaufmann Publishers}
}

@ARTICLE{poo98stdp,
  author = {Bi, G. Q. and Poo, M. M.},
  title = {{Synaptic modifications in cultured hippocampal neurons: dependence
	on spike timing, synaptic strength, and postsynaptic cell type.}},
  journal = {The Journal of neuroscience : the official journal of the Society
	for Neuroscience},
  year = {1998},
  volume = {18},
  pages = {10464--10472},
  number = {24},
  month = dec,
  abstract = {{In cultures of dissociated rat hippocampal neurons, persistent potentiation
	and depression of glutamatergic synapses were induced by correlated
	spiking of presynaptic and postsynaptic neurons. The relative timing
	between the presynaptic and postsynaptic spiking determined the direction
	and the extent of synaptic changes. Repetitive postsynaptic spiking
	within a time window of 20 msec after presynaptic activation resulted
	in long-term potentiation (LTP), whereas postsynaptic spiking within
	a window of 20 msec before the repetitive presynaptic activation
	led to long-term depression (LTD). Significant LTP occurred only
	at synapses with relatively low initial strength, whereas the extent
	of LTD did not show obvious dependence on the initial synaptic strength.
	Both LTP and LTD depended on the activation of NMDA receptors and
	were absent in cases in which the postsynaptic neurons were GABAergic
	in nature. Blockade of L-type calcium channels with nimodipine abolished
	the induction of LTD and reduced the extent of LTP. These results
	underscore the importance of precise spike timing, synaptic strength,
	and postsynaptic cell type in the activity-induced modification of
	central synapses and suggest that Hebb's rule may need to incorporate
	a quantitative consideration of spike timing that reflects the narrow
	and asymmetric window for the induction of synaptic modification.}},
  address = {Department of Biology, University of California at San Diego, La
	Jolla, California 92093, USA.},
  day = {15},
  issn = {0270-6474},
  keywords = {plasticity, stdp},
  pmid = {9852584},
  posted-at = {2006-08-04 05:51:50},
  priority = {2},
  url = {http://www.jneurosci.org/content/18/24/10464.abstract}
}

@ARTICLE{bi02spatiotemporal,
  author = {Guo-Qiang Bi},
  title = {Spatiotemporal specificity of synaptic plasticity: cellular rules
	and mechanisms},
  journal = {Biol. Cybern.},
  year = {2002},
  volume = {87},
  pages = {319-332},
  number = {5-6},
  month = {December},
  abstract = {Recent experimental results on spike-timing-dependent plasticity (STDP)
	and heterosynaptic interaction in various systems have revealed new
	temporal and spatial properties of activity-dependent synaptic plasticity.
	These results challenge the conventional understanding of Hebb's
	rule and raise intriguing questions regarding the fundamental processes
	of cellular signaling. In this article, I review these new findings
	that lead to formulation of a new set of cellular rules. Emphasis
	is on evaluating potential molecular and cellular mechanisms that
	may underlie the spike-timing window of STDP and different patterns
	of heterosynaptic modifications. I also highlight several unresolved
	issues, and suggest future lines of research.},
  file = {bi02spatiotemporal.pdf:bi02spatiotemporal.pdf:PDF},
  owner = {mreuss},
  url = {http://www.springerlink.com/link.asp?id=ettuhaa7wrmwrx6t}
}

@ARTICLE{bienenstock1995,
  author = {Bienenstock, Elie},
  title = {A model of neocortex},
  journal = {Network: Computation in Neural Systems},
  year = {1995},
  volume = {6},
  pages = {179--224},
  number = {2},
  month = {May},
  citeulike-article-id = {3184551},
  keywords = {file-import-08-09-03, synfire-chain},
  posted-at = {2008-09-03 10:41:51},
  priority = {0}
}

@BOOK{bienenstock1982,
  title = {Theory for the development of neuron selectivity: orientation specificity
	and binocular interaction in visual cortex},
  publisher = {MIT Press},
  year = {1988},
  author = {Elie L. Bienenstock and Leon N. Cooper and Paul W. Munro},
  pages = {437--455},
  address = {Cambridge, MA, USA},
  book = {Neurocomputing: foundations of research},
  isbn = {0-262-01097-6}
}

@ARTICLE{bienenstock1982a,
  author = {Elie L. Bienenstock and Leon N. Cooper and Paul W. Munro},
  title = {Theory for the Development of Neuron Selectivity: Orientation Specificity
	and Binocular Interaction in Visual Cortex},
  journal = {Journal of Neuroscience},
  year = {1982},
  volume = {2},
  pages = {32-48},
  number = {2},
  biburl = {http://www.bibsonomy.org/bibtex/222190c5961a968c95a79d1d853727fbd/brian.mingus},
  description = {CCNLab BibTeX},
  keywords = {nnets }
}

@article{blott2019qutibench,
    title={{QuTiBench}: Benchmarking Neural Networks on Heterogeneous Hardware},
    author={Michaela Blott and Lisa Halder and Miriam Leeser and Linda Doyle},
    year={2019},
    eprint={1909.05009},
    archivePrefix={arXiv},
    primaryClass={cs.AR}
}

@BOOK{brun2012from,
  editor = {René Brun and Federico Carminati and Giuliana Galli Carminati},
  title = {{F}rom the {W}eb to the {G}rid and {B}eyond},
  publisher = {Springer, Berlin, Heidelberg},
  year = {2012}
}

@ARTICLE{pfeil2012arxiv,
  author = {{Pfeil}, T. and {Potjans}, T.~C. and {Schrader}, S. and {Potjans}, W. and {Schemmel}, J. and {Diesmann}, M. and {Meier}, K.},
  title = {{Is a 4-bit synaptic weight resolution enough? -- Constraints on enabling spike-timing dependent plasticity in neuromorphic hardware}},
  journal = {ArXiv e-prints},
  archivePrefix = "arXiv",
  eprint = {1201.6255},
  primaryClass = "q-bio.NC",
  keywords = {Quantitative Biology - Neurons and Cognition},
  year = 2012,
  month = jan,
  adsurl = {http://adsabs.harvard.edu/abs/2012arXiv1201.6255P},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{pfeil2013,
  crossref = {pfeil2013six}
}

@article{pfeil2013six,
  author  = {Pfeil, Thomas  and  Gr\"ubl, Andreas  and  Jeltsch, Sebastian  and  M\"uller, Eric  and  M\"uller, Paul  and  Petrovici, Mihai A.  and  Schmuker, Michael  and  Br\"uderle, Daniel  and  Schemmel, Johannes  and  Meier, Karlheinz},
  title   = {Six networks on a universal neuromorphic computing substrate},
  journal = {Frontiers in Neuroscience},
  volume  = {7},
  year    = {2013},
  pages   = {11},
  URL     = {http://www.frontiersin.org/neuromorphic\_engineering/10.3389/fnins.2013.00011/abstract},
  DOI     = {10.3389/fnins.2013.00011},
  ISSN    = {1662-453X}
}

@article{pfeil2014effect,
  title={The effect of heterogeneity on decorrelation mechanisms in spiking neural networks: a neuromorphic-hardware study},
  author={Pfeil, Thomas and Jordan, Jakob and Tetzlaff, Tom and Gr{\"u}bl, Andreas and Schemmel, Johannes and Diesmann, Markus and Meier, Karlheinz},
  journal={arXiv preprint arXiv:1411.7916},
  year={2014}
}

@MISC{probst11bachelor,
  author = {Probst, Dimitri},
  title = {Analysis of the Liquid Computing Paradigm on a Neuromorphic Hardware System},
  howpublished = {Bachelor thesis (English), University of Heidelberg, HD-KIP 11-47},
  year = {2011},
}

@ARTICLE{bill2010compensating,
  author = {Bill, Johannes and Schuch, Klaus and Br\"uderle, Daniel and Schemmel,
	Johannes and Maass, Wolfgang and Meier, Karlheinz},
  title = {Compensating inhomogeneities of neuromorphic {VLSI} devices via short-term
	synaptic plasticity},
  journal = {Front. Comp. Neurosci.},
  year = {2010},
  volume = {4},
  number = {129},
  abstract = {Recent developments in neuromorphic hardware engineering make mixed-signal
	VLSI neural network models promising candidates for neuroscientific
	research tools and massively parallel computing devices, especially
	for tasks which exhaust the computing power of software simulations.
	Still, like all analog hardware systems, neuromorphic models suffer
	from a constricted configurability and production-related fluctuations
	of device characteristics. Since also future systems, involving ever-smaller
	structures, will inevitably exhibit such inhomogeities on the unit
	level, self-regulation properties become a crucial requirement for
	their successful operation. By applying a cortically inspired self-adjusting
	network architecture, we show that the activity of generic spiking
	neural networks emulated on a neuromorphic hardware system can be
	kept within a biologically realistic firing regime and gain a remarkable
	robustness against transistor-level variations. As a first approach
	of this kind in engineering practice, the short-term synaptic depression
	and facilitation mechanisms implemented within an analog VLSI model
	of I&F neurons are functionally utilized for the purpose of network
	level stabilization. We present experimental data acquired both from
	the hardware model and from comparative software simulations which
	prove the applicability of the employed paradigm to neuromorphic
	VLSI devices.}
}

@article{bill2015ncsem,
    author = {Bill, Johannes AND Buesing, Lars AND Habenschuss, Stefan AND Nessler, Bernhard AND Maass, Wolfgang AND Legenstein, Robert},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {Distributed Bayesian Computation and Self-Organized Learning in Sheets of Spiking Neurons with Local Lateral Inhibition},
    year = {2015},
    month = {08},
    volume = {10},
    url = {https://doi.org/10.1371/journal.pone.0134356},
    pages = {1-51},
    abstract = {During the last decade, Bayesian probability theory has emerged as a framework in cognitive science and neuroscience for describing perception, reasoning and learning of mammals. However, our understanding of how probabilistic computations could be organized in the brain, and how the observed connectivity structure of cortical microcircuits supports these calculations, is rudimentary at best. In this study, we investigate statistical inference and self-organized learning in a spatially extended spiking network model, that accommodates both local competitive and large-scale associative aspects of neural information processing, under a unified Bayesian account. Specifically, we show how the spiking dynamics of a recurrent network with lateral excitation and local inhibition in response to distributed spiking input, can be understood as sampling from a variational posterior distribution of a well-defined implicit probabilistic model. This interpretation further permits a rigorous analytical treatment of experience-dependent plasticity on the network level. Using machine learning theory, we derive update rules for neuron and synapse parameters which equate with Hebbian synaptic and homeostatic intrinsic plasticity rules in a neural implementation. In computer simulations, we demonstrate that the interplay of these plasticity rules leads to the emergence of probabilistic local experts that form distributed assemblies of similarly tuned cells communicating through lateral excitatory connections. The resulting sparse distributed spike code of a well-adapted network carries compressed information on salient input features combined with prior experience on correlations among them. Our theory predicts that the emergence of such efficient representations benefits from network architectures in which the range of local inhibition matches the spatial extent of pyramidal cells that share common afferent input.},
    number = {8},
    doi = {10.1371/journal.pone.0134356}
}

@ARTICLE{binzegger07stereotypical,
  author = {Tom Binzegger and Rodney J Douglas and Kevan A C Martin},
  title = {Stereotypical bouton clustering of individual neurons in cat primary
	visual cortex},
  journal = {J Neurosci},
  year = {2007},
  volume = {27},
  pages = {12242--54},
  number = {45},
  month = {Nov},
  abstract = {In all species examined, with the exception of rodents, the axons
	of neocortical neurons form boutons in multiple separate clusters.
	Most descriptions of clusters are anecdotal, so here we developed
	an objective method for identifying clusters. We applied a mean-shift
	cluster-algorithm to three-dimensional reconstructions of 39 individual
	neurons and three thalamic afferents from the cat primary visual
	cortex. Both spiny (20 of 26) and smooth (7 of 13) neurons formed
	at least two distinct ellipsoidal clusters (range, 2-7). For all
	cell types, cluster formation is heterogenous, but is regulated so
	that cluster size and the number of boutons allocated to a cluster
	equalize with increasing number of clusters formed by a neuron. The
	bouton density within a cluster is inversely related to the spatial
	scale of the axon, resulting in a four times greater density for
	smooth neurons than for spiny neurons. Thus, the inhibitory action
	of the smooth neurons is much more concentrated and focal than the
	excitatory action of spiny neurons. The cluster with the highest
	number of boutons (primary cluster) was typically located around
	or above the soma of the parent neuron. The distance to the next
	cluster was proportional to the diameter of the primary cluster,
	suggesting that there is an optimal distance and spatial focus of
	the lateral influence of a neuron. The lateral spread of clustered
	axons may thus support a spoke-like network architecture that routes
	signals to localized sites, thereby reducing signal correlation and
	redundancy.},
  affiliation = {Institute of Neuroinformatics, University Z{\"u}rich and Swiss Federal
	Institute of Technology Z{\"u}rich, 8057 Z{\"u}rich, Switzerland.
	tom.binzegger@ncl.ac.uk}
}

@ARTICLE{binzegger04quantitative,
  author = {Tom Binzegger and Rodney J Douglas and Kevan A C Martin},
  title = {A quantitative map of the circuit of cat primary visual cortex},
  journal = {J Neurosci},
  year = {2004},
  volume = {24},
  pages = {8441--53},
  number = {39},
  month = {Sep},
  abstract = {We developed a quantitative description of the circuits formed in
	cat area 17 by estimating the "weight" of the projections between
	different neuronal types. To achieve this, we made three-dimensional
	reconstructions of 39 single neurons and thalamic afferents labeled
	with horseradish peroxidase during intracellular recordings in vivo.
	These neurons served as representatives of the different types and
	provided the morphometrical data about the laminar distribution of
	the dendritic trees and synaptic boutons and the number of synapses
	formed by a given type of neuron. Extensive searches of the literature
	provided the estimates of numbers of the different neuronal types
	and their distribution across the cortical layers. Applying the simplification
	that synapses between different cell types are made in proportion
	to the boutons and dendrites that those cell types contribute to
	the neuropil in a given layer, we were able to estimate the probable
	source and number of synapses made between neurons in the six layers.
	The predicted synaptic maps were quantitatively close to the estimates
	derived from the experimental electron microscopic studies for the
	case of the main sources of excitatory and inhibitory input to the
	spiny stellate cells, which form a major target of layer 4 afferents.
	The map of the whole cortical circuit shows that there are very few
	"strong" but many "weak" excitatory projections, each of which may
	involve only a few percentage of the total complement of excitatory
	synapses of a single neuron.},
  affiliation = {Institute of Neuroinformatics, University of Z{\"u}rich, and Eidgen{\"o}ssische
	Technische Hochschule Z{\"u}rich, CH-8057 Z{\"u}rich, Switzerland.
	tom.binzegger@ncl.ac.uk}
}

@BOOK{bishop95pattern,
  title = {Neural Networks for Pattern Recognition},
  publisher = {Oxford University Press Inc., New York},
  year = {1995},
  author = {Bishop, Christopher M.},
  address = {Walton Street, Oxford},
  isbn = {0-19-853849-9},
  keywords = {NN}
}

@TECHREPORT{blickle95comparison,
  author = {Blickle, Tobias and Thiele, Lothar },
  title = {A Comparison of Selection Schemes Used in Genetic Algorithms},
  institution = {Computer Engineering and Communications Networks Lab (TIK), Swiss
	Federal Institute of Technology (ETH)},
  year = {1995},
  number = {11},
  address = {Gloriastrasse 35, 8092 Zurich, Switzerland},
  file = {blickle95comparison.pdf:blickle95comparison.pdf:PDF}
}

@ARTICLE{block62perceptron,
  author = {Block, H. D.},
  title = {The Perceptron: a model for brain functioning},
  journal = {Reviews of Modern Physics},
  year = {1962},
  volume = {34},
  pages = {123--135}
}

@ARTICLE{blum91approx,
  author = {Blum, E. K. and Li, L. K.},
  title = {Approximation theory and feedforward networks},
  journal = {Neural Networks},
  year = {1991},
  volume = {4},
  pages = {511--515},
  number = {4}
}

@article{blundell2018code,
  title={Code Generation in Computational Neuroscience: A Review of Tools and Techniques},
	AUTHOR={Blundell, Inga and Brette, Romain and Cleland, Thomas A. and Close, Thomas G. and Coca, Daniel and Davison, Andrew P. and Diaz-Pier, Sandra and Fernandez Musoles, Carlos and Gleeson, Padraig and Goodman, Dan F. M. and Hines, Michael and Hopkins, Michael W. and Kumbhar, Pramod and Lester, David R. and Marin, Bóris and Morrison, Abigail and Müller, Eric and Nowotny, Thomas and Peyser, Alexander and Plotnikov, Dimitri and Richmond, Paul and Rowley, Andrew and Rumpe, Bernhard and Stimberg, Marcel and Stokes, Alan B. and Tomkins, Adam and Trensch, Guido and Woodman, Marmaduke and Eppler, Jochen Martin},
  journal={Frontiers in Neuroinformatics},
  volume=12,
  pages=68,
  year=2018,
	URL={https://www.frontiersin.org/article/10.3389/fninf.2018.00068},
  DOI={10.3389/fninf.2018.00068},
	ISSN={1662-5196},
  publisher={Frontiers}
}

@ARTICLE{boegerhausen03modeling,
  author = {Malte Boegershausen and Pascal Suter and Shih-Chii Liu},
  title = {Modeling Short-Term Synaptic Depression in Silicon},
  journal = {Neural Computation},
  year = {2003},
  volume = {15},
  pages = {331-348},
  number = {2}
}

@ARTICLE{deboer1968,
  author = {de Boer, R. and Kuyper, P. },
  title = {Triggered correlation.},
  journal = {IEEE Trans Biomed Eng},
  year = {1968},
  volume = {15},
  pages = {169--179},
  number = {3},
  month = {July},
  citeulike-article-id = {802012},
  issn = {0018-9294},
  keywords = {project2},
  posted-at = {2006-08-15 12:28:58},
  priority = {2},
  url = {http://view.ncbi.nlm.nih.gov/pubmed/5667803}
}

@MISC{bofill-circuits,
  author = {Adria Bofill and Alan F. Murray and Damon P. Thompson},
  title = {Circuits for {VLSI} Implementation of Temporally-Asymmetric Hebbian
	Learning},
  year = {2002},
  citeseercitationcount = {0},
  citeseerurl = {http://citeseer.ist.psu.edu/541205.html},
  keywords = {spiking learning}
}

@ARTICLE{bothe07reducing,
  author = {Bohte, Sander M. and Mozer, Michael C.},
  title = {{Reducing the Variability of Neural Responses: A Computational Theory
	of Spike-Timing-Dependent Plasticity}},
  journal = {Neural Comp.},
  year = {2007},
  volume = {19},
  pages = {371-403},
  number = {2},
  abstract = {Experimental studies have observed synaptic potentiation when a presynaptic
	neuron fires shortly before a postsynaptic neuron and synaptic depression
	when the presynaptic neuron fires shortly after. The dependence of
	synaptic modulation on the precise timing of the two action potentials
	is known as spike-timing dependent plasticity (STDP). We derive STDP
	from a simple computational principle: synapses adapt so as to minimize
	the postsynaptic neuron's response variability to a given presynaptic
	input, causing the neuron's output to become more reliable in the
	face of noise. Using an objective function that minimizes response
	variability and the biophysically realistic spike-response model
	of Gerstner (2001), we simulate neurophysiological experiments and
	obtain the characteristic STDP curve along with other phenomena,
	including the reduction in synaptic plasticity as synaptic efficacy
	increases. We compare our account to other efforts to derive STDP
	from computational principles and argue that our account provides
	the most comprehensive coverage of the phenomena. Thus, reliability
	of neural response in the face of noise may be a key goal of unsupervised
	cortical adaptation.},
  eprint = {http://neco.mitpress.org/cgi/reprint/19/2/371.pdf},
  keywords = {plasticity},
  url = {http://neco.mitpress.org/cgi/content/abstract/19/2/371}
}

@MISC{bohte-unsupervised,
  author = {Sander M. Bohte and Han La Poutr{\'e} and Joost N. Kok},
  title = {Unsupervised Clustering with Spiking Neurons by Sparse Temporal Coding
	and Multi-Layer RBF Networks},
  year = {2001},
  citeseercitationcount = {0},
  citeseerurl = {http://citeseer.ist.psu.edu/535874.html},
  keywords = {spiking learning}
}

@INPROCEEDINGS{bontorin2007realtime,
  author = {Bontorin, G. and Renaud, S. and Garenne, A. and Alvado, L. and Le
	Masson, G. and Tomas, J.},
  title = {A Real-Time Closed-Loop Setup for Hybrid Neural Networks},
  booktitle = {Proceedings of the 29th Annual International Conference of the IEEE
	Engineering in Medicine and Biology Society (EMBS2007)},
  year = {2007},
  keywords = {neuromorphic},
  owner = {bruederl},
  timestamp = {2008.09.02}
}

@ARTICLE{Borkar2011,
  author = {Borkar, Shekhar and Chien, Andrew A},
  title = {The future of microprocessors},
  journal = {Communications of the ACM},
  year = {2011},
  volume = {54},
  pages = {67--77},
  number = {5},
  owner = {simon},
  publisher = {ACM},
  timestamp = {2013.05.07}
}

@ARTICLE{bornholdt92asymmetric,
  author = {Bornholdt, S. and Graudez, D.},
  title = {General asymmetric neural networks and structure design by genetic
	algorithms},
  journal = {Neural Networks},
  year = {1992},
  volume = {5},
  pages = {327--334},
  number = {1}
}

@ARTICLE{boser_jssc91,
  author = {Boser and others},
  title = {An Analog Neural Network Processor with Programmable Topology.},
  journal = {IEEE Journal of Solid-State Circuits},
  year = {1991},
  volume = {26},
  pages = {2017--2025},
  month = Dec,
  annote = {neural network a little bit similar to hagen, per synapse dacs},
  file = {boser_jssc91.pdf:boser_jssc91.pdf:PDF},
  groupsearch = {0},
  keywords = {vlsi, convolutional, convolutional NN}
}

@MISC{bothe-implementing,
  author = {Sander M. Bothe and et al.},
  title = {Implementing Position-Invariant Detection of Feature-Conjunctions
	in a Network of Spiking Neurons},
  citeseercitationcount = {0},
  citeseerurl = {citeseer.nj.nec.com/bothe02implementing.html},
  keywords = {spiking}
}

@inproceedings{bothe2000, crossref = {bohte2000spikeprop} }
@inproceedings{bothe2000spikeprop, crossref = {bohte2000spikeprop} }
@inproceedings{bohte2000spikeprop,
  author = {Sander M. Bohte and Joost N. Kok and Han La Poutr{\'e}},
  title = {{SpikeProp}: backpropagation for networks of spiking neurons},
  booktitle={ESANN 2000, 8th European Symposium on Artificial Neural Networks, Brugres, Belgium, April},
  year={2000},
  keywords = {spiking}
}

@ARTICLE{elboustani07activated,
  author = {Sami El Boustani and Martin Pospischil and Michelle Rudolph-Lilith
	and Alain Destexhe},
  title = {Activated cortical states: experiments, analyses and models},
  journal = {Journal of Physiology (Paris)},
  year = {2007},
  volume = {101},
  pages = {99-109},
  owner = {bruederl},
  timestamp = {2008.07.18}
}

@BOOK{bower1998genesis,
  title = {The Book of GENESIS: Exploring Realistic Neural Models with the {GE}neral
	{NE}ural {SI}mulation {S}ystem (Second edition)},
  publisher = {Springer-Verlag, New York},
  year = {1998},
  author = {J. M. Bower and D. Beeman},
  isbn = {0387949380},
  owner = {bruederl},
  timestamp = {2008.09.04}
}

@PHDTHESIS{bruederle09phd,
  author = {Daniel Br{\"u}derle},
  title = {Neuroscientific Modeling with a Mixed-Signal VLSI Hardware System},
  year = {2009},
  school = {Ruprecht-Karls-Universit{\"a}t Heidelberg},
}

@PHDTHESIS{friedmann13phd,
  author = {Simon Friedmann},
  title = {A New Approach to Learning in Neuromorphic Hardware},
  year = {2013},
  school = {Ruprecht-Karls-Universit{\"a}t Heidelberg},
}

@MISC{bruederle2008,
  author = {Daniel Br\"uderle},
  title = {personal communication},
  year = {2008},
  address = {Kirchhoff Institut f{\"u}r Physik, Universit{\"a}t Heidelberg, Deutschland}
}

@INPROCEEDINGS{bruederle10simulator,
  author = {Br{\"u}derle, Daniel and Bill, Johannes and Kaplan, Bernhard and Kremkow,
	Jens and Meier, Karlheinz and M{\"u}ller, Eric and Schemmel, Johannes},
  title = {Simulator-Like Exploration of Cortical Network Architectures with
	a Mixed-Signal VLSI System},
  booktitle = {Proceedings of the 2010 IEEE International Symposium on Circuits
	and Systems (ISCAS)},
  year = {2010},
  pages = {2784--2787}
}

@MISC{bruederle2010capocaccia,
    author = {Br{\"u}derle, Daniel and Petrovici, Mihai A. and Jeltsch, Sebastian and Vogginger, Bernhard and Friedmann, Simon and Schmuker, Michael and Kremkow, Jens and Clayton, Thomas and Petkov, Venelin and Bill, Johannes and Albert, Marvin and Hartel, Andreas and Partzsch, Johannes and M{\"u}ller, Eric and Muller, Lyle and Bichler, Olivier and Schemmel, Johannes and Meier, Karlheinz},
    title = {Simulator-Like Exploration of Network Architectures with the FACETS Hardware Systems and PyNN},
    url = {http://www.kip.uni-heidelberg.de/cms/groups/vision/galleries_media/cne2010_experiments/},
    year = {2010}
}

@INPROCEEDINGS{bruederle_iwann07,
  author = {D. Br\"uderle and A. Gr\"ubl and K. Meier and E. Muller and J. Schemmel},
  title = {A Software Framework for Tuning the Dynamics of Neuromorphic Silicon
	Towards Biology},
  booktitle = {Proceedings of the 2007 International Work-Conference on Artificial
	Neural Networks (IWANN'07)},
  year = {2007},
  volume = {LNCS 4507},
  pages = {479-486},
  publisher = {Springer Verlag},
  howpublished = {Lecture Notes in Computer Science},
  key = {bruederle_07},
  keywords = {vision,spikey,hardware, neuromorphic}
}

@ARTICLE{bruederle_ijcir06,
  author = {Br\"uderle, D. and Sunat, K. and Chiewchanwattana, S. and Lursinsap,
	C. and Siripant, S.},
  title = {Achieving Compatible Numeral Handwriting Recognition Rate by a Simple
	Activation Function},
  journal = {International Journal of Computational Intelligence Research},
  year = {2006},
  volume = {2},
  pages = {1--9},
  number = {1}
}

@MISC{RFC1122,
  author = {R. T. Braden},
  title = {{RFC 1122}: Requirements for {Internet} hosts --- communication layers},
  month = oct,
  year = {1989},
  format = {TXT=295992 bytes},
  online = {yes},
  status = {STANDARD},
  url = {ftp://ftp.internic.net/rfc/rfc1122.txt}
}

@MISC{brainscales_d3-31,
  author = {UHEI and TUD},
  title = {Implement the {FPGA} firmware for routing of the Layer 2 network},
  howpublished = {{BrainScaleS} Deliverable D3-3.1},
  year = {2011},
  keywords = {brainscales, deliverable}
}

@MISC{brainscales_webpublic,
  author = {{BrainScaleS}},
  title = {Research Project},
  howpublished = {\url{http://brainscales.kip.uni-heidelberg.de/public/index.html}},
  year = 2012
}

@MISC{facets_webpublic,
  author = {{FACETS}},
  title = {Research Project},
  howpublished = {\url{http://http://facets.kip.uni-heidelberg.de/}},
  year = 2010
}

@BOOK{braitenberg91anatomy,
  title = {Anatomy of the Cortex: Statistics and Geometry},
  year = {1991},
  author = {Valentin Braitenberg and Almut Sch\"uz}
}

@INPROCEEDINGS{branke95evolutionary,
  author = {Branke, J.},
  title = {Evolutionary Algorithms in Neural Network Design and Training --
	{A} Review},
  booktitle = {{P}roceedings of the {F}irst {N}ordic {W}orkshop on {G}enetic {A}lgorithms
	and their {A}pplications (1{NWGA})},
  year = {1995},
  editor = {Jarmo T. Alander},
  number = {95-1},
  pages = {145--163},
  address = {Vaasa, Finnland},
  file = {branke95evolutionary.pdf:branke95evolutionary.pdf:PDF}
}

@article{brascamp2006time,
  title={The time course of binocular rivalry reveals a fundamental role of noise},
  author={Brascamp, Jan W and Van Ee, Raymond and Noest, Andre J and Jacobs, Richard HAH and van den Berg, Albert V},
  journal={Journal of vision},
  volume={6},
  number={11},
  pages={8},
  year={2006},
  publisher={Association for Research in Vision and Ophthalmology}
}

@ARTICLE{brecht2002dynamic,
  author = {Michael Brecht and Bert Sakmann},
  title = {Dynamic representation of whisker deflection by synaptic potentials
	in spiny stellate and pyramidal cells in the barrels and septa of
	layer 4 rat somatosensory cortex},
  journal = {J Physiol (Lond)},
  year = {2002},
  volume = {543},
  pages = {49--70},
  number = {1},
  month = {Aug},
  abstract = {Whole-cell voltage recordings were made in vivo from excitatory neurons
	(n = 23) in layer 4 of the barrel cortex in urethane-anaesthetised
	rats. Their receptive fields (RFs) for a brief whisker deflection
	were mapped, the position of the cell soma relative to barrel borders
	was determined for 15 cells and dendritic and axonal arbors were
	reconstructed for all cells. Three classes of neurons were identified:
	spiny stellate cells and pyramidal cells located in barrels and pyramidal
	cells located in septa. Dendritic and, with some exceptions, axonal
	arborisations of barrel cells were mostly restricted to the borders
	of a column with a cross sectional area of a barrel, defining a cytoarchitectonic
	barrel-column. Dendrites and axons of septum cells, in contrast,
	mostly extended across barrel borders. The subthreshold RFs measured
	by evoked postsynaptic potentials (PSPs) comprised a principal whisker
	(PW) and several surround whiskers (SuWs) indicating that deflection
	of a single whisker is represented in multiple barrels and septa.
	Barrel cells responded with larger depolarisation to stimulation
	of the PW (13.7 +/- 4.6 mV (mean +/- S.D.), n = 10) than septum cells
	(5.7 +/- 2.4 mV, n = 5), the gradient between peak responses to PW
	and SuW deflection was steeper and the latency of depolarisation
	onset was shorter (8 +/- 1.4 ms vs. 11 +/- 2 ms). In barrel cells
	the response onset and the peak to SuW deflection was delayed depending
	on the distance to the PW thus indicating that the spatial representation
	of a single whisker deflection in the barrel map is dynamic and varies
	on the scale of milliseconds to tens of milliseconds. Septum cells
	responded later and with comparable latencies to PW and SuW stimulation.
	Spontaneous (0.053 +/- 0.12 action potentials (APs) s(-1)) and evoked
	APs (0.14 +/- 0.29 APs per principal whisker (PW) stimulus) were
	sparse. We conclude that PSPs in ensembles of barrel cells represent
	dynamically the deflection of a single whisker with high temporal
	and spatial acuity, initially by the excitation in a single PW-barrel
	followed by multi-barrel excitation. This presumably reflects the
	divergence of thalamocortical projections to different barrels. Septum
	cell PSPs preferably represent multiple whisker deflections, but
	less dynamically and with less spatial acuity.},
  keywords = {Cell Size, Male, Brain Mapping, Rats, Somatosensory Cortex, Rats:
	Wistar, Dendrites, Vibrissae, Pyramidal Cells, Animals, Patch-Clamp
	Techniques, Neural Inhibition, Female, Axons}
}

@INPROCEEDINGS{breidenassel04flexible,
  author = {Breidenassel, A. and Meier, K. and Schemmel, J.},
  title = {A flexible scheme for adaptive integration time control},
  booktitle = {Proceedings of the Third IEEE Conference on Sensors},
  year = {2004},
  pages = {280--283},
  address = {Vienna},
  month = {October},
  isbn = {0-7803-8693-0},
  keywords = {vision}
}

@MASTERSTHESIS{breitwieser2011bachelorthesis,
  author = {Breitwieser, Oliver},
  title = {Investigation of a Cortical Attractor-Memory Network},
  year = 2011,
  type = {Bachelor thesis},
  school = {Ruprecht-Karls-Universit{\"a}t Heidelberg},
  note = {HD-KIP 11-173},
}

@MASTERSTHESIS{breitwieser2015masterthesis,
  author = {Breitwieser, Oliver},
  title = {Towards a Neuromorphic Implementation of Spike-Based Expectation Maximization},
  year = 2015,
  type = {Master thesis},
  school = {Ruprecht-Karls-Universit{\"a}t Heidelberg}
}

@MASTERSTHESIS{ilmberger2017masterthesis,
  author = {Ilmberger, Joscha},
  title = {Development of a digitizer for the BrainScaleS neuromorphic hardware system},
  year = 2017,
  type = {Master thesis},
  school = {Ruprecht-Karls-Universit{\"a}t Heidelberg}
}

@ARTICLE{breitbach2020devops,
	author = {Breitwieser, Oliver Julien and Baumbach, Andreas and Stradmann, Yannik and Schmitt, Sebastian and M{\"u}ller, Eric and Schemmel, Johannes},
	title = {DevOps for the {BrainScaleS} Neuromorphic Hardware Platforms},
	year = 2020,
	journal={arXiv preprint},
	url={TODO}
}

@article{brette_05, crossref = {brette2005adaptive} }
@ARTICLE{brette2005adaptive,
  author = {Brette, R. and Gerstner, W.},
  title = {Adaptive Exponential Integrate-and-Fire Model as an Effective Description
	of Neuronal Activity},
  journal = {J. Neurophysiol.},
  year = {2005},
  volume = {94},
  pages = {3637 -- 3642},
  abstract = {We introduce a two-dimensional integrate-and-fire model that combines
	an exponential spike mechanism with an adaptation equation, based
	on recent theoretical findings. We describe a systematic method to
	estimate its parameters with simple electrophysiological protocols
	(current-clamp injection of pulses and ramps) and apply it to a detailed
	conductance-based model of a regular spiking neuron. Our simple model
	predicts correctly the timing of 96\% of the spikes (±2 ms) of the
	detailed model in response to injection of noisy synaptic conductances.
	The model is especially reliable in high-conductance states, typical
	of cortical activity in vivo, in which intrinsic conductances were
	found to have a reduced role in shaping spike trains. These results
	are promising because this simple model has enough expressive power
	to reproduce qualitatively several electrophysiological classes described
	in vitro.},
  affiliation = {EPFL},
  details = {http://infoscience.epfl.ch/record/97829},
  documenturl = {http://infoscience.epfl.ch/getfile.py?mode=best&recid=97829},
  doi = {10.1152/jn.00686.2005},
  oai-id = {oai:infoscience.epfl.ch:97829},
  oai-set = {article; fulltext},
  review = {REVIEWED},
  status = {PUBLISHED},
  unit = {LCN}
}

@MISC{brianhomepage,
  author = {Romain Brette and Dan Goodman},
  title = {Brian},
  year = {2008},
  note = {A simulator for spiking neural networks based on {Python}},
  url = {\url{http://brian.di.ens.fr}}
}

@ARTICLE{brette06simulators,
  crossref = {brette2007simulation}
}

@ARTICLE{brette2007simulation,
  author = {R. Brette and M. Rudolph and T. Carnevale and M. Hines and D. Beeman
	and J.~M. Bower and M. Diesmann and A. Morrison and P.~H. Goodman
	and F.~C. {Harris Jr} and M. Zirpe and T. Natschlager and D. Pecevski
	and B. Ermentrout and M. Djurfeldt and A. Lansner and O. Rochel and
	T. Vieville and E. Muller and A.~P. Davison and S.~El Boustani and
	A. Destexhe},
  title = {Simulation of networks of spiking neurons: A review of tools and
	strategies},
  journal = {Journal of Computational Neuroscience},
  year = {2007},
  volume = {23},
  pages = {349-398},
  number = {3},
  month = {December},
  abstract = {We review different aspects of the simulation of spiking neural networks.
	We start by reviewing the different types of simulation strategies
	and algorithms that are currently implemented. We next review the
	precision of those simulation strategies, in particular in cases
	where plasticity depends on the exact timing of the spikes. We overview
	different simulators and simulation environments presently available
	(restricted to those freely available, open source and documented).
	For each simulation tool, its advantages and pitfalls are reviewed,
	with an aim to allow the reader to identify which simulator is appropriate
	for a given task. Finally, we provide a series of benchmark simulations
	of different types of networks of spiking neurons, including Hodgkin-Huxley
	type, integrate-and-fire models, interacting with current-based or
	conductance-based synapses, using clock-driven or event-driven integration
	strategies. The same set of models are implemented on the different
	simulators, and the codes are made available. The ultimate goal of
	this review is to provide a resource to facilitate identifying the
	appropriate integration strategy and simulation tool to use for a
	given modeling problem related to spiking neural networks.}
}

@ARTICLE{briggmann06towards,
  author = {Briggman, Kevin L. and Denk, Winfried },
  title = {Towards neural circuit reconstruction with volume electron microscopy
	techniques},
  journal = {Current Opinion in Neurobiology},
  year = {2006},
  volume = {16},
  pages = {562--570},
  number = {5},
  month = {October},
  abstract = {Electron microscopy is the only currently available technique with
	a resolution adequate to identify and follow every axon and dendrite
	in dense neuropil. Reconstructions of large volumes of neural tissue,
	necessary to reconstruct even local neural circuits, have, however,
	been inhibited by the daunting task of serially sectioning and reconstructing
	thousands of sections. Recent technological developments have improved
	the quality of volume electron microscopy data and automated its
	acquisition. This opens up the prospect of reconstructing almost
	complete invertebrate and sizable fractions of vertebrate nervous
	systems. Such reconstructions of complete neural wiring diagrams
	could rekindle the tradition of relating neural function to the underlying
	neuroanatomical circuitry.},
  booktitle = {Neuronal and glial cell biology / New technologies},
  keywords = {alg}
}

@ARTICLE{bringuier1999,
  author = {Bringuier, Vincent and Chavane, Frédéric and Glaeser, Larry and Frégnac,
	Yves},
  title = {Horizontal Propagation of Visual Activity in the Synaptic Integration
	Field of Area 17 Neurons},
  journal = {Science},
  year = {1999},
  volume = {283},
  pages = {695-699},
  number = {5402},
  doi = {10.1126/science.283.5402.695},
  eprint = {http://www.sciencemag.org/content/283/5402/695.full.pdf},
  url = {http://www.sciencemag.org/content/283/5402/695.abstract}
}

@article{brown1975, crossref = {brown1975electric} }
@article {brown1975electric,
  author = {Brown, A. G.},
  title = {ELECTRIC CURRENT FLOW IN EXCITABLE CELLS. By J. J. B. Jack, D. Noble, R. W. Tsien. Clarendon Press, Oxford, 1975. Pp. xv+502. £18},
  journal = {Quarterly Journal of Experimental Physiology and Cognate Medical Sciences},
  volume = {61},
  number = {1},
  issn = {1469-445X},
  url = {http://dx.doi.org/10.1113/expphysiol.1976.sp002339},
  doi = {10.1113/expphysiol.1976.sp002339},
  pages = {75--75},
  year = {1976},
}

@article{brown2020language,
    title={Language Models are Few-Shot Learners},
    author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
    year={2020},
    eprint={2005.14165},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@ARTICLE{brunel1998firingfrequency,
  author = {Brunel, Nicolas and Sergi, Simone},
  title = {Firing Frequency of Leaky Integrate-and-Fire Neurons with Synaptic Current Dynamics},
  journal = {Journal of Theoretical Biology},
  year = {1998},
  volume = {195},
  pages = {87-95},
}

@ARTICLE{brunel_jcns2000,
  author = {Brunel, N.},
  title = {Dynamics of sparsely connected networks of excitatory and inhibitory
	spiking neurons},
  journal = {Journal of Computational Neuroscience},
  year = {2000},
  volume = {8},
  pages = {183--208},
  number = {3},
  abstract = {The dynamics of networks of sparsely connected excitatory and inhibitory
	integrate-and-fire neurons are studied analytically. The analysis
	reveals a rich repertoire of states, including synchronous states
	in which neurons fire regularly; asynchronous states with stationary
	global activity and very irregular individual cell activity; and
	states in which the global activity oscillates but individual cells
	fire irregularly, typically at rates lower than the global oscillation
	frequency. The network can switch between these states, provided
	the external frequency, or the balance between excitation and inhibition,
	is varied. Two types of network oscillations are observed. In the
	fast oscillatory state, the network frequency is almost fully controlled
	by the synaptic time scale. In the slow oscillatory state, the network
	frequency depends mostly on the membrane time constant. Finite size
	effects in the asynchronous state are also discussed.},
  date-modified = {2007-03-01 17:43:57 +0100},
  keywords = {Action Potentials; Biological Clocks; Brain; Cortical Synchr; Electric
	Stimulation; Excitatory Postsynaptic Potentials; Hippocampus; Interneurons;
	Linear Models; Models, Neurological; Nerve Net; Neural Inhibition;
	Neural Pathways; Neurons; Pyramidal Cells; Synapses; Time Factors;}
}

@article{brunel2001effects,
  title={Effects of neuromodulation in a cortical network model of object working memory dominated by recurrent inhibition},
  author={Brunel, Nicolas and Wang, Xiao-Jing},
  journal={Journal of computational neuroscience},
  volume={11},
  number={1},
  pages={63--85},
  year={2001},
  publisher={Springer}
}

@ARTICLE{bruno2006cortex,
  author = {Bruno, Randy M. and Sakmann, Bert},
  title = {{Cortex Is Driven by Weak but Synchronously Active Thalamocortical
	Synapses}},
  journal = {Science},
  year = {2006},
  volume = {312},
  pages = {1622-1627},
  number = {5780},
  abstract = {Sensory stimuli reach the brain via the thalamocortical projection,
	a group of axons thought to be among the most powerful in the neocortex.
	Surprisingly, these axons account for only [~]15% of synapses onto
	cortical neurons. The thalamocortical pathway might thus achieve
	its effectiveness via high-efficacy thalamocortical synapses or via
	amplification within cortical layer 4. In rat somatosensory cortex,
	we measured in vivo the excitatory postsynaptic potential evoked
	by a single synaptic connection and found that thalamocortical synapses
	have low efficacy. Convergent inputs, however, are both numerous
	and synchronous, and intracortical amplification is not required.
	Our results suggest a mechanism of cortical activation by which thalamic
	input alone can drive cortex.}
}

@article{brunel2007lapicque,
  title={Lapicque’s 1907 paper: from frogs to integrate-and-fire},
  author={Brunel, Nicolas and Van Rossum, Mark CW},
  journal={Biological cybernetics},
  volume={97},
  number={5-6},
  pages={337--339},
  year={2007},
  publisher={Springer}
}

@MISC{facets_d7-1,
  author = {Daniel Br{\"u}derle},
  title = {Complete the software interface to the {S}tage 1 system to make it
	accessible by the common {FACETS} simulator {API}},
  howpublished = {{FACETS} Deliverable D7-1},
  year = {2007},
  keywords = {facets, deliverable},
  owner = {mueller},
  timestamp = {2008.08.22}
}

@MISC{facets_m7-1,
  author = {Daniel Br{\"u}derle},
  title = {Evaluate the implementation of biologically realistic networks within
	the event-based routing framework of the Stage 1 system using benchmark
	experiments},
  howpublished = {{FACETS} Milestone M7-1},
  year = {2007},
  keywords = {facets, deliverable},
  owner = {bruederle}
}

@MISC{bruederle04diplomathesis,
  author = {Br{\"u}derle, Daniel},
  title = {Implementing Spike-Based Computation on a Hardware Perceptron},
  howpublished = {Diploma thesis (English), University of Heidelberg, HD-KIP-04-16},
  year = {2004},
  keywords = {vision, perceptron, hagen, liquid, spike-based}
}

@ARTICLE{bruederle2009pyhal,
	AUTHOR={Brüderle, Daniel and Müller, Eric and Davison, Andrew and Muller, Eilif and Schemmel, Johannes and Meier, Karlheinz},
	TITLE={Establishing a novel modeling tool: a python-based interface for a neuromorphic hardware system},
	JOURNAL={Frontiers in Neuroinformatics},
	VOLUME={3},
	PAGES={17},
	YEAR={2009},
	URL={https://www.frontiersin.org/article/10.3389/neuro.11.017.2009},
	DOI={10.3389/neuro.11.017.2009},
	ISSN={1662-5196},
	ABSTRACT={Neuromorphic hardware systems provide new possibilities for the neuroscience modeling community. Due to the intrinsic parallelism of the micro-electronic emulation of neural computation, such models are highly scalable without a loss of speed. However, the communities of software simulator users and neuromorphic engineering in neuroscience are rather disjoint. We present a software concept that provides the possibility to establish such hardware devices as valuable modeling tools. It is based on the integration of the hardware interface into a simulator-independent language which allows for unified experiment descriptions that can be run on various simulation platforms without modification, implying experiment portability and a huge simplification of the quantitative comparison of hardware and simulator results. We introduce an accelerated neuromorphic hardware device and describe the implementation of the proposed concept for this system. An example setup and results acquired by utilizing both the hardware system and a software simulator are demonstrated.}
}

@ARTICLE{bruederle09establishing_hack,
  author = {Br{\"u}derle, Daniel and M{\"u}ller, Eric and Davison, Andrew and
	Muller, Eilif and Schemmel, Johannes and Meier, Karlheinz},
  title = {Establishing a Novel Modeling Tool: {A} Python-based Interface for
	a Neuromorphic Hardware System},
  journal = {Front. Neuroinform.},
  year = {2009},
  volume = {3},
  number = {17}
}

@ARTICLE{bruderle2011comprehensive, crossref = {bruederle2011comprehensive}}
@ARTICLE{bruederle_biolcybern2010, crossref = {bruederle2011comprehensive}}
@ARTICLE{bruederle2011comprehensive,
  author = {Br{\"u}derle, Daniel and Petrovici, Mihai A. and Vogginger, Bernhard and
	Ehrlich, Matthias and Pfeil, Thomas and Millner, Sebastian and Gr{\"u}bl,
	Andreas and Wendt, Karsten and M{\"u}ller, Eric and Schwartz, Marc-Olivier
	and de Oliveira, Dan and Jeltsch, Sebastian and Fieres, Johannes
	and Schilling, Moritz and M{\"u}ller, Paul and Breitwieser, Oliver and
	Petkov, Venelin and Muller, Lyle and Davison, Andrew and Krishnamurthy,
	Pradeep and Kremkow, Jens and Lundqvist, Mikael and Muller, Eilif
	and Partzsch, Johannes and Scholze, Stefan and Z{\"u}hl, Lukas and Mayr,
	Christian and Destexhe, Alain and Diesmann, Markus and Potjans, Tobias
	and Lansner, Anders and Sch{\"u}ffny, Ren{\'e} and Schemmel, Johannes and
	Meier, Karlheinz},
  title = {A comprehensive workflow for general-purpose neural modeling with
	highly configurable neuromorphic hardware systems},
  journal = {Biological Cybernetics},
  year = {2011},
  volume = {104},
  pages = {263--296},
  affiliation = {Kirchhoff Institute for Physics, Ruprecht-Karls-Universität Heidelberg,
	Heidelberg, Germany},
  issn = {0340-1200},
  issue = {4},
  keyword = {Computer Science},
  publisher = {Springer Berlin / Heidelberg},
  url = {http://dx.doi.org/10.1007/s00422-011-0435-9}
}

@ARTICLE{buesing2011neural,
  author = {Buesing, L. and Bill, J. and Nessler, B. and Maass, W.},
  title = {Neural dynamics as sampling: A model for stochastic computation in
	recurrent networks of spiking neurons},
  journal = {PLoS Computational Biology},
  year = {2011},
  volume = {7},
  pages = {e1002211},
  number = {11},
  publisher = {Public Library of Science}
}

@ARTICLE{burges98tutorial,
  author = {Burges, C. J. C.},
  title = {A Tutorial on Support Vector Machines for Pattern Recognition},
  journal = {Data Mining and Knowledge Discovery},
  year = {1998},
  volume = {2},
  pages = {121--167},
  number = {2},
  file = {burges98tutorial.pdf:burges98tutorial.pdf:PDF},
  url = {citeseer.ist.psu.edu/burges98tutorial.html}
}

@ARTICLE{burkitt2003,
  author = {A. N. Burkitt and H. Meffin and D. B. Grayden},
  title = {Study of neuronal gain in a conductance-based leaky integrate-and-fire
	neuron model with balanced excitatory and inhibitory synaptic input},
  journal = {Biological Cybernetics},
  year = {2003},
  volume = {89},
  pages = {119-125}
}

@Article{burkitt2006review,
  Author         = {Burkitt, Anthony N},
  Title          = {A review of the integrate-and-fire neuron model: II.
                   Inhomogeneous synaptic input and network properties},
  Journal        = {Biological cybernetics},
  Volume         = {95},
  Number         = {2},
  Pages          = {97--112},
  publisher      = {Springer},
  year           = 2006
}

@article{butz2013simple,
  title={A simple rule for dendritic spine and axonal bouton formation can account for cortical reorganization after focal retinal lesions},
  author={Butz, Markus and van Ooyen, Arjen},
  journal={PLoS Comput Biol},
  volume={9},
  number={10},
  pages={e1003259},
  year={2013},
  publisher={Public Library of Science}
}

@ARTICLE{buxhoeveden2002minicolumn,
  author = {Buxhoeveden, DP and Casanova, MF},
  title = {The Minicolumn and Evolution of the Brain},
  journal = {Brain Behav Evol},
  year = {2002},
  volume = {60},
  pages = {125-151}
}

@ARTICLE{buzsaki1984inhibition,
  author = {Buzsaki},
  title = {Feed-forward inhibition in the hippocampal formation},
  journal = {Progress in Neurobiology},
  year = {1984},
  volume = {22},
  pages = {131-153},
  owner = {bkaplan},
  timestamp = {2008.11.12}
}

@ARTICLE{buzsaki1995interneuralNetworks,
  author = {Buzsaki, Chrobak},
  title = {Temporal structure in spatially organized neuronal ensembles: a role
	for interneuronal networks},
  journal = {Current Opinion in Neurobiology},
  year = {1995},
  volume = {5},
  pages = {504-510},
  owner = {bkaplan},
  timestamp = {2008.11.12}
}

@ARTICLE{buzsaki1996inhibition,
  author = {Buzsaki, G. and Penttonen, M. and Nadasdy, Z. and Bragin, A.},
  title = {Pattern and inhibition-dependent invasion of pyramidal cell dendrites
	by fast spikes in the hippocampus in vivo},
  journal = {Proceedings of the National Academy of Sciences of the United States
	of America},
  year = {1996},
  volume = {93},
  pages = {9921-9925},
  owner = {bkaplan},
  timestamp = {2008.11.12}
}

@INPROCEEDINGS{baeck_icga93optmut,
  author = {B{\"a}ck, Thomas},
  title = {Optimal mutation rates in genetic search},
  booktitle = {Proceedings of the 5th International Conference on Genetic Algorithms},
  year = {1993},
  editor = {Forrest},
  pages = {2--8},
  month = {June},
  publisher = {Morgan Kauffman},
  file = {baeck_icga93optmut.pdf:baeck_icga93optmut.pdf:PDF}
}

@MISC{boostcircularbuffer_1461_homepage,
  author = {{B}oost.{C}ircular{B}uffer},
  title = {Version 1.46.1 Website},
  howpublished = {\url{http://www.boost.org/doc/libs/1_46_1/libs/circular_buffer/doc/circular_buffer.html}},
  year = {2011}
}

@MISC{boostinterprocess_1461_homepage,
  author = {{B}oost.{I}nterprocess},
  title = {Version 1.46.1 Website},
  howpublished = {\url{http://www.boost.org/doc/libs/1_46_1/doc/html/interprocess.html}},
  year = {2011}
}

@MISC{boostpython_1341_homepage,
  author = {{B}oost.{P}ython},
  title = {Version 1.34.1 Website},
  howpublished = {\url{http://www.boost.org/doc/libs/1_34_1/libs/python}},
  year = {2008}
}

@MISC{boostpython_1710_homepage,
  author = {{B}oost.{P}ython},
  title = {Version 1.71.0 Website},
  howpublished = {\url{http://www.boost.org/doc/libs/1_71_0/libs/python}},
  year = {2019}
}

@MISC{boostgraph_1710_homepage,
  author = {{B}oost.{G}raph},
  title = {Version 1.71.0 Website},
  howpublished = {\url{http://www.boost.org/doc/libs/1_71_0/libs/graph}},
  year = {2019}
}

@phdthesis{bytschok2011shared,
  title={From shared input to correlated neuron dynamics: Development of a predictive framework},
  author={Bytschok, Ilja},
  year={2011},
  school={Diploma thesis, University of Heidelberg}
}

@MISC{encounter,
  author = {Cadence Design Systems, Inc.},
  title = {Encounter Digital Implementation System},
  howpublished = {www.cadence.com},
  year = {2012},
  owner = {simon},
  timestamp = {2012.12.14}
}

@MISC{spectre,
  author = {Cadence Design Systems, Inc},
  title = {Virtuoso Multi-Mode Simulation},
  howpublished = {www.cadence.com},
  year = {2012},
  timestamp = {2012.08.11}
}

@INPROCEEDINGS{cairns_dexa01comparison,
  author = {Cairns, P. and Huyck, C. and Mitchell, I. and Xinju Wu, W.},
  title = {A Comphoarison of categorisation algorithms for predicting the cellular
	localization sites of proteins},
  booktitle = {Proceedings of the 12th International Workshop on Database and Expert
	Systems Applications},
  year = {2001},
  pages = {296--300},
  address = {Munich, Germany},
  publisher = {IEEE},
  file = {cairns_dexa01comparison.pdf:cairns_dexa01comparison.pdf:PDF}
}

@ARTICLE{Cameron2005,
  author = {Cameron, K. and Boonsobhak, V. and Murray, A. and Renshaw, D.},
  title = {Spike timing dependent plasticity (STDP) can ameliorate process variations
	in neuromorphic VLSI},
  journal = {Neural Networks, IEEE Transactions on},
  year = {2005},
  volume = {16},
  pages = {1626-1637},
  number = {6},
  doi = {10.1109/TNN.2005.852238},
  issn = {1045-9227},
  keywords = {CMOS integrated circuits;VLSI;digital signal processing chips;neural
	nets;video coding;CMOS;adaptive system;depth-recovery algorithm;neuromorphic
	VLSI;spike timing dependent plasticity;very large scale integration;visual-processing;Animals;CMOS
	process;Circuits;Educational institutions;Error correction;Image
	motion analysis;Neuromorphics;Neurons;Timing;Very large scale integration;Active
	pixel;CMOS integrated circuits;focal-plane sensor;neuromorphic analogue
	very large scale integration (VLSI);spike timing dependent plasticity
	(STDP);temporal processing;transistor mismatch;Action Potentials;Algorithms;Artificial
	Intelligence;Biomimetics;Equipment Design;Equipment Failure Analysis;Image
	Enhancement;Image Interpretation, Computer-Assisted;Neuronal Plasticity;Pattern
	Recognition, Automated;Quality Control;Semiconductors;Time Factors;Transducers;Video
	Recording},
  owner = {simon},
  timestamp = {2013.05.03}
}

@INPROCEEDINGS{cannon98approximate,
  author = {Cannon, A. H. and Cowen, L. J. and Priebe, C. E.},
  title = {Approximate distance classification},
  booktitle = {Proceedings of the 1998 Symposium on the Interface between Computer
	Science and Statistics},
  year = {1998},
  number = {30-1},
  pages = {544--549},
  file = {cannon98approximate.pdf:cannon98approximate.pdf:PDF}
}

@ARTICLE{caporale08_stdp,
  author = {Caporale, Natalia and Dan, Yang },
  title = {Spike Timing-Dependent Plasticity: A Hebbian Learning Rule.},
  journal = {Annual review of neuroscience},
  year = {2008},
  month = {February},
  abstract = {Spike timing-dependent plasticity (STDP) as a Hebbian synaptic learning
	rule has been demonstrated in various neural circuits over a wide
	spectrum of species, from insects to humans. The dependence of synaptic
	modification on the order of pre- and postsynaptic spiking within
	a critical window of tens of milliseconds has profound functional
	implications. Over the past decade, significant progress has been
	made in understanding the cellular mechanisms of STDP at both excitatory
	and inhibitory synapses and of the associated changes in neuronal
	excitability and synaptic integration. Beyond the basic asymmetric
	window, recent studies have also revealed several layers of complexity
	in STDP, including its dependence on dendritic location, the nonlinear
	integration of synaptic modification induced by complex spike trains,
	and the modulation of STDP by inhibitory and neuromodulatory inputs.
	Finally, the functional consequences of STDP have been examined directly
	in an increasing number of neural circuits in vivo. Expected final
	online publication date for the Annual Review of Neuroscience Volume
	31 is June 16, 2008. Please see http://www.annualreviews.org/catalog/pubdates.aspx
	for revised estimates.},
  address = {Division of Neurobiology, Department of Molecular and Cell Biology,
	and Helen Wills Neuroscience Institute, University of California,
	Berkeley, California 94720; email: caporale@socrates.berkeley.edu.},
  doi = {http://dx.doi.org/10.1146/annurev.neuro.31.060407.125639},
  issn = {0147-006X},
  keywords = {platicity, somatosensory, spike, viva},
  posted-at = {2008-09-17 04:15:17},
  priority = {2}
}

@ARTICLE{carandini2007,
  author = {Matteo Carandini and Jonathan C. Horton and Lawrence C. Sincich},
  title = {Thalamic filtering of retinal spike trains by postsynaptic summation},
  journal = {Journal of Vision},
  year = {2007},
  volume = {7},
  number = {14}
}

@article{caroni2012structural,
  title={Structural plasticity upon learning: regulation and functions},
  author={Caroni, Pico and Donato, Flavio and Muller, Dominique},
  journal={Nature Reviews Neuroscience},
  volume={13},
  number={7},
  pages={478--490},
  year={2012},
  publisher={Nature Publishing Group}
}

@ARTICLE{carota2004,
  author = {Luciana Carota and Giacomo Indiveri and Vittorio Dante},
  title = {A software-hardware selective attention system},
  journal = {Neurocomputing},
  year = {2004},
  volume = {58-60},
  pages = {647-653},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  ee = {http://dx.doi.org/10.1016/j.neucom.2004.01.109}
}

@INPROCEEDINGS{carr_gecco02alignment,
  author = {Carr, B. and Hart, W. and Krasnogor, N. and Hirst, J. and Burke,
	E. and Smith, J.},
  title = {Alignment of Protein Structures with a Memetic Evolutionary Algorithm},
  booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference
	{GECCO} 2002},
  year = {2002},
  editor = {Langdon, W.B. and others},
  pages = {1027--1034},
  month = {July},
  publisher = {Morgan Kaufmann Publishers}
}

@article{carr1990circuit,
  title={A circuit for detection of interaural time differences in the brain stem of the barn owl},
  author={Carr, CE and Konishi, M},
  journal={The Journal of Neuroscience},
  volume={10},
  number={10},
  pages={3227--3246},
  year={1990},
  publisher={Soc Neuroscience}
}

@INPROCEEDINGS{carusone02,
  author = {Carusone, A.C. and Johns, D.A.},
  title = {A 5th order Gm-C filter in 0.25 mu;m CMOS with digitally programmable
	poles and zeroes},
  booktitle = {Circuits and Systems, 2002. ISCAS 2002. IEEE International Symposium
	on},
  year = {2002},
  volume = {4},
  pages = { IV-635 - IV-638 vol.4},
  abstract = {This paper describes a 5th order Gm-C filter in a 0.25 mu;m CMOS technology.
	The signal path, including transconductors and Miller integrators,
	consists entirely of NMOS transistors for high speed operation. All
	poles and zeros are digitally programmable. A prototype configured
	for a lowpass response demonstrates a signal bandwidth of 45 MHz.},
  doi = {10.1109/ISCAS.2002.1010536},
  keywords = { 0.25 micron; 45 MHz; 5th order Gm-C filter; CMOS technology; Miller
	integrators; NMOS transistors; common-mode feedback; digitally programmable
	poles; digitally programmable zeros; high speed operation; lowpass
	response; transconductors; CMOS analogue integrated circuits; active
	filters; circuit feedback; low-pass filters; poles and zeros; programmable
	filters;}
}

@ARTICLE{castellano_cyber04,
  author = {Castellano, Giovanna and Fanelli, Anna M. and Mencar, Corrado},
  title = {An empirical risk functional to improve learning in a neuro-fuzzy
	classifier},
  journal = {IEEE transactions on Systems, Man, and Cybernetics},
  year = {2004},
  volume = {34},
  pages = {725-731},
  month = {February}
}

@INPROCEEDINGS{cater87learning,
  author = {Cater, J. P.},
  title = {Successfully using peak learning rates of 10 (and greater) in back-propagation
	networks with the heuristic learning algorithm},
  booktitle = {Proceedings of the IEEE First International Conference on Neural
	Networks},
  year = {1987},
  editor = {Caudill, M. and Butler, C.},
  volume = {II},
  pages = {645--651},
  address = {San Diego},
  publisher = {IEEE}
}

@INPROCEEDINGS{caudell_icga89parametric,
  author = {Caudell, T. P. and Dolan, C. P.},
  title = {Parametric connectivity: {T}raining of constrained networks using
	genetic algorithms},
  booktitle = {Proceedings of the Third International Conference on Genetic Algorithms},
  year = {1989},
  editor = {Schaffer, J. D.},
  pages = {370--374},
  publisher = {Morgan Kaufmann}
}

@INPROCEEDINGS{cauwenberghs99learning,
  author = {Cauwenberghs, G.},
  title = {Learning on Silicon: A Survey},
  booktitle = {Learning on Silicon: Adaptive {VLSI} Neural Systems},
  year = {1999},
  editor = {Cauwenberghs, G. and Bayoumi, M. A. },
  pages = {1--29},
  address = {Norwell, MA},
  publisher = {Kluwer Academic Publisher},
  chapter = {1},
  keywords = {neuromorphic}
}

@ARTICLE{cauwenberghs93descent,
  author = {Cauwenberghs, G.},
  title = {A fast stochastic error-descent algorithm for supervised learning
	and optimization},
  journal = {Advances in Neural Information Processing Systems},
  year = {1993},
  volume = {5},
  pages = {244--251},
  address = {San Mateo, CA},
  publisher = {Morgan Kaufmann}
}

@INPROCEEDINGS{chalmers90evolution,
  author = {Chalmers, D. J.},
  title = {The evolution of learning: {A}n experiment in genetic connectionism},
  booktitle = {Proceedings of the 1990 Connectionist Models Summer School},
  year = {1990},
  editor = {Touretzky, D. S. and Elman, J. L. and Hinten, G. E.},
  pages = {81--90},
  address = {San Mateo, CA},
  publisher = {Morgan Kaufmann},
  file = {chalmers90evolution.pdf:chalmers90evolution.pdf:PDF}
}

@ARTICLE{chan2007aer,
  author = {Chan, V. and Liu, S.C. and van Schaik, A.},
  title = {AER EAR: A matched silicon cochlea pair with address event representation
	interface},
  journal = {Circuits and Systems I: Regular Papers, IEEE Transactions on},
  year = {2007},
  volume = {54},
  pages = {48--59},
  number = {1},
  publisher = {IEEE}
}

@INPROCEEDINGS{chechik00temporally,
  author = {Gal Chechik and Naftali Tishby},
  title = {Temporally Dependent Plasticity: An Information Theoretic Account},
  booktitle = {{NIPS}},
  year = {2000},
  pages = {110-116},
  citeseerurl = {citeseer.nj.nec.com/388033.html},
  keywords = {spiking learning}
}

@PERIODICAL{chen2019race,
  editor = {Chen, Jim X. and Carver, Jeffrey and Gottlieb, Steven and Post, Douglass E. and Schneider, Barry I.},
  title = {Computing in Science \& Engineering},
  issuetitle = {Race to Exascale},
  year = 2019,
  volume = 21,
  number = 1
}

@inproceedings{chilimbi2014project,
  title={Project adam: Building an efficient and scalable deep learning training system},
  author={Chilimbi, Trishul and Suzue, Yutaka and Apacible, Johnson and Kalyanaraman, Karthik},
  booktitle={11th USENIX Symposium on Operating Systems Design and Implementation (OSDI 14)},
  pages={571--582},
  year={2014}
}

@article{chua2012hodgkin,
  author = {Leon Chua and Valery Sbitnev and Hyongsuk Kim},
  title = {Hodgkin–Huxley axon is made of memristors},
  journal = {International Journal of Bifurcation and Chaos},
  volume = 22,
  number = 03,
  month = mar,
  year = 2012,
  pages = 1230011,
  doi = {10.1142/s021812741230011x}
}

@ARTICLE{liu_patrecog97,
  author = {Cheng-Lin Liu, Jin H. Kim, Ru-Wei Dai},
  title = {Multiresolution locally expanded HONN for handwritten numeral recognition},
  journal = {Pattern Recognition Letters},
  year = {1997},
  volume = {18},
  pages = {1019-1025}
}

@ARTICLE{chicca2007hardsoft,
  author = {Elisabetta Chicca and Adrian M. Whatley and Patrick Lichtsteiner
	and Vittorio Dante and Tobias Delbruck and Paolo Del Giudice and
	Rodney J. Douglas and Giacomo Indiveri and Senior Member and Senior
	Member},
  title = {A multi-chip pulse-based neuromorphic infrastructure and its application
	to a model of orientation selectivity},
  journal = {IEEE Transactions on Circuits and Systems I, Regular Papers},
  year = {2007},
  volume = {2006},
  pages = {981--993}
}

@article{chow1996spontaneous,
  title={Spontaneous action potentials due to channel fluctuations.},
  author={Chow, Carson C and White, John A},
  journal={Biophysical Journal},
  volume={71},
  number={6},
  pages={3013},
  year={1996},
  publisher={The Biophysical Society}
}

@ARTICLE{chow98,
  author = {Chow, C.C.},
  title = {Phase-locking in weakly heterogeneous neuronal networks},
  journal = {Physica D},
  year = {1998},
  volume = {118},
  pages = {343-370(28)},
  abstract = {We examine analytically the existence and stability of phase-locked
	states in a weakly heterogeneous neuronal network. We consider a
	model of N neurons with all-to-all synaptic coupling where the heterogeneity
	is in the intrinsic firing frequency of the individual neurons. We
	consider both inhibitory and excitatory coupling. We derive the conditions
	under which stable phase-locking is possible. In homogeneous networks,
	many different periodic phase-locked states are possible. Their stability
	depends on the dynamics of the neuron and the coupling. For weak
	heterogeneity, the phase-locked states are perturbed from the homogeneous
	states and can remain stable if their homogeneous counterparts are
	stable. For enough heterogeneity, phase-locked solutions either lose
	stability or are destroyed completely. We analyze the possible states
	the network can take when phase-locking is broken.},
  doi = {doi:10.1016/S0167-2789(98)00082-7},
  url = {ttp://www.ingentaconnect.com/content/els/01672789/1998/00000118/00000003/art00082}
}

@ARTICLE{chua1976memristive,
  author = {Chua, Leon O and Kang, Sung Mo},
  title = {Memristive devices and systems},
  journal = {Proceedings of the IEEE},
  year = {1976},
  volume = {64},
  pages = {209--223},
  number = {2},
  owner = {simon},
  publisher = {IEEE},
  timestamp = {2013.04.19}
}

@inproceedings{ciresan2011committee,
  title={A committee of neural networks for traffic sign classification},
  author={Ciresan, Dan and Meier, Ueli and Masci, Jonathan and Schmidhuber, J{\"u}rgen},
  booktitle={Neural Networks (IJCNN), The 2011 International Joint Conference on},
  pages={1918--1921},
  year={2011},
  organization={IEEE}
}

@article{cirecsan2012multi,
  title={Multi-column deep neural network for traffic sign classification},
  author={Cire{\c{s}}an, Dan and Meier, Ueli and Masci, Jonathan and Schmidhuber, J{\"u}rgen},
  journal={Neural Networks},
  volume={32},
  pages={333--338},
  year={2012},
  publisher={Elsevier}
}

@article{clemence1947relativity,
  title={The relativity effect in planetary motions},
  author={Clemence, Gerald Maurice},
  journal={Reviews of Modern Physics},
  volume={19},
  number={4},
  pages={361},
  year={1947},
  publisher={APS}
}

@ARTICLE{clopath2007predicting,
  author = {Clopath, C. and Jolivet, R. and Rauch, A. and L{\"u}scher, H.R. and
	Gerstner, W.},
  title = {Predicting neuronal activity with simple models of the threshold
	type: Adaptive Exponential Integrate-and-Fire model with two compartments},
  journal = {Neurocomputing},
  year = {2007},
  volume = {70},
  pages = {1668--1673},
  number = {10},
  publisher = {Elsevier}
}

@ARTICLE{Clopath2008,
  author = {Clopath, Claudia AND Ziegler, Lorric AND Vasilaki, Eleni AND Büsing,
	Lars AND Gerstner, Wulfram},
  title = {Tag-Trigger-Consolidation: A Model of Early and Late Long-Term-Potentiation
	and Depression},
  journal = {PLoS Comput Biol},
  year = {2008},
  volume = {4},
  pages = {e1000248},
  number = {12},
  month = {12},
  abstract = { <title>Author Summary</title> <p>Humans and animals learn by changing
	the strength of connections between neurons, a phenomenon called
	synaptic plasticity. These changes can be induced by rather short
	stimuli (lasting sometimes only a few seconds) but should then be
	stable for months or years in order to be useful for long-term memory.
	Experimentalists have shown that synapses undergo a sequence of steps
	that transforms the rapid change during the early phase of synaptic
	plasticity into a stable memory trace in the late phase. In this
	paper we introduce a model with a small number of equations that
	can describe the phenomena of induction of synaptic changes during
	the early phase of synaptic plasticity, the trigger process for protein
	synthesis, and the final stabilization. The model covers a broad
	range of experimental phenomena known as tagging experiments and
	makes testable predictions. The ability to model the stabilization
	of synapses is crucial to understand learning and memory processes
	in animals and humans and a necessary ingredient for any large-scale
	model of the brain.</p> },
  doi = {10.1371/journal.pcbi.1000248},
  owner = {simon},
  publisher = {Public Library of Science},
  timestamp = {2012.12.18},
  url = {http://dx.doi.org/10.1371%2Fjournal.pcbi.1000248}
}

@BOOK{coello02multiobjective,
  title = {Evolutionary Algorithms for Solving Multi-Objective Problems},
  publisher = {Kluwer Academic/Plenum Publishers},
  year = {2002},
  author = {Coello Coello, C. A. and Van Veldhuizen, D. A. and Lamont, G. B.},
  address = {New York},
  isbn = {0-306-46762-3}
}

@ARTICLE{cohen68magneto,
  author = {Cohen, David },
  title = {Magnetoencephalography: Evidence of Magnetic Fields Produced by Alpha-Rhythm
	Currents},
  journal = {Science},
  year = {1968},
  volume = {161},
  pages = {784--786},
  number = {3843},
  month = {August},
  abstract = {Weak alternating magnetic fields outside the human scalp, produced
	by alpha-rhythm currents, are demonstrated. Subject ard magnetic
	detector were housed in a multilayer magnetically shielded chamber.
	Background magnetic noise was reduced by signal-averaging. The fields
	near the scalp are about 1 x 10-9 gauss (peak to peak). A course
	distribution shows left-right symmetry for the particular averaging
	technique used here. 10.1126/science.161.3843.784},
  keywords = {alpha-rhythm, martinos, meg}
}

@INPROCEEDINGS{cohoon_icga87punctuated,
  author = {Cohoon, J. P. and Hedge, S. U. and Martin, W. N. and Richards, D.},
  title = {Punctuated Equilibria: A Parallel Genetic Algorithm},
  booktitle = {Proceedings of the 2nd International Conference on Genetic Algorithms},
  year = {1987},
  editor = {J. J. Grefenstette},
  pages = {148--154},
  address = {Hillsdale, New Jersey},
  publisher = {Lawrence Erlbaum}
}

@INPROCEEDINGS{cohoon_ppsn91genetic,
  author = {Cohoon, J. P. and Martin, W. N. and Richards, D. S.},
  title = {Genetic Algorithms and Punctuated Equilibria in {VLSI}},
  booktitle = {Proceedings of the 1st International Conference on Parallel Problem
	Solving from Nature},
  year = {1991},
  editor = {Schwefel, H.-P. and M{\"a}nner, R.},
  volume = {496},
  pages = {134--141},
  publisher = {Springer Verlag},
  howpublished = {Lecture Notes in Computer Science}
}

@MISC{pyqwt5homepage,
  author = {Colclough, Mark and Vermeulen, Gerard},
  title = {{P}y{Q}wt -- a set of {P}ython bindings for the {Q}wt {C}++ class
	library},
  howpublished = {http://pyqwt.sourceforge.net/},
  year = {2007},
  owner = {mueller},
  timestamp = {2008.08.05}
}

@MISC{green500_nov2012,
  author = {CompuGreen, LLC.},
  title = {Green 500 List},
  howpublished = {Website},
  month = {November},
  year = {2012},
  owner = {simon},
  timestamp = {2013.05.03},
  url = {http://www.green500.org/lists/green201211}
}

@article{connor1977neural,
  title={Neural repetitive firing: modifications of the Hodgkin-Huxley axon suggested by experimental results from crustacean axons.},
  author={Connor, JOHN A and Walter, DAVID and McKowN, RUSSELL},
  journal={Biophysical Journal},
  volume={18},
  number={1},
  pages={81},
  year={1977},
  publisher={The Biophysical Society}
}

@ARTICLE{connors90intrinsic,
  author = {Connors, B.W. and Gutnick, M.J.},
  title = {Intrinsic Firing Patterns of Diverse Neocortical Neurons},
  journal = {Trends Neurosci.},
  year = {1990},
  volume = {13},
  pages = {99--104}
}

@ARTICLE{constantine06shining,
  author = {Constantine-Paton, Martha},
  title = {Shining Light on Spike Timing-Dependent Plasticity},
  journal = {Neuron},
  year = {2006},
  volume = {50},
  pages = {5--7},
  number = {1},
  month = apr,
  abstract = {Although spike timing-dependent plasticity has been well-characterized
	in vitro, it is less clear to what degree spike timing-dependent
	plasticity contributes to shaping visual system properties in vivo.
	In this issue of Neuron, two papers by Vislay-Meltzer et al. and
	Mu and Poo provide evidence that STDP contributes to the effects
	of sensory stimuli in refinement of the retinotectal system in Xenopus.},
  owner = {mreuss},
  timestamp = {2007.06.14},
  url = {http://www.sciencedirect.com/science/article/B6WSS-4JN1NYS-3/2/1b1993ec81a2f48139cfb207040f763e}
}

@ARTICLE{contreras97,
  author = {Contreras, Diego and Destexhe, Alain and Steriade, Mircea},
  title = {Intracellular and computational characterization of the intracortical
	inhibitory control of synchronized thalamic inputs in vivo},
  journal = {Journal of Neurophysiology},
  year = {1997},
  volume = {77},
  pages = {335-350}
}

@BOOK{corbet2005,
  title = {Linux Device Drivers},
  publisher = {O'Reilly \& Associates, Inc.},
  year = {2005},
  author = {Jonathan Corbet and Alessandro Rubini and Greg Kroah-Hartman},
  address = {Sebastopol, CA, USA},
  edition = {3rd},
  isbn = {978-0-596-00590-0}
}

@ARTICLE{cortes95supportvm,
  author = {Cortez, C. and Vapnik, V.},
  title = {Support-Vector Networks},
  journal = {Machine Learning},
  year = {1995},
  volume = {20},
  pages = {273--297}
}

@INPROCEEDINGS{cosatto94net32k,
  author = {Cosatto, E. and Graf, H.P.},
  title = {NET32K high speed image understanding system},
  booktitle = {Proceedings of the Fourth International Conference on Microelectronics
	for Neural Networks and Fuzzy Systems, 1994},
  year = {1994},
  pages = {413-421},
  file = {cosatto94net32k.pdf:cosatto94net32k.pdf:PDF},
  journal = {Proceedings of the Fourth International Conference on Microelectronics
	for Neural Networks and Fuzzy Systems},
  owner = {fieres}
}

@ARTICLE{cossart03attractor,
  author = {Cossart, R. and Aronov, D. and Yuste, R.},
  title = {Attractor dynamics of network up states in the neocortex},
  journal = {Nature},
  year = {2003},
  volume = {423},
  pages = {238--283},
  owner = {bruederl},
  timestamp = {2008.07.18}
}

@ARTICLE{serrano_tcas2007,
  author = {Costas-Santos, Jes\'us and Serrano-Gotarredona, Teresa and Serrano-Gotarredona,
	Rafael and Linares-Barranco, Bernab\'e},
  title = {A Spatial Contrast Retina with On-chip Calibration for Neuromorphic
	Spike-Based {AER} Vision Systems},
  journal = {IEEE Transactions on Circuits and Systems},
  year = {2007},
  volume = {54},
  pages = {1444-1458},
  number = {7}
}

@ARTICLE{coultrip92,
  author = {Robert Coultrip and Richard Granger and Gary Lynch},
  title = {A cortical model of winner-take-all competition via lateral inhibition},
  journal = {Neural Netw.},
  year = {1992},
  volume = {5},
  pages = {47--54},
  number = {1},
  address = {Oxford, UK, UK},
  doi = {http://dx.doi.org/10.1016/S0893-6080(05)80006-1},
  issn = {0893-6080},
  publisher = {Elsevier Science Ltd.}
}

@ARTICLE{cover65geometrical,
  author = {Cover, T.M.},
  title = {Geometrical and statistical properties of systems of linear inequalities
	with applications in pattern recognition},
  journal = {IEEE Trans. on Electronic Computers},
  year = {1965},
  volume = {EC-14},
  pages = {326--334},
  file = {cover65geometrical.pdf:cover65geometrical.pdf:PDF}
}

@article{cox2019surviving,
	author = {Cox, Russ},
	title = {Surviving Software Dependencies},
	year = {2019},
	issue_date = {August 2019},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {62},
	number = {9},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/3347446},
	doi = {10.1145/3347446},
	journal = {Commun. ACM},
	month = aug,
	pages = {36–43},
	numpages = {8}
}

@ARTICLE{Cudmore2008,
  author = {Cudmore, R. H. and Desai, N. S.},
  title = {Intrinsic plasticity},
  journal = {Scholarpedia},
  year = {2008},
  volume = {3},
  pages = {1363},
  number = {2},
  owner = {simon},
  timestamp = {2013.04.24}
}

@INPROCEEDINGS{cummings03,
  author = {Cliffort E. Cummings},
  title = {Synthesizeable Finite State Machine Design Techniques Using the New
	{SystemVerilog} 3.0 Enhancements},
  booktitle = {SNUG 2003 San Jose},
  year = {2003}
}

@INPROCEEDINGS{cummings08,
  author = {Cliffort E. Cummings},
  title = {Clock Domain Crossing ({CDC}) Design \& Verification
	  Techniques Using {SystemVerilog}},
  booktitle = {SNUG 2008 Boston},
  year = 2008
}

@MISC{cecill_homepage,
  author = {{C}e{CILL 2009}},
  title = {Website},
  howpublished = {\url{http://www.cecill.info/}}
}

@BOOK{dally1998digital,
  title = {Digital systems engineering},
  publisher = {Cambridge University Press},
  year = {1998},
  author = {William J. Dally and John W. Poulton},
  address = {New York, NY, USA},
  isbn = {0-521-59292-5}
}

@article{dally2020domainspecific,
  author = {Dally, William J. and Turakhia, Yatish and Han, Song},
  title = {Domain-Specific Hardware Accelerators},
  year = 2020,
  issue_date = {July 2020},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = 63,
  number = 7,
  issn = {0001-0782},
  url = {https://doi.org/10.1145/3361682},
  doi = {10.1145/3361682},
  abstract = {DSAs gain efficiency from specialization and performance from parallelism.},
  journal = {Commun. ACM},
  month = jun,
  pages = {48--57},
  numpages = 10
}

@ARTICLE{clifford2017af,
  author = {Clifford, Gari D and Liu, Chengyu and Moody, Benjamin and Lehman, Li-Wei H and Silva, Ikaro and Li, Qiao and Johnson, A E and Mark, Roger G},
  title = {AF Classification from a Short Single Lead ECG Recording: the PhysioNet/Computing in Cardiology Challenge 2017},
  address = {United States},
  year = 2017,
  issn = {2325-8861 and 2325-887X},
  volume = {44},
  journal = {Computing in Cardiology},
}

@mastersthesis{czierlinski2020pynn,
  author = {Milena Czierlinski},
  title = {{PyNN} for {BrainScaleS-2}},
  school = {Universit{\"a}t Heidelberg},
  year = 2020,
  type = {Bachelor thesis}
}

@article{czischek2021spiking_nopreprint,
  title = {Spiking neuromorphic chip learns entangled quantum states},
  author = {Stefanie Czischek and Andreas Baumbach and Sebastian Billaudelle and Benjamin Cramer and Lukas Kades and Jan M. Pawlowski and Markus K. Oberthaler and Johannes Schemmel and Mihai A. Petrovici and Thomas Gasenzer and Martin Gärttner},
  journal = {SciPost Phys.},
  volume = 12,
  issue = 1,
  pages = 39,
  year = 2022,
  publisher = {SciPost},
  doi = {10.21468/SciPostPhys.12.1.039}
}

@article{czischek2021spiking,
  title = {Spiking neuromorphic chip learns entangled quantum states},
  author = {Stefanie Czischek and Andreas Baumbach and Sebastian Billaudelle and Benjamin Cramer and Lukas Kades and Jan M. Pawlowski and Markus K. Oberthaler and Johannes Schemmel and Mihai A. Petrovici and Thomas Gasenzer and Martin Gärttner},
  eprint = {2008.01039},
  archivePrefix = {arXiv},
  primaryClass = {cs.ET},
  journal = {SciPost Phys.},
  volume = 12,
  issue = 1,
  pages = 39,
  year = 2022,
  publisher = {SciPost},
  doi = {10.21468/SciPostPhys.12.1.039}
}

@ARTICLE{dan04stdp,
  author = {Yang Dan and Muming Poo},
  title = {Spike Timing-Dependent Plasticity of Neural Circuits},
  journal = {Neuron},
  year = {2004},
  volume = {44},
  pages = {23--30},
  number = {1},
  month = sep,
  abstract = {Recent findings of spike timing-dependent plasticity (STDP) have stimulated
	much interest among experimentalists and theorists. Beyond the traditional
	correlation-based Hebbian plasticity, STDP opens up new avenues for
	understanding information coding and circuit plasticity that depend
	on the precise timing of neuronal spikes. Here we summarize experimental
	characterization of STDP at various synapses, the underlying cellular
	mechanisms, and the associated changes in neuronal excitability and
	dendritic integration. We also describe STDP in the context of complex
	spike patterns and its dependence on the dendritic location of the
	synapse. Finally, we discuss timing-dependent modification of neuronal
	receptive fields and human visual perception and the computational
	significance of STDP as a synaptic learning rule.},
  owner = {mreuss},
  timestamp = {2007.06.14}
}

@article {dan06stdp,
  author = {Dan, Yang and Poo, Mu-Ming},
  title = {Spike Timing-Dependent Plasticity: From Synapse to Perception},
  volume = 86,
  number = 3,
  pages = {1033--1048},
  year = 2006,
  doi = {10.1152/physrev.00030.2005},
  publisher = {American Physiological Society},
  abstract = {Information in the nervous system may be carried by both the rate and timing of neuronal spikes. Recent findings of spike timing-dependent plasticity (STDP) have fueled the interest in the potential roles of spike timing in processing and storage of information in neural circuits. Induction of long-term potentiation (LTP) and long-term depression (LTD) in a variety of in vitro and in vivo systems has been shown to depend on the temporal order of pre- and postsynaptic spiking. Spike timing-dependent modification of neuronal excitability and dendritic integration was also observed. Such STDP at the synaptic and cellular level is likely to play important roles in activity-induced functional changes in neuronal receptive fields and human perception.},
  issn = {0031-9333},
  journal = {Physiological Reviews}
}

@ARTICLE{dante2005hardsoft,
  author = {Dante, V. and Del Giudice, P. and Whatley, A.M.},
  title = {Hardware and software for interfacing to address-event based neuromorphic
	systems},
  journal = {The Neuromorphic Engineer},
  year = {2005},
  volume = {2(1)},
  pages = {5-6},
  owner = {bruederl},
  timestamp = {2008.09.02}
}

@INPROCEEDINGS{daouzli08weights,
  author = {A. Daouzli and S. Saighi and L. Buhry and Y. Bornat and S. Renaud},
  title = {Weights Convergence and Spikes Correlation in an Adaptive Neural
	Network Implemented on VLSI},
  booktitle = {Proceedings of the International Conference on Bio-inspired Systems
	and Signal Processing ({BIOSIGNALS})},
  year = {2008},
  pages = {286-291}
}

@BOOK{darwin59origin,
  title = {On the Origin of Species},
  publisher = {John Murray},
  year = {1859},
  author = {Darwin, Charles R.},
  address = {London}
}

@ARTICLE{Davies20123,
  author = {S. Davies and F. Galluppi and A.D. Rast and S.B. Furber},
  title = {A forecast-based STDP rule suitable for neuromorphic implementation},
  journal = {Neural Networks},
  year = {2012},
  volume = {32},
  pages = {3 - 14},
  number = {0},
  note = {Selected Papers from IJCNN 2011},
  doi = {10.1016/j.neunet.2012.02.018},
  issn = {0893-6080},
  keywords = {STDP},
  owner = {simon},
  timestamp = {2012.12.18},
  url = {http://www.sciencedirect.com/science/article/pii/S0893608012000470}
}

@article{davies2018loihi,
  title={Loihi: A neuromorphic manycore processor with on-chip learning},
  author={Davies, Mike and Srinivasa, Narayan and Lin, Tsung-Han and Chinya, Gautham and Cao, Yongqiang and Choday, Sri Harsha and Dimou, Georgios and Joshi, Prasad and Imam, Nabil and Jain, Shweta and others},
  journal={IEEE Micro},
  volume=38,
  number=1,
  pages={82--99},
  year=2018,
  publisher={IEEE},
  doi = {10.1109/MM.2018.112130359},
}

@INPROCEEDINGS{davis_icga89adapting,
  author = {Davis, L.},
  title = {Adapting operator probabilities in genetic algorithms},
  booktitle = {Proceedings of the Third International Conference on Genetic Algorithms},
  year = {1989},
  editor = {Schaffer, J. D.},
  pages = {61--69},
  publisher = {Morgan Kaufmann}
}

@ARTICLE{davison12sumatra,
  author = {Andrew P. Davison},
  title = {Automated capture of experiment context for easier reproducibility in computational research},
  journal = {Computing in Science and Engineering},
  volume = 14,
  pages = {48--56},
  year = {2012},
  abstract = {Published scientific research that relies on numerical computations is too often not reproducible. For computational research to become consistently and reliably reproducible, the process must become easier to achieve, as part of day-to-day research. A combination of best practices and automated tools can make it easier to create reproducible research.}
}

@ARTICLE{davison10common,
  author = {Davison, Andrew and Muller, Eilif and Br{\"u}derle, Daniel and Kremkow,
	Jens},
  title = {A common language for neuronal networks in software and hardware},
  journal = {The Neuromorphic Engineer},
  year = {2010},
  doi = {doi: 10.2417/1201001.1712}
}

@ARTICLE{davison08pynn, crossref = {davison2009pynn}}
@ARTICLE{davison2009pynn,
  author = {Davison, Andrew P. and Br\"{u}derle, Daniel and Eppler, Jochen and Kremkow, Jens
	and Muller, Eilif and Pecevski, Dejan and Perrinet, Laurent and Yger, Pierre},
  title = {{PyNN}: a common interface for neuronal network simulators},
  journal = {Front. Neuroinform.},
  year = 2009,
  volume = 2,
  number = 11,
  abstract = {Computational neuroscience has produced a diversity of software for
	simulations of networks of spiking neurons, with both negative and
	positive consequences. On the one hand, each simulator uses its own
	programming or configuration language, leading to considerable difficulty
	in porting models from one simulator to another. This impedes communication
	between investigators and makes it harder to reproduce and build
	on the work of others. On the other hand, simulation results can
	be cross-checked between different simulators, giving greater confidence
	in their correctness, and each simulator has different optimizations,
	so the most appropriate simulator can be chosen for a given modelling
	task. A common programming interface to multiple simulators would
	reduce or eliminate the problems of simulator diversity while retaining
	the benefits. PyNN is such an interface, making it possible to write
	a simulation script once, using the Python programming language,
	and run it without modification on any supported simulator (currently
	NEURON, NEST, PCSIM, Brian and the Heidelberg VLSI neuromorphic hardware).
	PyNN increases the productivity of neuronal network modelling by
	providing high-level abstraction, by promoting code sharing and reuse,
	and by providing a foundation for simulator-agnostic analysis, visualization,
	and data-management tools. PyNN increases the reliability of modelling
	studies by making it much easier to check results on multiple simulators.
	PyNN is open-source software and is available from http://neuralensemble.org/PyNN.},
  keywords = {Python, interoperability, large-scale models, simulation, parallel
	computing, reproducibility, computational neuroscience, translation},
	doi = {10.3389/neuro.11.011.2008}
}

@BOOK{dayan01neuroscience,
  title = {Theoretical Neuroscience: Computational and Mathematical Modeling
	of Neural Systems},
  publisher = {The MIT press},
  year = {2001},
  author = {Dayan, Peter and Abbott, L. F.},
  address = {Cambride, Massachusetts},
  isbn = {0-262-04199-5}
}

@INPROCEEDINGS{deb_icga89niche,
  author = {Deb, K. and Goldberg, D. E.},
  title = {An Investigation of Niche and Species Formation in Genetic Function
	Optimization},
  booktitle = {Proceedings of the Third International Conference on Genetic Algorithms},
  year = {1989},
  editor = {Schaffer, J. D.},
  pages = {42--50},
  address = {San Francisco},
  publisher = {Morgan Kaufmann}
}

@ARTICLE{Debanne1998,
  author = {Debanne, Dominique and G"ahwiler, Beat H. and Thompson, Scott M.},
  title = {Long-term synaptic plasticity between pairs of individual CA3 pyramidal
	cells in rat hippocampal slice cultures},
  journal = {The Journal of Physiology},
  year = {1998},
  volume = {507},
  pages = {237--247},
  number = {1},
  doi = {10.1111/j.1469-7793.1998.237bu.x},
  issn = {1469-7793},
  publisher = {Blackwell Science Ltd},
  url = {http://dx.doi.org/10.1111/j.1469-7793.1998.237bu.x}
}

@article{decharms2000neural,
  title={Neural representation and the cortical code},
  author={Decharms, R Christopher and Zador, Anthony},
  journal={Annual review of neuroscience},
  volume={23},
  number={1},
  pages={613--647},
  year={2000},
  publisher={Annual Reviews 4139 El Camino Way, PO Box 10139, Palo Alto, CA 94303-0139, USA}
}

@article{deco2012ongoing,
  title={Ongoing cortical activity at rest: criticality, multistability, and ghost attractors},
  author={Deco, Gustavo and Jirsa, Viktor K},
  journal={The Journal of Neuroscience},
  volume={32},
  number={10},
  pages={3366--3375},
  year={2012},
  publisher={Soc Neuroscience}
}

@article{deco2013brain,
  title={Brain mechanisms for perceptual and reward-related decision-making},
  author={Deco, Gustavo and Rolls, Edmund T and Albantakis, Larissa and Romo, Ranulfo},
  journal={Progress in neurobiology},
  volume={103},
  pages={194--213},
  year={2013},
  publisher={Elsevier}
}

@MISC{dejong75genetic,
  author = {DeJong, K.},
  title = {The Analysis and Behaviour of a Class of Genetic Adaptive Systems},
  howpublished = {{\it PhD thesis}},
  year = {1975},
  file = {dejong75genetic.pdf:dejong75genetic.pdf:PDF},
  institution = {University of Michigan},
  number = {Diss.\ Abstr.\ Int.\ 36(10), 5140B, University Microfilms No. 76-9381}
}

@ARTICLE{delbrueck04silicon,
  author = {Delbr\"{u}ck, T. and Liu, S. C. },
  title = {A silicon early visual system as a model animal.},
  journal = {Vision Res},
  year = {2004},
  volume = {44},
  pages = {2083--2089},
  number = {17},
  abstract = {Examples that show the transfer of our basic knowledge of brain function
	into practical electronic models are rare. Here we present a user-friendly
	silicon model of the early visual system that contributes to animal
	welfare. The silicon chip emulates the neurons in the visual system
	by using analog Very Large Scale Integration (aVLSI) circuits. It
	substitutes for a live animal in experiment design and lecture demonstrations.
	The neurons on this chip display properties that are central to biological
	vision: receptive fields, spike coding, adaptation, band-pass filtering,
	and complementary signaling. Unlike previous laboratory devices whose
	complexity was limited by the use of discrete components on printed
	circuit boards, this battery-powered chip is a self-contained patch
	of the visual system. The realistic responses of the chip's cells
	and the self-contained adjustment-free correct operation of the chip
	suggest the possibility of implementation of similar circuits for
	visual prosthetics.}
}

@ARTICLE{dembo90modelfree,
  author = {Dembo, A. and Kailath, T.},
  title = {Model-Free Distributed Learning},
  journal = {IEEE Transactions on Neural Networks},
  year = {1990},
  volume = {1},
  pages = {58--70},
  number = {1}
}

@ARTICLE{denby03triggering,
  author = {Denby, B. and Garda, P. and Dranado, B. and Kiesling, C. and Pr{\'e}votet,
	J.-C. and Wassatsch, A},
  title = {Fast triggering in high-energy physics experiments using hardware
	neural network},
  journal = {IEEE Transactions on Neural Networks},
  year = {2003},
  volume = {14},
  pages = {1010--1026},
  number = {5}
}

@article{deneve2005bayesian,
  title={Bayesian inference in spiking neurons},
  author={Deneve, Sophie},
  journal={Advances in neural information processing systems},
  volume={17},
  pages={353--360},
  year={2005},
  publisher={MIT Press Cambridge, MA}
}

@Article{deneve2008bayesian,
  Author         = {Deneve, Sophie},
  Title          = {Bayesian spiking neurons I: inference},
  Journal        = {Neural computation},
  Volume         = {20},
  Number         = {1},
  Pages          = {91--117},
  publisher      = {MIT Press},
  year           = 2008
}

@INPROCEEDINGS{deng04,
  author = {Yunbin Deng and Chakrabartty, S. and Cauwenberghs, G.},
  title = {Three-decade programmable fully differential linear OTA},
  booktitle = {Circuits and Systems, 2004. ISCAS '04. Proceedings of the 2004 International
	Symposium on},
  year = {2004},
  volume = {1},
  pages = { I - 697-700 Vol.1},
  month = {may},
  abstract = {Acoustic and sonar analog signal processing applications require design
	operational transconductance amplifiers (OTAs) that can be configured
	over wide frequency range in multiple bands and yet achieve low power
	consumption and low harmonic distortion. A fully differential, linear
	OTA is presented with digitally programmable transconductance ranging
	over three-decades of dynamic range. Measurements from a prototype
	fabricated in a 0.5 mu;m CMOS process demonstrate a 0.4 nA/V to 0.8
	mu;A/V transconductance range, 40 dB common-mode rejection ratio
	(CMRR),and -48 dB third-order harmonic distortion, at 12 mu;W power
	dissipation.},
  doi = {10.1109/ISCAS.2004.1328290},
  keywords = { 0.5 micron; 12 muW; CMOS process; acoustic analog signal processing;
	common-mode rejection ratio; differential OTA; digitally programmable
	transconductance; linear OTA; low harmonic distortion; low power
	consumption; multiple bands; operational transconductance amplifier;
	power dissipation; prototype fabrication; sonar analog signal processing;
	third-order harmonic distortion; three-decade programmable OTA; wide
	frequency range; CMOS digital integrated circuits; differential amplifiers;
	harmonic distortion; integrated circuit design; low-power electronics;
	operational amplifiers; programmable circuits;}
}

@ARTICLE{denker1989zipcode,
  author = {J. S. Denker and W. R. Gardner and H. P. Graf and D. Henderson and
	R. E. Howard and W. Hubbard and L. D. Jackel and H. S. Baird and
	I. Guyon},
  title = {Neural network recognizer for hand-written zip code digits},
  journal = {Advances in neural information processing systems},
  year = {1989},
  volume = {1},
  pages = {323 - 331},
  file = {denker1989zipcode.djvu:denker1989zipcode.djvu:PDF},
  owner = {fieres}
}

@MASTERSTHESIS{denne2014bachelorthesis,
  author   = {Maximilian Denne},
  title    = {Testen der Software und Vermessen des Multi-Compartment Chips},
  year     = 2014,
  type     = {Bachelor thesis},
  school   = {Ruprecht-Karls-Universit{\"a}t Heidelberg}
}

@MISC{DesignWare2013,
  author = {{DesignWare}},
  title = {DesignWare Datapath Building Block IP},
  year = {2013},
  owner = {simon},
  timestamp = {2013.02.21},
  url = {http://www.synopsys.com/dw/buildingblock.php}
}

@ARTICLE{destexhe09self,
  author = {Destexhe, A.},
  title = {Self-sustained asynchronous irregular states and {U}p/{D}own states
	in thalamic, cortical and thalamocortical networks of nonlinear integrate-and-fire
	neurons.},
  journal = {Journal of Computational Neuroscience},
  year = {2009},
  volume = {3},
  pages = {493 - 506}
}

@ARTICLE{destexhe08mail,
  author = {Alain Destexhe},
  title = {Email conversation},
  year = {2008}
}

@ARTICLE{destexhe1997,
  author = {Alain Destexhe},
  title = {Conductance-based integrate-and-fire models},
  journal = {Neural Comput.},
  year = {1997},
  volume = {9},
  pages = {503--514},
  number = {3},
  address = {Cambridge, MA, USA},
  doi = {http://dx.doi.org/10.1162/neco.1997.9.3.503},
  issn = {0899-7667},
  publisher = {MIT Press}
}

@ARTICLE{destexhe2006,
  author = {Destexhe, Alain and Contreras, Diego},
  title = {Neuronal Computations with Stochastic Network States},
  journal = {Science},
  year = {2006},
  volume = {314},
  pages = {85-90},
  number = {5796},
  doi = {10.1126/science.1127241},
  eprint = {http://www.sciencemag.org/content/314/5796/85.full.pdf},
  url = {http://www.sciencemag.org/content/314/5796/85.abstract}
}

@ARTICLE{destexhe1998,
  author = {Alain Destexhe and Diego Contreras and Mircea Steriade},
  title = {Mechanisms underlying the synchronizing action of corticothalamic
	feedback through inhibition of thalamic relay cells},
  journal = {Journal of Neurophysiology},
  year = {1998},
  volume = {79},
  pages = {999-1016}
}

@ARTICLE{destexhe1998dent,
  author = {Alain Destexhe and Mike Neubig and Daniel Ulrich and John R. Huguenard},
  title = {Dendritic low-threshold calcium currents in thalamic realy cells},
  journal = {Journal of Neuroscience},
  year = {1998},
  volume = {18},
  pages = {3574-3588}
}

@ARTICLE{destexhe1999impact,
  author = {Destexhe, Alain and Pare, Denis},
  title = {{Impact of Network Activity on the Integrative Properties of Neocortical
	Pyramidal Neurons In Vivo}},
  journal = {J Neurophysiol},
  year = {1999},
  volume = {81},
  pages = {1531-1547},
  number = {4},
  abstract = {Destexhe, Alain and Denis Pare. Impact of network activity on the
	integrative properties of neocortical pyramidal neurons in vivo.
	During wakefulness, neocortical neurons are subjected to an intense
	synaptic bombardment. To assess the consequences of this background
	activity for the integrative properties of pyramidal neurons, we
	constrained biophysical models with in vivo intracellular data obtained
	in anesthetized cats during periods of intense network activity similar
	to that observed in the waking state. In pyramidal cells of the parietal
	cortex (area 5-7), synaptic activity was responsible for an approximately
	fivefold decrease in input resistance (Rin), a more depolarized membrane
	potential (Vm), and a marked increase in the amplitude of Vm fluctuations,
	as determined by comparing the same cells before and after microperfusion
	of tetrodotoxin (TTX). The model was constrained by measurements
	of Rin, by the average value and standard deviation of the Vm measured
	from epochs of intense synaptic activity recorded with KAc or KCl-filled
	pipettes as well as the values measured in the same cells after TTX.
	To reproduce all experimental results, the simulated synaptic activity
	had to be of relatively high frequency (1-5 Hz) at excitatory and
	inhibitory synapses. In addition, synaptic inputs had to be significantly
	correlated (correlation coefficient ~0.1) to reproduce the amplitude
	of Vm fluctuations recorded experimentally. The presence of voltage-dependent
	K+ currents, estimated from current-voltage relations after TTX,
	affected these parameters by <10%. The model predicts that the conductance
	due to synaptic activity is 7-30 times larger than the somatic leak
	conductance to be consistent with the approximately fivefold change
	in Rin. The impact of this massive increase in conductance on dendritic
	attenuation was investigated for passive neurons and neurons with
	voltage-dependent Na+/K+ currents in soma and dendrites. In passive
	neurons, correlated synaptic bombardment had a major influence on
	dendritic attenuation. The electrotonic attenuation of simulated
	synaptic inputs was enhanced greatly in the presence of synaptic
	bombardment, with distal synapses having minimal effects at the soma.
	Similarly, in the presence of dendritic voltage-dependent currents,
	the convergence of hundreds of synaptic inputs was required to evoke
	action potentials reliably. In this case, however, dendritic voltage-dependent
	currents minimized the variability due to input location, with distal
	apical synapses being as effective as synapses on basal dendrites.
	In conclusion, this combination of intracellular and computational
	data suggests that, during low-amplitude fast electroencephalographic
	activity, neocortical neurons are bombarded continuously by correlated
	synaptic inputs at high frequency, which significantly affect their
	integrative properties. A series of predictions are suggested to
	test this model.},
  eprint = {http://jn.physiology.org/cgi/reprint/81/4/1531.pdf}
}

@ARTICLE{destexhe2001,
  author = {Destexhe, A. and Rudolph, M. and Fellous, J. M. and Sejnowski, T.
	J. },
  title = {Fluctuating synaptic conductances recreate in vivo-like activity
	in neocortical neurons.},
  journal = {Neuroscience},
  year = {2001},
  volume = {107},
  pages = {13--24},
  number = {1},
  abstract = {To investigate the basis of the fluctuating activity present in neocortical
	neurons in vivo, we have combined computational models with whole-cell
	recordings using the dynamic-clamp technique. A simplified 'point-conductance'
	model was used to represent the currents generated by thousands of
	stochastically releasing synapses. Synaptic activity was represented
	by two independent fast glutamatergic and GABAergic conductances
	described by stochastic random-walk processes. An advantage of this
	approach is that all the model parameters can be determined from
	voltage-clamp experiments. We show that the point-conductance model
	captures the amplitude and spectral characteristics of the synaptic
	conductances during background activity. To determine if it can recreate
	in vivo-like activity, we injected this point-conductance model into
	a single-compartment model, or in rat prefrontal cortical neurons
	in vitro using dynamic clamp. This procedure successfully recreated
	several properties of neurons intracellularly recorded in vivo, such
	as a depolarized membrane potential, the presence of high-amplitude
	membrane potential fluctuations, a low-input resistance and irregular
	spontaneous firing activity. In addition, the point-conductance model
	could simulate the enhancement of responsiveness due to background
	activity.We conclude that many of the characteristics of cortical
	neurons in vivo can be explained by fast glutamatergic and GABAergic
	conductances varying stochastically.},
  address = {Unit\'{e} de Neurosciences Int\'{e}gratives et Computationnelles,
	CNRS, UPR-2191, Gif-sur-Yvette, France. destexhe@iaf.cnrs-gif.fr},
  citeulike-article-id = {554344},
  issn = {0306-4522},
  keywords = {dynamic-clamp, essay2},
  posted-at = {2006-03-16 15:42:40},
  priority = {2},
  url = {http://view.ncbi.nlm.nih.gov/pubmed/11744242}
}

@ARTICLE{destexhe03hcs,
  author = {Alain Destexhe and Michael Rudolph and Denis Pare},
  title = {The high-conductance state of neocortical neurons in vivo},
  journal = {Nature Reviews Neuroscience},
  year = {2003},
  volume = {4},
  pages = {739-751},
  keywords = {spiking}
}

@article{destexhe2007high,
  title={High-conductance state},
  author={Destexhe, Alain},
  journal={Scholarpedia},
  volume={2},
  number={11},
  pages={1341},
  year={2007}
}

@ARTICLE{deweerth95,
  author = {S. P. Deweerth and T. G. Morris},
  title = {CMOS current mode winner-take-all circuit with distributed hysteresis},
  journal = {Electronics Letters},
  year = {1995},
  volume = {31},
  pages = {1051--1053}
}

@ARTICLE{dewolf2020nengo,
  author = {DeWolf, Travis and Jaworski, Pawel and Eliasmith, Chris},
  title = {Nengo and Low-Power AI Hardware for Robust, Embedded Neurorobotics},
  journal = {Frontiers in Neurorobotics},
  volume = 14,
  year = 2020,
  doi = {10.3389/fnbot.2020.568359},
  issn = {1662-5218},
  abstract = {In this paper we demonstrate how the Nengo neural modeling and simulation libraries enable users to quickly develop robotic perception and action neural networks for simulation on neuromorphic hardware using tools they are already familiar with, such as Keras and Python. We identify four primary challenges in building robust, embedded neurorobotic systems, including: (1) developing infrastructure for interfacing with the environment and sensors; (2) processing task specific sensory signals; (3) generating robust, explainable control signals; and (4) compiling neural networks to run on target hardware. Nengo helps to address these challenges by: (1) providing the NengoInterfaces library, which defines a simple but powerful API for users to interact with simulations and hardware; (2) providing the NengoDL library, which lets users use the Keras and TensorFlow API to develop Nengo models; (3) implementing the Neural Engineering Framework, which provides white-box methods for implementing known functions and circuits; and (4) providing multiple backend libraries, such as NengoLoihi, that enable users to compile the same model to different hardware. We present two examples using Nengo to develop neural networks that run on CPUs and GPUs as well as Intel's neuromorphic chip, Loihi, to demonstrate two variations on this workflow. The first example is an implementation of an end-to-end spiking neural network in Nengo that controls a rover simulated in Mujoco. The network integrates a deep convolutional network that processes visual input from cameras mounted on the rover to track a target, and a control system implementing steering and drive functions in connection weights to guide the rover to the target. The second example uses Nengo as a smaller component in a system that has addressed some but not all of those challenges. Specifically it is used to augment a force-based operational space controller with neural adaptive control to improve performance during a reaching task using a real-world Kinova Jaco robotic arm. The code and implementation details are provided, with the intent of enabling other researchers to build and run their own neurorobotic systems.}
}

@PHDTHESIS{diesmann02,
  author = {Diesmann, Markus},
  title = {Conditions for Stable Propagation of Synchronous Spiking in Cortical
	Neural Networks: Single Neuron Dynamics and Network Properties},
  school = {Ruhr-Universit{\"a}t Bochum},
  year = {2002}
}

@INCOLLECTION{diesmann01nest,
  author = {Diesmann, Markus and Gewaltig, Marc-Oliver},
  title = {{NEST}: An Environment for Neural Systems Simulations},
  booktitle = {Forschung und wisschenschaftliches Rechnen, Beitr{\"a}ge zum Heinz-Billing-Preis
	2001},
  publisher = {Ges. f{\"u}r Wiss. Datenverarbeitung},
  year = {2002},
  editor = {Plesser, Theo and Macho, Volker},
  volume = {58},
  series = {GWDG-Bericht},
  pages = {43--70},
  address = {G{\"o}ttingen}
}

@article{diesmann2001state,
  title={State space analysis of synchronous spiking in cortical neural networks},
  author={Diesmann, Markus and Gewaltig, Marc-Oliver and Rotter, Stefan and Aertsen, Ad},
  journal={Neurocomputing},
  volume={38},
  pages={565--571},
  year={2001},
  publisher={Elsevier}
}

@ARTICLE{diesmann99,
  author = {M. Diesmann and M.-O. Gewaltig and A. Aertsen},
  title = {Stable propagation of synchronous spiking in cortical neural networks},
  journal = {Nature},
  year = {1999},
  volume = {402},
  pages = {529--533}
}

@MISC{dimitrov00natural,
  author = {A. Dimitrov and J. Miller},
  title = {Natural time scales for neural encoding},
  year = {2000},
  citeseercitationcount = {0},
  citeseerurl = {citeseer.nj.nec.com/dimitrov00natural.html},
  keywords = {spiking},
  text = {A. G. Dimitrov and J. P. Miller. Natural time scales for neural encoding.
	Neurocomputing, 32-33:1027-1034, 2000.}
}

@INPROCEEDINGS{diotalevi_weightperturbation,
  author = {Diotalevi, F. and Valle, M. and Bo, G.M and Caviglia, D.D.},
  title = {A {VLSI} Architecture for Weight Perturbation On Chip Learning Implementation},
  booktitle = {Proceedings IJCNN 2000},
  year = {2000},
  volume = {IV},
  pages = {219-224},
  groupsearch = {0},
  howpublished = {ISBN 0-7695-0619-4},
  key = {diotalevi_weightperturbation}
}

@ARTICLE{djurfeldt2008bluegene,
  author = {Djurfeldt, M. and Lundqvist, M. and Johansson, C. and Rehn, M. and
	Ekeberg, O. and Lansner, A.},
  title = {Brain-scale simulation of the neocortex on the IBM Blue Gene/L supercomputer},
  journal = {IBM Journal of Research and Development},
  year = {2008},
  volume = {52},
  pages = {31-41},
  number = {1.2},
  month = {January},
  abstract = {Biologically detailed large-scale models of the brain can now be simulated
	thanks to increasingly powerful massively parallel supercomputers.
	We present an overview, for the general technical reader, of a neuronal
	network model of layers II/III of the neocortex built with biophysical
	model neurons. These simulations, carried out on an IBM Blue Gene/L
	supercomputer, comprise up to 22 million neurons and 11 billion synapses,
	which makes them the largest simulations of this type ever performed.
	Such model sizes correspond to the cortex of a small mammal. The
	SPLIT library, used for these simulations, runs on single-processor
	as well as massively parallel machines. Performance measurements
	show good scaling behavior on the Blue Gene/L supercomputer up to
	8,192 processors. Several key phenomena seen in the living brain
	appear as emergent phenomena in the simulations. We discuss the role
	of this kind of model in neuroscience and note that full-scale models
	may be necessary to preserve natural dynamics. We also discuss the
	need for software tools for the specification of models as well as
	for analysis and visualization of output data. Combining models that
	range from abstract connectionist type to biophysically detailed
	will help us unravel the basic principles underlying neocortical
	function.},
  owner = {mpedro},
  timestamp = {2011.09.27}
}

@ARTICLE{dongarra2019race,
  author = {Dongarra, Jack and Gottlieb, Steven and Kramer, William T. C.},
  journal = {Computing in Science \& Engineering},
  title = {Race to Exascale},
  year = 2019,
  volume = 21,
  number = 1,
  pages = {4--5},
  doi = {10.1109/MCSE.2018.2882574}
}

@ARTICLE{douglas95neuromorphic,
  author = {Douglas, R. and Mahowald, M. and Mead, C.},
  title = {Neuromorphic analogue {VLSI}},
  journal = {Annu. Rev. Neurosci.},
  year = {1995},
  volume = {18},
  pages = {255-281}
}

@INBOOK{douglas04neocortex,
  chapter = {Neocortex},
  pages = {499-558},
  title = {The Synaptic Organization in the Brain},
  publisher = {Oxford University Press},
  year = {2004},
  editor = {Gordon M. Shepherd},
  author = {Douglas, Rodney and Markram, Henry and Martin, Kevan},
  edition = {5},
  isbn = {0-19-515955-1}
}

@ARTICLE{douglas04neuronal,
  author = {Rodney J Douglas and Kevan A C Martin},
  title = {Neuronal circuits of the neocortex},
  journal = {Annu Rev Neurosci},
  year = {2004},
  volume = {27},
  pages = {419--51},
  month = {Jan},
  abstract = {We explore the extent to which neocortical circuits generalize, i.e.,
	to what extent can neocortical neurons and the circuits they form
	be considered as canonical? We find that, as has long been suspected
	by cortical neuroanatomists, the same basic laminar and tangential
	organization of the excitatory neurons of the neocortex is evident
	wherever it has been sought. Similarly, the inhibitory neurons show
	characteristic morphology and patterns of connections throughout
	the neocortex. We offer a simple model of cortical processing that
	is consistent with the major features of cortical circuits: The superficial
	layer neurons within local patches of cortex, and within areas, cooperate
	to explore all possible interpretations of different cortical input
	and cooperatively select an interpretation consistent with their
	various cortical and subcortical inputs.}
}

@UNPUBLISHED{drepper2009,
  author = {Ulrich Drepper},
  title = {{F}utexes {A}re {T}ricky},
  note = {{Red Hat, Inc.}},
  month = aug,
  year = {2009}
}

@UNPUBLISHED{drepper2007,
  author = {Ulrich Drepper},
  title = {{What Every Programmer Should Know About Memory}},
  note = {{Red Hat, Inc.}},
  month = nov,
  year = {2007}
}

@UNPUBLISHED{drepper2005,
  author = {Ulrich Drepper and Ingo Molnar},
  title = {{The Native POSIX Thread Library for Linux}},
  note = {{Red Hat, Inc.}},
  month = feb,
  year = {2005}
}

@MISC{facets_d7-8,
  author = {TU Dresden},
  title = {Design the final digital network {ASIC}},
  howpublished = {{FACETS} Deliverable D7-8},
  year = {2010},
  keywords = {facets, deliverable, dnc}
}

@MISC{facets_d7-5,
  author = {TU Dresden},
  title = {Demonstrate the FPGA-based host interface as well as the communication
	between the individual components of the wafer-sclae system},
  howpublished = {{FACETS} Deliverable D7-5},
  year = {2008},
  keywords = {facets, deliverable},
  owner = {agruebl},
  timestamp = {2010.08.30}
}

@MISC{facets_DNC_spec,
  author = {TU Dresden},
  title = {{DNC} Specification},
  howpublished = {{FACETS} project internal documentation},
  year = {2008},
  keywords = {facets, dnc, specification},
  owner = {smillner},
  timestamp = {2010.08.31}
}

@MISC{dnc_if_spec,
  author = {TU Dresden},
  title = {{HICANN} - {D}igital {I}nterface {S}pecification},
  howpublished = {{FACETS} project internal documentation},
  year = {2007},
  keywords = {facets, dnc, hicann}
}

@ARTICLE{druckmann07,
  author = {Shaul Druckmann and Yoav Banitt and Albert Gidon and Felix Schürmann
	and Henry Markram and Idan Segev},
  title = {A Novel Multiple Objective Optimization Framework for Constraining
	Conductance-Based Neuron Models by Experimental Data},
  journal = {Front Neurosci},
  year = {2007},
  volume = {1},
  pages = {7-18},
  number = {1},
  month = {Nov},
  abstract = {We present a novel framework for automatically constraining parameters
	of compartmental models of neurons, given a large set of experimentally
	measured responses of these neurons. In experiments, intrinsic noise
	gives rise to a large variability (e.g., in firing pattern) in the
	voltage responses to repetitions of the exact same input. Thus, the
	common approach of fitting models by attempting to perfectly replicate,
	point by point, a single chosen trace out of the spectrum of variable
	responses does not seem to do justice to the data. In addition, finding
	a single error function that faithfully characterizes the distance
	between two spiking traces is not a trivial pursuit. To address these
	issues, one can adopt a multiple objective optimization approach
	that allows the use of several error functions jointly. When more
	than one error function is available, the comparison between experimental
	voltage traces and model response can be performed on the basis of
	individual features of interest (e.g., spike rate, spike width).
	Each feature can be compared between model and experimental mean,
	in units of its experimental variability, thereby incorporating into
	the fitting this variability. We demonstrate the success of this
	approach, when used in conjunction with genetic algorithm optimization,
	in generating an excellent fit between model behavior and the firing
	pattern of two distinct electrical classes of cortical interneurons,
	accommodating and fast-spiking. We argue that the multiple, diverse
	models generated by this method could serve as the building blocks
	for the realistic simulation of large neuronal networks.},
  eprint = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2570085/pdf/fnins-01-007.pdf},
  keywords = {Compartmental model, multi-objective optimization, noisy neurons,
	firing pattern, cortical interneurons},
  url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2570085/?report=abstract}
}

@webpage{dynapse2021nice,
  title = {NICE Workshop 2021: A Tiny Spiking Neural Network on Dynap-SE1 Board Simulator},
  url = {https://code.ini.uzh.ch/yigit/NICE-workshop-2021},
  year = 2021
}

@webpage{ebrains2022ri,
  title = {{EBRAINS} Research Infrastructure},
  url = {https://ebrains.eu},
  year = 2022
}

@MISC{EEMBC2013,
  author = {EEMBC, {E}mbedded {M}icroprocessor {B}enchmark {C}onsortium},
  title = {CoreMark Benchmark Scores},
  howpublished = {Website},
  month = {April},
  year = {2013},
  owner = {simon},
  timestamp = {2013.04.15},
  url = {http://www.coremark.org/benchmark/index.php?pg=benchmark}
}

@MISC{coremarkbench,
  author = {EEMBC, {E}mbedded {M}icroprocessor {B}enchmark {C}onsortium},
  title = {CoreMark Benchmark},
  howpublished = {website},
  year = {2012},
  owner = {simon},
  timestamp = {2013.01.16},
  url = {http://www.coremark.org/}
}

@ARTICLE{eccles1954cholinergic,
  author = {J. C. Eccles and P. Fatt and K. Koketsu},
  title = {Cholinergic and Inhibitory Synapses in a Pathway from Motor-Axon Collaterals to Motoneurones},
  journal = {J. Physiol.},
  year = {1954},
  volume = {126},
  pages = {524--562}
}

@ARTICLE{eccles1976symposium,
  author = {J. C. Eccles},
  title = {From Electrical to Chemical Transmission in the Central Nervous System},
  journal = {Notes Rec. R. Soc. Lond. 1},
  year = {1976},
  volume = {30},
  number = {2},
  pages = {219-230}
}

@INPROCEEDINGS{ehrlich_ssd07,
  author = {Ehrlich, M. and Mayr, C. and Eisenreich, H. and Henker, S. and Srowig,
	A. and Gr\"ubl, A. and Schemmel, J. and Sch\"uffny, R.},
  title = {Wafer-Scale {VLSI} Implementations of Pulse Coupled Neural Networks},
  booktitle = {Proceedings of the International Conference on Sensors, Circuits
	and Instrumentation Systems (SSD-07)},
  year = {2007},
  month = {March},
  keywords = {neuromorphic},
  owner = {agruebl}
}

@INPROCEEDINGS{ehrlich08,
  author = {Matthias Ehrlich and Karsten Wendt and Ren\'{e} Sch\"{u}ffny},
  title = {Parallel mapping algorithms for a novel mapping \& configuration
	software for the FACETS project},
  booktitle = {CEA'08: Proceedings of the 2nd WSEAS International Conference on
	Computer Engineering and Applications},
  year = {2008},
  pages = {152--157},
  address = {Stevens Point, Wisconsin, USA},
  publisher = {World Scientific and Engineering Academy and Society (WSEAS)},
  isbn = {978-960-6766-33-6},
  location = {Acapulco, Mexico}
}

@INPROCEEDINGS{ehrlich2010anniip,
  author = {Ehrlich, M. and Wendt, K. and Z\"uhl, L. and Sch\"uffny, R. and Br\"uderle,
	D. and M\"uller, E. and Vogginger, B.},
  title = {A software framework for mapping neural networks to a wafer-scale
	neuromorphic hardware system},
  booktitle = {Proceedings of the Artificial Neural Networks and Intelligent Information
	Processing Conference (ANNIIP) 2010},
  year = {2010},
  pages = {43--52}
}

@BOOK{eiben03evolutionary,
  title = {Introduction to Evolutionary Computing},
  publisher = {Springer Verlag},
  year = {2003},
  author = {Eiben, A. E. and Smith, J. E.},
  address = {Berlin, Heidelberg, New York},
  isbn = {3-540-40184-9}
}

@ARTICLE{ekeberg91,
  author = {Ekeberg, {\"O}. and Wall{\'e}n, P. and Lansner, A. and Trav{\'e}n,
	H. and Brodin, L. and Grillner, S.},
  title = {A computer based model for realistic simulations of neural networks},
  journal = {Biological Cybernetics},
  year = {1991},
  volume = {65},
  pages = {81-90},
  note = {10.1007/BF00202382},
  abstract = {The use of computer simulations as a neurophysiological tool creates
	new possibilities to understand complex systems and to test whether
	a given model can explain experimental findings. Simulations, however,
	require a detailed specification of the model, including the nerve
	cell action potential and synaptic transmission. We describe a neuron
	model of intermediate complexity, with a small number of compartments
	representing the soma and the dendritic tree, and equipped with Na
	+ , K + , Ca 2+ , and Ca 2+ dependent K + channels. Conductance changes
	in the different compartments are used to model conventional excitatory
	and inhibitory synaptic interactions. Voltage dependent NMDA-receptor
	channels are also included, and influence both the electrical conductance
	and the inflow of Ca 2+ ions. This neuron model has been designed
	for the analysis of neural networks and specifically for the simulation
	of the network generating locomotion in a simple vertebrate, the
	lamprey. By assigning experimentally established properties to the
	simulated cells and their synapses, it has been possible to verify
	the sufficiency of these properties to account for a number of experimental
	findings of the network in operation. The model is, however, sufficiently
	general to be useful for realistic simulation also of other neural
	systems.},
  affiliation = {Royal Institute of Technology Department of Numerical Analysis and
	Computing Science S-98 44 Stockholm Sweden S-100 44 Stockholm Sweden},
  issn = {0340-1200},
  issue = {2},
  keyword = {Computer Science},
  publisher = {Springer Berlin / Heidelberg},
  url = {http://dx.doi.org/10.1007/BF00202382}
}

@ARTICLE{elboustani2009,
  author = {El Boustani, Sami and Destexhe, Alain},
  title = {A Master Equation Formalism for Macroscopic Modeling of Asynchronous
	Irregular Activity States},
  journal = {Neural Computation},
  year = {2009},
  volume = {21},
  pages = {46--100},
  number = {1},
  publisher = {MIT Press},
  ty = {JOUR},
  url = {http://dx.doi.org/10.1162/neco.2009.02-08-710}
}

@MISC{visionshomepage,
  author = {{Electronic Vision(s) -- Website}},
  title = {Website},
  howpublished = {\url{http://www.kip.uni-heidelberg.de/vision}},
  year = {2011}
}

@INBOOK{elias99,
  chapter = {5},
  pages = {135--156},
  title = {{Building silicon nervous systems with dendritic tree neuromorphs}},
  publisher = {The MIT Press},
  year = {1998},
  editor = {Maass, W. and Bishop, C. M.},
  author = {Elias, J. G. and Northmore, D. P. M.},
  edition = {1st},
  month = nov,
  abstract = {{Traditional neural network computing uses continuous propagation
	of its signals, whereas biological networks use signal timing and
	frequency in their transmission and computation. <I>Pulsed Neural
	Networks</I> arose out of a two-day workshop at the Isaac Newton
	Institute for Mathematical Studies at Cambridge, and provides a broad
	overview of the comparatively recent developments in building and
	working with these machines.<p> A compelling foreword by Terrence
	J. Sejnowski explains the basics. Several tutorial chapters covering
	biological and electronic pulsed computing follow. The rest of the
	book is divided into two parts: "Implementations" and "Design and
	Analysis of Pulsed Neural Systems."<p> Useful for neuroscientists,
	engineers, and, of course, computer scientists, <I>Pulsed Neural
	Networks</I> requires a certain familiarity with traditional neural
	networks and demands a willingness to probe neurobiological theory.
	However, this text rewards readers for their hard work with a much
	more powerful and robust approach to the problems of neural computing.
	<I>--Rob Lightner</I> } {Most practical applications of artificial
	neural networks are based on a computational model involving the
	propagation of continuous variables from one processing unit to the
	next. In recent years, data from neurobiological experiments have
	made it increasingly clear that biological neural networks, which
	communicate through pulses, use the timing of the pulses to transmit
	information and perform computation. This realization has stimulated
	significant research on pulsed neural networks, including theoretical
	analyses and model development, neurobiological modeling, and hardware
	implementation.<br /> <br /> This book presents the complete spectrum
	of current research in pulsed neural networks and includes the most
	important work from many of the key scientists in the field. Terrence
	J. Sejnowski's foreword, "Neural Pulse Coding," presents an overview
	of the topic. The first half of the book consists of longer tutorial
	articles spanning neurobiology, theory, algorithms, and hardware.
	The second half contains a larger number of shorter research chapters
	that present more advanced concepts. The contributors use consistent
	notation and terminology throughout the book.<br /> <br /> <b>Contributors</b>:
	Peter S. Burge, Stephen R. Deiss, Rodney J. Douglas, John G. Elias,
	Wulfram Gerstner, Alister Hamilton, David Horn, Axel Jahnke, Richard
	Kempter, Wolfgang Maass, Alessandro Mortara, Alan F. Murray, David
	P. M. Northmore, Irit Opher, Kostas A. Papathanasiou, Michael Recce,
	Barry J. P. Rising, Ulrich Roth, Tim Schnauer, Terrence J. Sejnowski,
	John Shawe-Taylor, Max R. van Daalen, J. Leo van Hemmen, Philippe
	Venier, Hermann Wagner, Adrian M. Whatley, Anthony M. Zador.}},
  booktitle = {Pulsed Neural Networks},
  day = {20},
  howpublished = {Hardcover},
  isbn = {0262133504},
  location = {Cambridge, Massachusetts}
}

@ARTICLE{elias75,
  author = {Elias, S. A. and Grossberg, S.},
  title = {Pattern formation, contrast control, and oscillations in the short
	term memory of shunting on-center off-surround networks},
  journal = {Biol. Cybern.},
  year = {1975},
  volume = {20},
  pages = {69--98}
}

@mastersthesis{emmel2020msc,
  author   = {Arne Emmel},
  title    = {Inference with Convolutional Neural Networks on Analog Neuromorphic Hardware},
  school   = {Universit{\"a}t Heidelberg},
  year     = 2020,
  type     = {Master's Thesis},
  month    = nov
}

@ARTICLE{enck1994,
  author = {Enck, John},
  title = {Ethernet/802.3 and token ring/802.5},
  year = {1994},
  pages = {265--295},
  address = {San Diego, CA, USA},
  book = {Handbook of networking and connectivity},
  isbn = {0-12-482080-8},
  publisher = {Academic Press Professional, Inc.}
}

@MISC{bluebrain05,
  author = {{EPFL} and {IBM}},
  title = {Blue Brain Project},
  year = {2008},
  abstract = {In July 2005, EPFL and IBM announced an exciting new research initiative
	- a project to create a biologically accurate, functional model of
	the brain using IBM's Blue Gene supercomputer. Analogous in scope
	to the Genome Project, the Blue Brain will provide a huge leap in
	our understanding of brain function and dysfunction and help us explore
	solutions to intractable problems in mental health and neurological
	disease. At the end of 2006, the Blue Brain project had created a
	model of the basic functional unit of the brain, the neocortical
	column. At the push of a button, the model could reconstruct biologically
	accurate neurons based on detailed experimental data, and automatically
	connect them in a biological manner, a task that involves positioning
	around 30 million synapses in precise 3D locations. In November,
	2007, the Blue Brain project reached an important milestone and the
	conclusion of its first Phase, with the announcement of an entirely
	new data-driven process for creating, validating, and researching
	the neocortical column.},
  address = {Lausanne},
  institution = {EPFL},
  keywords = {alife, brain},
  url = {http://bluebrain.epfl.ch/}
}

@ARTICLE{eppler2008,
  author = {Eppler, Jochen M. and Helias, Moritz and Muller, Eilif and Diesmann,
	Markus and Gewaltig, Marc-Oliver},
  title = {{Py{NEST}}: a convenient interface to the {NEST} simulator},
  journal = {Front. Neuroinform.},
  year = {2008},
  volume = {2},
  number = {12},
  abstract = {The neural simulation tool NEST (http://www.nest-initiative.org) is
	a simulator for heterogeneous networks of point neurons or neurons
	with a small number of compartments. It aims at simulations of large
	neural systems with more than 10^4 neurons and 10^7 to 10^9 synapses.
	NEST is implemented in C++ and can be used on a large range of architectures
	from single-core laptops over multi-core desktop computers to super-computers
	with thousands of processor cores. Python (http://www.python.org)
	is a modern programming language that has recently received considerable
	attention in Computational Neuroscience. Python is easy to learn
	and has many extension modules for scientific computing (e.g. http://www.scipy.org).
	In this contribution we describe PyNEST, the new user interface to
	NEST. PyNEST combines NEST's efficient simulation kernel with the
	simplicity and flexibility of Python. Compared to NEST�s native simulation
	language SLI, PyNEST makes it easier to set up simulations, generate
	stimuli, and analyze simulation results. We describe how PyNEST connects
	NEST and Python and how it is implemented. With a number of examples,
	we illustrate how it is used.},
  keywords = {Python, modelling, integrate-and-fire neuron, large-scale simulation,
	scientific computing, Networks, programming},
  doi = {10.3389/neuro.11.012.2008},
}

@article{ermentrout1996type,
  title={Type I membranes, phase resetting curves, and synchrony},
  author={Ermentrout, Bard},
  journal={Neural computation},
  volume={8},
  number={5},
  pages={979--1001},
  year={1996},
  publisher={MIT Press}
}

@INPROCEEDINGS{eshelmann_icga89biases,
  author = {Eshelmann, L. J. and Caruana, R. A. and Schaffer, J. D.},
  title = {Biases in crossover landscape},
  booktitle = {Proceedings of the Third International Conference on Genetic Algorithms},
  year = {1989},
  editor = {Schaffer, J. D.},
  pages = {10--19},
  publisher = {Morgan Kaufmann}
}

@article {esser2016convolutional,
    author = {Esser, Steven K. and Merolla, Paul A. and Arthur, John V. and Cassidy, Andrew S. and Appuswamy, Rathinakumar and Andreopoulos, Alexander and Berg, David J. and McKinstry, Jeffrey L. and Melano, Timothy and Barch, Davis R. and di Nolfo, Carmelo and Datta, Pallab and Amir, Arnon and Taba, Brian and Flickner, Myron D. and Modha, Dharmendra S.},
    title = {Convolutional networks for fast, energy-efficient neuromorphic computing},
    volume = 113,
    number = 41,
    pages = {11441--11446},
    year = 2016,
    doi = {10.1073/pnas.1604850113},
    publisher = {National Academy of Sciences},
    issn = {0027-8424},
    URL = {https://www.pnas.org/content/113/41/11441},
    eprint = {https://www.pnas.org/content/113/41/11441.full.pdf},
    journal = {Proceedings of the National Academy of Sciences}
}

@article {esser2016convolutional_nourl,
    author = {Esser, Steven K. and Merolla, Paul A. and Arthur, John V. and Cassidy, Andrew S. and Appuswamy, Rathinakumar and Andreopoulos, Alexander and Berg, David J. and McKinstry, Jeffrey L. and Melano, Timothy and Barch, Davis R. and di Nolfo, Carmelo and Datta, Pallab and Amir, Arnon and Taba, Brian and Flickner, Myron D. and Modha, Dharmendra S.},
    title = {Convolutional networks for fast, energy-efficient neuromorphic computing},
    volume = 113,
    number = 41,
    pages = {11441--11446},
    year = 2016,
    doi = {10.1073/pnas.1604850113},
    publisher = {National Academy of Sciences},
    issn = {0027-8424},
    journal = {Proceedings of the National Academy of Sciences}
}


@BOOK{ex2,
  title = {A book He Wrote},
  publisher = {His Publisher},
  year = {1999},
  author = {Expert, Andreas Nikolaos},
  address = {Erewhon, NC}
}

@MISC{neuralensemble_homepage,
  author = {{N}eural {E}nsemble},
  title = {Website},
  howpublished = {\url{http://www.neuralensemble.org}},
  year = {2008}
}

@MISC{facets_m7-5,
  author = {{FACETS M7-5}},
  title = {Verify that the layer-2 communication reaches the bandwidth requirements
	for a multi-wafer system, including the host communication via {GBit-E}thernet},
  howpublished = {{FACETS} Milestone M7-5, UHEI and TUD},
  year = {2009},
  keywords = {facets, milestone},
  owner = {mueller}
}

@MISC{facetsproject_homepage,
  author = {{FACETS}},
  title = {{F}ast {A}nalog {C}omputing with {E}mergent {T}ransient {S}tates
	-- Project Website},
  howpublished = {\url{http://www.facets-project.org}},
  year = {2010}
}

@MISC{RFC3366,
  author = {G. Fairhurst},
  title = {{RFC 3366}: {A}dvice to link designers on link {A}utomatic {R}epeat
	re{Q}uest ({ARQ})},
  month = aug,
  year = {2002},
  online = {yes},
  status = {STANDARD},
  url = {http://www.rfc-editor.org/rfc/rfc3366.txt}
}

@ARTICLE{farina2001sta,
  author = {Farina, D. and Arendt-Nielsen, L. and Merletti, R. and Graven-Nielsen,
	T.},
  title = {A spike triggered averaging technique for high resolution assessment
	of single motor unit conduction velocity changes during fatiguing
	voluntary contractions},
  journal = {Engineering in Medicine and Biology Society, 2001. Proceedings of
	the 23rd Annual International Conference of the IEEE},
  year = {2001},
  volume = {2},
  pages = { 1097-1100 vol.2},
  issn = {1094-687X },
  keywords = { biomedical electrodes, delay estimation, electromyography, maximum
	likelihood estimation, medical signal processing, signal resolution
	electromyography, fatiguing voluntary contractions, high resolution
	assessment, intramuscular electrodes, linear electrode arrays, maximum
	likelihood delay estimators, multichannel surface signals, single
	motor unit conduction velocity, spike triggered averaging, submaximal
	muscle contractions, surface array technique, temporal resolution}
}

@INPROCEEDINGS{farquhar04,
  author = {Farquhar, E. and Abramson, D. and Hasler, P.},
  title = {A reconfigurable bidirectional active 2 dimensional dendrite model},
  booktitle = {Circuits and Systems, 2004. ISCAS '04. Proceedings of the 2004 International
	Symposium on},
  year = {2004},
  volume = {1},
  pages = { I-313 - I-316 Vol.1},
  month = {may},
  abstract = { The physical phenomenon of ion flow in dendrites can be easily modelled
	using a diffuser circuit, and biologically based active channels.
	Here we describe a 2 dimensional diffuser array which provides a
	general model for implementing and studying dendrites with an arbitrary
	arborization pattern. The array makes use of floating gate transistors
	for biasing, and has provided us with some promising results.},
  doi = {10.1109/ISCAS.2004.1328194},
  keywords = { 2D dendrite model; 2D diffuser array; arbitrary arborization pattern;
	bidirectional active dendrite; biologically-based active channels;
	circuit biasing; diffuser circuit; floating gate transistors; general
	model; ion flow; reconfigurable dendrite; dendrites; neural chips;
	neurophysiology; physiological models; transistors;}
}

@INPROCEEDINGS{farquhar06,
  author = {Farquhar, E. and Gordon, C. and Hasler, P.},
  title = {A field programmable neural array},
  booktitle = {Circuits and Systems, 2006. ISCAS 2006. Proceedings. 2006 IEEE International
	Symposium on},
  year = {2006},
  pages = {4 pp. -4117},
  month = {may},
  abstract = {An analog circuit capable of accurately emulating large complex cells,
	or multiple less complex ones is described. This circuit is termed
	the FPNA or the field programmable neural array. It is analogous
	to the more familiar FPGA, but is composed of biologically relevant
	circuit components including active channels, dendrites, and synapses.
	Taking each of these circuit models, and adding a routing structure
	capable of routing outputs from cells (or external inputs) to any
	individual synapse at any node yields a device which is capable of
	emulating complex biological circuits. This circuit opens doors to
	investigating what particular types of computation individual cells
	are performing, as well as small networks simpler cells},
  doi = {10.1109/ISCAS.2006.1693534},
  keywords = {analog circuit;biologically relevant circuit components;field programmable
	neural array;routing structure;analogue circuits;network routing;neural
	nets;programmable logic arrays;}
}

@ARTICLE{farquhar05,
  author = {Farquhar, E. and Hasler, P.},
  title = {A bio-physically inspired silicon neuron},
  journal = {Circuits and Systems I: Regular Papers, IEEE Transactions on},
  year = {2005},
  volume = {52},
  pages = { 477 - 488},
  number = {3},
  month = {march},
  abstract = {The physical principles governing ion flow in biological neurons share
	interesting similarities to electron flow through the channels of
	MOSFET transistors. Here, is described a circuit which exploits the
	similarities better than previous approaches to build an elegant
	circuit with electrical properties similar to real biological neurons.
	A two-channel model is discussed including sodium (Na+) and potassium
	(K+). The Na+ channel uses four transistors and two capacitors. The
	K+ channel uses two transistors and one capacitor. One more capacitor
	simulates the neuron membrane capacitance yielding a total circuit
	of four capacitors and six transistors. This circuit operates in
	real-time, is fabricated on standard CMOS processes, runs in subthreshold,
	and has a power supply similar to that of real biology. Voltage and
	current responses of this circuit correspond well with biology in
	terms of shape, magnitude, and time.},
  doi = {10.1109/TCSI.2004.842871},
  issn = {1549-8328},
  keywords = { CMOS process; MOSFET transistor; analog circuits; bioelectric potentials;
	biological cells; biological neurons; biophysically inspired silicon
	neuron; capacitors; electron flow; nervous system; neuron membrane
	capacitance; two-channel model; CMOS integrated circuits; MOSFET;
	bioelectric potentials; capacitors; neural nets;}
}

@ARTICLE{Farries2007,
  author = {Farries, Michael A. and Fairhall, Adrienne L.},
  title = {Reinforcement Learning With Modulated Spike Timing–Dependent Synaptic
	Plasticity},
  journal = {Journal of Neurophysiology},
  year = {2007},
  volume = {98},
  pages = {3648-3665},
  number = {6},
  month = {December},
  abstract = {Spike timing–dependent synaptic plasticity (STDP) has emerged as the
	preferred framework linking patterns of pre- and postsynaptic activity
	to changes in synaptic strength. Although synaptic plasticity is
	widely believed to be a major component of learning, it is unclear
	how STDP itself could serve as a mechanism for general purpose learning.
	On the other hand, algorithms for reinforcement learning work on
	a wide variety of problems, but lack an experimentally established
	neural implementation. Here, we combine these paradigms in a novel
	model in which a modified version of STDP achieves reinforcement
	learning. We build this model in stages, identifying a minimal set
	of conditions needed to make it work. Using a performance-modulated
	modification of STDP in a two-layer feedforward network, we can train
	output neurons to generate arbitrarily selected spike trains or population
	responses. Furthermore, a given network can learn distinct responses
	to several different input patterns. We also describe in detail how
	this model might be implemented biologically. Thus our model offers
	a novel and biologically plausible implementation of reinforcement
	learning that is capable of training a neural population to produce
	a very wide range of possible mappings between synaptic input and
	spiking output.},
  doi = {10.1152/jn.00364.2007},
  eprint = {http://jn.physiology.org/content/98/6/3648.full.pdf+html},
  url = {http://jn.physiology.org/content/98/6/3648.abstract}
}

@INPROCEEDINGS{fasnacht2008,
  author = {Fasnacht, D.B. and Whatley, A.M. and Indiveri, G.},
  title = {A serial communication infrastructure for multi-chip address event
	systems},
  booktitle = {Circuits and Systems, 2008. ISCAS 2008. IEEE International Symposium
	on},
  year = {2008},
  pages = {648 -651},
  month = {may},
  doi = {10.1109/ISCAS.2008.4541501},
  keywords = {AER communication infrastructures;address-event representation;communication
	protocol;multi-chip address event systems;serial AER interface;serial
	communication infrastructure;multichip modules;protocols;}
}

@ARTICLE{wafermap07,
  author = {J. Fieres},
  title = {L1 mapping documentation},
  journal = {WP7 documentation},
  year = {2007}
}

@article{feldmann2019all,
  title={All-optical spiking neurosynaptic networks with self-learning capabilities},
  author={Feldmann, Johannes and Youngblood, Nathan and Wright, C David and Bhaskaran, Harish and Pernice, Wolfram HP},
  journal={Nature},
  volume=569,
  number=7755,
  pages={208--214},
  year=2019,
  publisher={Nature Publishing Group}
}

@misc{feynman2011feynman,
  title={Feynman Lectures on Physics, Vol. II--The New Millennium Edition: Mainly Electromagnetism and Matter},
  author={Feynman, Richard P and Leighton, Robert B and Sands, Matthew},
  year={2011},
  publisher={Basic Books (New York)}
}

@INPROCEEDINGS{fick2017analog,
  author={Fick, L. and Blaauw, D. and Sylvester, D. and Skrzyniarz, S. and Parikh, M. and Fick, D.},
  booktitle={2017 {IEEE} Custom Integrated Circuits Conference ({CICC})},
  title={Analog in-memory subthreshold deep neural network accelerator},
  year=2017,
  pages={1--4},
  doi={10.1109/CICC.2017.7993629}
}

@INPROCEEDINGS{fick2022analog,
  author={Fick, Laura and Skrzyniarz, Skylar and Parikh, Malav and Henry, Michael B. and Fick, David},
  booktitle={2022 {IEEE} International Solid- State Circuits Conference ({ISSCC})},
  title={Analog Matrix Processor for Edge AI Real-Time Video Analytics},
  year=2022,
  volume=65,
  pages={260--262},
  doi={10.1109/ISSCC42614.2022.9731773}
}

@PHDTHESIS{fieres06method,
  author = {Johannes Fieres},
  title = {A Method for Image Classification Using Low-Precision Analog Computing
	Arrays},
  school = {Ruprecht-Karls University Heidelberg},
  year = {2006},
  abstract = {Das Rechnen mit analogen integrierten Schaltkreisen kann gegenüber
	der weit verbreiteten Digitaltechnik einige Vorteile bieten, z.B.:
	geringerer Fläche- und Stromverbrauch und die M¨oglichkeit der massiven
	Parallelisierung. Dabei muss allerdings aufgrund unvermeidlicher
	Produktionsschwankungen und analogen Rauschens auf die Präzision
	digitaler Rechner verzichtet werden. Künstliche neuronale Netzwerke
	sind hinsichtlich einer Realisierung in paralleler, analoger Elektronik
	gut geeignet. Erstens zeigen sie immanente Paralleliät und zweitens
	können sie sich durch Training an eventuelle Hardwarefehler anpassen.
	Diese Dissertation untersucht die Implementierbarkeit eines neuronalen
	Faltungsnetzwerkes zur Bilderkennung auf einem massiv parallelen
	Niedrigleistungs-Hardwaresystem. Das betrachtete, gemischt analog-digitale,
	Hardwaremodell realisiert einfache Schwellwertneuronen. Geeignete
	gradientenfreie Trainingsalgorithmen, die Elemente der Selbstorganisation
	und des überwachten Lernens verbinden, werden entwickelt und an zwei
	Testproblemen (handschrifltiche Ziffern (MNIST) und Verkehrszeichen)
	erprobt. In Softwaresimulationen wird das Verhalten der Methode unter
	verschiedenen Arten von Rechenfehlern untersucht. Durch die Einbeziehung
	der Hardware in die Trainingsschleife können selbst schwere Rechenfehler,
	ohne dass diese quantifiziert werden müssen, implizit ausgeglichen
	werden. Nicht zuletzt werden die entwickelten Netzwerke und Trainingstechniken
	auf einem existierenden Prototyp-Chip überprüft.},
  keywords = {convolutional NN, vlsi, vision},
  owner = {fieres}
}

@INPROCEEDINGS{fieres_bics04,
  author = {J. Fieres and A. Gr{\"u}bl and S. Philipp and K. Meier and J. Schemmel
	and F. Sch{\"u}rmann},
  title = {A Platform for Parallel Operation of {VLSI} Neural Networks},
  booktitle = {Proc.\ of the 2004 Brain Inspired Cognitive Systems Conference (BICS2004)},
  year = {2004},
  address = {University of Stirling, Scotland, UK},
  key = {fieres_backplane},
  keywords = {vision, vlsi, convolutional NN, neuromorphic}
}

@INPROCEEDINGS{fieres_ijcnn2008,
  author = {Fieres, J. and Schemmel, J. and Meier, K.},
  title = {Realizing Biological Spiking Network Models in a Configurable Wafer-Scale
	Hardware System},
  booktitle = {Proceedings of the 2008 International Joint Conference on Neural
	Networks (IJCNN)},
  year = {2008},
  keywords = {neuromorphic}
}

@INPROCEEDINGS{fieres06convolutional,
  author = {J. Fieres and J. Schemmel and K. Meier},
  title = {A convolutional neural network tolerant of synaptic faults for low-power
	analog hardware},
  booktitle = {Proceedings of 2nd IAPR International Workshop on ArtificialNeural
	Networks in Pattern Recognition (ANNPR 2006), Springer LectureNotes
	in Artificial Intelligence},
  year = {2006},
  volume = {4087},
  pages = {122--132},
  abstract = {Recently, the authors described a training method for a convolutional
	neural network of threshold neurons. Hidden layers are trained by
	by clustering, in a feed-forward manner, while the output layer is
	trained using the supervised Perceptron rule. The system is designed
	for implementation on an existing low-power analog hardware architecture,
	exhibiting inherent error sources affecting the computation accuracy
	in unspecified ways. One key technique is to train the network on-chip,
	taking possible errors into account without any need to quantify
	them. For the hidden layers, an on-chip approach has been applied
	previously. In the present work, a chip-in-the-loop version of the
	iterative Perceptron rule is introduced for training the output layer.
	Influences of various types of errors are thoroughly investigated
	(noisy, deleted, and clamped weights) for all network layers, using
	the MNIST database of hand-written digits as a benchmark.},
  keywords = {convolutional NN, vlsi, vision, neuromorphic},
  owner = {fieres}
}

@BOOK{fischer86linalg,
  title = {Lineare Algebra},
  publisher = {Friedrich Vieweg und Sohn Verlagsgesellschaft mbH},
  year = {1986},
  author = {J. Fieres and J. Schemmel and K. Meier},
  address = {Braunschweig, Wiesbaden}
}

@article{fiser2010statistically,
  title={Statistically optimal perception and learning: from behavior to neural representations},
  author={Fiser, J{\'o}zsef and Berkes, Pietro and Orb{\'a}n, Gerg{\H{o}} and Lengyel, M{\'a}t{\'e}},
  journal={Trends in cognitive sciences},
  volume={14},
  number={3},
  pages={119--130},
  year={2010},
  publisher={Elsevier}
}

@article{fitzhugh1961impulses,
  title={Impulses and physiological states in theoretical models of nerve membrane},
  author={FitzHugh, Richard},
  journal={Biophysical journal},
  volume={1},
  number={6},
  pages={445--466},
  year={1961},
  publisher={Elsevier}
}

@BOOK{fliessbach95statphys,
  title = {Statistische Physik},
  publisher = {Spektrum Akademischer Verlag GmbH},
  year = {1995},
  author = {Flie{\ss}bach, Thorsten},
  address = {Heidelberg},
  isbn = {3-86025-715-3}
}

@ARTICLE{Florian2007,
  author = {Florian, Răzvan V.},
  title = {Reinforcement Learning Through Modulation of Spike-Timing-Dependent
	Synaptic Plasticity},
  journal = {Neural Computation},
  year = {2007},
  volume = {19},
  pages = {1468--1502},
  number = {6},
  month = apr,
  booktitle = {Neural Computation},
  comment = {doi: 10.1162/neco.2007.19.6.1468},
  issn = {0899-7667},
  owner = {simon},
  publisher = {MIT Press},
  timestamp = {2012.12.14},
  url = {http://dx.doi.org/10.1162/neco.2007.19.6.1468}
}

@ARTICLE{flower93summed,
  author = {Flower, B. and Jabri, M.},
  title = {Summed weight neuron perturbation: {A}n $\mathcal{O}(n)$ improvement
	over weight perturbation},
  journal = {Advances in Neural Information Processing Systems},
  year = {1993},
  volume = {5},
  pages = {212--219},
  address = {San Mateo, CA},
  file = {flower93summed.pdf:flower93summed.pdf:PDF},
  publisher = {Morgan Kaufmann}
}

@ARTICLE{Flynn1972,
  author = {Flynn, Michael J},
  title = {Some computer organizations and their effectiveness},
  journal = {Computers, IEEE Transactions on},
  year = {1972},
  volume = {100},
  pages = {948--960},
  number = {9},
  owner = {simon},
  publisher = {IEEE},
  timestamp = {2013.03.11}
}

@BOOK{fogel95evolutionary,
  title = {{E}volutionary {C}omputation -- Toward a New Philosophy of Machine
	Intelligence},
  publisher = {IEEE Press},
  year = {1995},
  author = {Fogel, D. B.},
  address = {New York},
  isbn = {0-7803-1038-1}
}

@ARTICLE{fogel90evolving,
  author = {Fogel, D. B. and Fogel, L. J. and Porto, V. W.},
  title = {Evolving Neural Networks},
  journal = {Biological Cybernetics},
  year = {1990},
  volume = {63},
  pages = {487--493},
  address = {Berlin, Heidelberg, New York},
  publisher = {Springer Verlag}
}

@INPROCEEDINGS{folowosele2009switched,
  author = {Folowosele, F. and Harrison, A. and Cassidy, A. and Andreou, A.G.
	and Etienne-Cummings, R. and Mihalas, S. and Niebur, E. and Hamilton,
	T.J.},
  title = {A switched capacitor implementation of the generalized linear integrate-and-fire
	neuron},
  booktitle = {Circuits and Systems, 2009. ISCAS 2009. IEEE International Symposium
	on},
  year = {2009},
  pages = {2149--2152},
  organization = {IEEE}
}

@BOOK{forster00analysis1,
  title = {Analysis 1},
  publisher = {Friedrich Vieweg und Sohn Verlagsgesellschaft mbH},
  year = {2000},
  author = {Forster, Otto},
  address = {Braunschweig, Wiesbaden},
  isbn = {3-528-47224-3}
}

@BOOK{forster92analysis3,
  title = {Analysis 3},
  publisher = {Friedrich Vieweg und Sohn Verlagsgesellschaft mbH},
  year = {1992},
  author = {Forster, Otto},
  address = {Braunschweig, Wiesbaden},
  isbn = {3-528-27252-X}
}

@BOOK{forster84analysis2,
  title = {Analysis 2},
  publisher = {Friedrich Vieweg und Sohn Verlagsgesellschaft mbH},
  year = {1984},
  author = {Forster, Otto},
  address = {Braunschweig, Wiesbaden},
  isbn = {3-528-37231-1}
}

@INBOOK{fregnac94advances,
  chapter = {Models of synaptic plasticity and cellular analogs of learning in
	the developing and adult vertebrate visual cortex},
  pages = {148--235},
  title = {Advances in neural and behavioral development},
  publisher = {Ablex Publ. Corp., Norwood, New Jersey},
  year = {1994},
  editor = {V.A. Casgrande and P.G. Shinkman},
  author = {Y. Fr\'egnac and D. Shulz},
  owner = {fieres}
}

@INBOOK{fregnac94plasticity,
  chapter = {4},
  pages = {148-235},
  title = {Models of synaptic plasticity and cellular analogs of learning in
	the developing and adult vertebrate visual cortex. In: Advances in
	neural and behavioral development},
  publisher = {Ablex Publ. Corp., Norwood, New Jersey},
  year = {1994},
  editor = {V.A. Casgrande and P.G. Shinkman},
  author = {Y. Fr\'egnac and D. Shulz},
  keywords = {learning},
  owner = {fieres}
}

@ARTICLE{Fremaux2013,
  author = {Fr\'emaux, Nicolas AND Sprekeler, Henning AND Gerstner, Wulfram},
  title = {Reinforcement Learning Using a Continuous Time Actor-Critic Framework
	with Spiking Neurons},
  journal = {PLoS Comput Biol},
  year = {2013},
  volume = {9},
  pages = {e1003024},
  number = {4},
  month = {04},
  abstract = {<title>Author Summary</title><p>As every dog owner knows, animals
	repeat behaviors that earn them rewards. But what is the brain machinery
	that underlies this reward-based learning? Experimental research
	points to plasticity of the synaptic connections between neurons,
	with an important role played by the neuromodulator dopamine, but
	the exact way synaptic activity and neuromodulation interact during
	learning is not precisely understood. Here we propose a model explaining
	how reward signals might interplay with synaptic plasticity, and
	use the model to solve a simulated maze navigation task. Our model
	extends an idea from the theory of reinforcement learning: one group
	of neurons form an “actor,” responsible for choosing the direction
	of motion of the animal. Another group of neurons, the “critic,”
	whose role is to predict the rewards the actor will gain, uses the
	mismatch between actual and expected reward to teach the synapses
	feeding both groups. Our learning agent learns to reliably navigate
	its maze to find the reward. Remarkably, the synaptic learning rule
	that we derive from theoretical considerations is similar to previous
	rules based on experimental evidence.</p>},
  doi = {10.1371/journal.pcbi.1003024},
  owner = {simon},
  publisher = {Public Library of Science},
  timestamp = {2013.04.19},
  url = {http://dx.doi.org/10.1371%2Fjournal.pcbi.1003024}
}

@ARTICLE{Fremaux2010,
  author = {Nicolas Fr\'emaux and Henning Sprekeler and Wulfram Gerstner},
  title = {Functional Requirements for Reward-Modulated Spike-Timing-Dependent
	Plasticity},
  journal = {The Journal of Neuroscience},
  year = {2010},
  volume = {30},
  pages = {13326-13337},
  owner = {sfriedma},
  timestamp = {2012.09.26}
}

@article{frenkel2018online,
  title={A 0.086-mm$^{2} $12.7-pJ/SOP 64k-synapse 256-neuron online-learning digital spiking neuromorphic processor in 28-nm CMOS},
  author={Frenkel, Charlotte and Lefebvre, Martin and Legat, Jean-Didier and Bol, David},
  journal={IEEE transactions on biomedical circuits and systems},
  volume=13,
  number=1,
  pages={145--158},
  year=2018,
  publisher={IEEE}
}

@article{frenkel2019morphic,
  title={MorphIC: A 65-nm 738k-Synapse/mm$^2$ quad-core binary-weight digital neuromorphic processor with stochastic spike-driven online learning},
  author={Frenkel, Charlotte and Legat, Jean-Didier and Bol, David},
  journal={IEEE transactions on biomedical circuits and systems},
  volume=13,
  number=5,
  pages={999--1010},
  year=2019,
  publisher={IEEE}
}

@INPROCEEDINGS{frenkel2020convolutional,
  author = {Frenkel, Charlotte and Legat, Jean-Didier and Bol, David},
  title = {A 28-nm Convolutional Neuromorphic Processor Enabling Online Learning with Spike-Based Retinas},
  booktitle = {2020 {IEEE} International Symposium on Circuits and Systems ({ISCAS})},
  year = 2020,
  pages = {1--5},
  doi = {10.1109/ISCAS45731.2020.9180440}
}

@INPROCEEDINGS{frenkel2022reckon,
  author = {Frenkel, Charlotte and Indiveri, Giacomo},
  title = {{ReckOn}: A 28nm Sub-mm2 Task-Agnostic Spiking Recurrent Neural Network Processor Enabling On-Chip Learning over Second-Long Timescales},
  booktitle = {2022 {IEEE} International Solid- State Circuits Conference ({ISSCC})},
  year = {2022},
  volume = {65},
  pages = {1--3},
  doi = {10.1109/ISSCC42614.2022.9731734}
}

@ARTICLE{frean90upstart,
  author = {Frean, M.},
  title = {The Upstart Algorithm: {A} Method for Constructing and Training Feedforward
	Neural Networks},
  journal = {Neural Computation},
  year = {1990},
  volume = {2},
  pages = {198--209}
}

@MISC{FreeSoftwareFoundation2013,
  author = {{Free Software Foundation}},
  title = {Binutils website},
  howpublished = {website},
  month = {March},
  year = {2013},
  note = {Version 2.21.51},
  owner = {simon},
  timestamp = {2013.03.27},
  url = {http://www.gnu.org/software/binutils/}
}

@MISC{freeglut,
  author = {Free{GLUT}},
  title = {The Free{GLUT} Project Website},
  howpublished = {\url{http://freeglut.sourceforge.net/}},
  key = {freeglut}
}

@INPROCEEDINGS{frey2000future,
  author = {Frey, D.},
  title = {Future implications of the log domain paradigm},
  booktitle = {Circuits, Devices and Systems, IEE Proceedings-},
  year = {2000},
  volume = {147},
  number = {1},
  pages = {65--72},
  organization = {IET}
}

@INPROCEEDINGS{frey_07mea,
  author = {U. Frey and C. D. Sanchez-Bustamante and T. Ugniwenko and F. Heer
	and J. Sedivy and S. Hafizovic and B. Roscic and M. Fussenegger and
	A. Blau and U. Egert and A. Hierlemann},
  title = {Cell Recordings with a CMOS High-density Microelectrode Array},
  booktitle = {Engineering in Medicine and Biology Society, 2007. EMBS 2007. 29th
	Annual International Conference of the IEEE},
  year = {2007},
  pages = {167--170},
  address = {Lyon},
  month = aug,
  abstract = {Recordings have been performed with a CMOS-based microelectrode array
	(MEA) featuring 11'016 metal electrodes and 126 channels, each of
	which comprises recording and stimulation electronics for extracellular,
	bidirectional communication with electrogenic cells. The important
	features of the device include (i) high spatial resolution at (sub)cellular
	level with 3'200 electrodes per mm2 (diameter 7 mum, pitch 18 mum),
	(ii) a reconfigurable routing of the electrodes to the 126 channels,
	and (iii) low noise levels. Recordings from neonatal rat cardiomyocytes
	forming confluent layers and microtissues are shown. Moreover, signals
	from dissociated rat hippocampal neurons and from neurons in an acute
	cerebellar slice preparation are presented.},
  owner = {mueller},
  timestamp = {2008.09.07}
}
@ARTICLE{friedmannschemmel2016, crossref = {friedmann2016hybridlearning} }
@ARTICLE{friedmannschemmel2016hybridlearning, crossref = {friedmann2016hybridlearning} }
@ARTICLE{friedmann2016hybridlearning,
  author={Simon Friedmann and Johannes Schemmel and Andreas Gr\"ubl and Andreas Hartel and Matthias Hock and Karlheinz Meier},
  journal={IEEE Transactions on Biomedical Circuits and Systems},
  title={Demonstrating Hybrid Learning in a Flexible Neuromorphic Hardware System},
  year=2017,
  volume=11,
  number=1,
  pages={128--142},
  keywords={Biological system modeling;Correlation;Hardware;Mathematical model;Neuromorphics;Neurons;Digital signal processing;learning;neuromorphic hardware;spike-time dependent plasticity;synapse circuit},
  doi={10.1109/TBCAS.2016.2579164},
  ISSN={1932-4545},
  month={}
}

@MISC{vkarasen11bathesis,
	author = {Karasenko, Vitali},

	title = {Design, implementation and testing of a high speed reliable
		link over an unreliable medium between a host computer and a
			Xilinx Virtex5 FPGA},
	howpublished = {Bachelor's thesis (English), University of Heidelberg},
	year = {2011},
	abstract = {The presented thesis describes a method of communication between a host
		computer and a Virtex5 FPGA over a packet network. The link features speeds of
		up to 1 Gbit/s in full duplex mode and provides error correction and retransmit
		functionality to ensure lossless communication over a lossy medium. The
		protocol was implemented using VHDL. Testing the protocol yielded net
		bandwidths of over 100 MB/s in both directions for realistic use cases. The
		throughput is unstable at large timescales with as of yet unknown reasons. The
		only bug that was found during testing is described with a workaround which
		does not affect the usability of the system.},
}

@MISC{vkarasen14masterthesis,
	author = {Karasenko, Vitali},

	title = {A communication infrastructure for a neuromorphic system},
	howpublished = {Master's thesis (English), University of Heidelberg},
	year = {2014},
	abstract = {This thesis presents the integration and testing of a
		generic ARQ protocol into two distinct network architectures
		which are part of the BrainScaleS HMF. The achieved net
		bandwidths of up to 117 MB/s for the host link and up
		to 83 MB/s for the FPGA-HICANN connection are a
		significant improvement over the existing
		implementation. Long-term evaluation proved the
		necessary protocol stability to allow production usage
		in the HMF. Several discovered shortcomings in the
		system are discussed and suggestions for their
		improvements are provided.},
}

@phdthesis{vkarasen20phd,
  author   = {Karasenko, Vitali},
  title    = {Von {N}eumann bottlenecks in non-von {N}eumann computing architectures},
  school   = {Universit\"at Heidelberg},
  year     = 2020,
  url      = {http://archiv.ub.uni-heidelberg.de/volltextserver/28691/1/KarasenkoPhD.pdf}
}

@ARTICLE{Friedmann2013, crossref={friedmann2013reward}}
@ARTICLE{friedmann2013reward,

AUTHOR={Friedmann, Simon and Fr\'emaux, Nicolas and Schemmel, Johannes and Gerstner, Wulfram and Meier, Karlheinz},

TITLE={Reward-based learning under hardware constraints — using a {RISC} processor embedded in a neuromorphic substrate},

JOURNAL={Frontiers in Neuroscience},

VOLUME={7},

PAGES={160},

YEAR={2013},

URL={http://journal.frontiersin.org/article/10.3389/fnins.2013.00160},

DOI={10.3389/fnins.2013.00160},

ISSN={1662-453X},

ABSTRACT={In this study, we propose and analyze in simulations a new, highly flexible method of imple-
menting synaptic plasticity in a wafer-scale, accelerated neuromorphic hardware system. The
study focuses on globally modulated STDP, as a special use-case of this method. Flexibility is
achieved by embedding a general-purpose processor dedicated to plasticity into the wafer. To
evaluate the suitability of the proposed system, we use a reward modulated STDP rule in a spike
train learning task. A single layer of neurons is trained to fire at specific points in time with
only the reward as feedback. This model is simulated to measure its performance, i.e. the in-
crease in received reward after learning. Using this performance as baseline, we then simulate
the model with various constraints imposed by the proposed implementation and compare the
performance. The simulated constraints include discretized synaptic weights, a restricted inter-
face between analog synapses and embedded processor, and mismatch of analog circuits. We
find that probabilistic updates can increase the performance of low-resolution weights, a simple
interface between analog synapses and processor is sufficient for learning, and performance is
insensitive to mismatch. Further, we consider communication latency between wafer and the
conventional control computer system that is simulating the environment. This latency increases
the delay, with which the reward is sent to the embedded processor. Because of the time continu-
ous operation of the analog synapses, delay can cause a deviation of the updates as compared to
the not delayed situation. We find that for highly accelerated systems latency has to be kept to a
minimum. This study demonstrates the suitability of the proposed implementation to emulate
the selected reward modulated STDP learning rule. It is therefore an ideal candidate for imple-
mentation in an upgraded version of the wafer-scale system developed within the BrainScaleS
project.}
}

@MISC{friedrich2014intership,
	tite={Charakterisierung der Inter-Compartment-Conductance},
	author={Arno Friedrich},
	month={jun},
	year=2014,
	note={Internship Report}
}

@article{friston2011action,
  title={Action understanding and active inference},
  author={Friston, Karl and Mattout, J{\'e}r{\'e}mie and Kilner, James},
  journal={Biological cybernetics},
  volume={104},
  number={1-2},
  pages={137--160},
  year={2011},
  publisher={Springer}
}

@ARTICLE{Froemke2002,
  author = {Froemke, Robert C. and Dan, Yang},
  title = {Spike-timing-dependent synaptic modification induced by natural spike
	trains},
  journal = {Nature},
  year = {2002},
  volume = {416},
  pages = {433--438},
  number = {6879},
  month = mar,
  comment = {10.1038/416433a},
  issn = {0028-0836},
  owner = {sfriedma},
  timestamp = {2012.11.01},
  url = {http://dx.doi.org/10.1038/416433a}
}

@ARTICLE{Froemke2010,
  author = {Froemke, Robert C and Debanne, Dominique and Bi, Guo-Qiang},
  title = {Temporal modulation of spike-timing-dependent plasticity},
  journal = {Frontiers in Synaptic Neuroscience},
  year = {2010},
  volume = {2},
  number = {19},
  abstract = {Spike-timing-dependent plasticity (STDP) has attracted considerable
	experimental and theoretical attention over the last decade. In the
	most basic formulation, STDP provides a fundamental unit- a spike
	pair- for quantifying the induction of long-term changes in synaptic
	strength. However, many factors, both pre- and postsynaptic, can
	affect synaptic transmission and integration, especially when multiple
	spikes are considered. Here we review the experimental evidence for
	multiple types of nonlinear temporal interactions in STDP, focusing
	on the contributions of individual spike pairs, overall spike rate,
	and precise spike timing for modification of cortical and hippocampal
	excitatory synapses. We discuss the underlying processes that determine
	the specific learning rules at different synapses, such as postsynaptic
	excitability and short-term depression. Finally, we describe the
	success of efforts towards building predictive, quantitative models
	of how complex and natural spike trains induce long-term synaptic
	modifications.},
  doi = {10.3389/fnsyn.2010.00019},
  issn = {1663-3563},
  url = {http://www.frontiersin.org/synaptic_neuroscience/10.3389/fnsyn.2010.00019/abstract}
}

@ARTICLE{Froemke2010a,
  author = {Froemke, Robert C and Letzkus, Johannes J and Kampa, Bjorn and Hang,
	Giao B and Stuart, Greg},
  title = {Dendritic synapse location and neocortical spike-timing-dependent
	plasticity},
  journal = {Frontiers in Synaptic Neuroscience},
  year = {2010},
  volume = {2},
  number = {29},
  abstract = {While it has been appreciated for decades that synapse location in
	the dendritic tree has a powerful influence on signal processing
	in neurons, the role of dendritic synapse location on the induction
	of long-term synaptic plasticity has only recently been explored.
	Here, we review recent work revealing how learning rules for spike-timing-dependent
	plasticity (STDP) in cortical neurons vary with the spatial location
	of synaptic input. A common principle appears to be that proximal
	synapses show conventional STDP, whereas distal inputs undergo plasticity
	according to novel learning rules. One crucial factor determining
	location-dependent STDP is the backpropagating action potential,
	which tends to decrease in amplitude and increase in width as it
	propagates into the dendritic tree of cortical neurons. We discuss
	additional location-dependent mechanisms as well as the functional
	implications of heterogeneous learning rules at different dendritic
	locations for the organization of synaptic inputs.},
  doi = {10.3389/fnsyn.2010.00029},
  issn = {1663-3563},
  url = {http://www.frontiersin.org/synaptic_neuroscience/10.3389/fnsyn.2010.00029/abstract}
}

@INPROCEEDINGS{zhengming08,
  author = {Zhengming Fu and Eugenio Culurciello and Patrick Lichtsteiner and
	Tobi Delbr\"uck},
  title = {Fall detection using an address-event temporal contrast vision sensor},
  booktitle = {Proceedings of the 2008 IEEE International Symposium on Circuits
	and Systems (ISCAS 2008)},
  year = {2008},
  pages = {424-427},
  publisher = {IEEE}
}

@ARTICLE{fuentes2005,
  author = {Fuentes, Felix and Kar, Dulal C.},
  title = {Ethereal vs. Tcpdump: a comparative study on packet sniffing tools
	for educational purpose},
  journal = {J. Comput. Small Coll.},
  year = {2005},
  volume = {20},
  pages = {169--176},
  number = {4},
  address = {, USA},
  issn = {1937-4771},
  publisher = {Consortium for Computing Sciences in Colleges}
}

@article{fuhrmann2002coding,
  title={Coding of temporal information by activity-dependent synapses},
  author={Fuhrmann, Galit and Segev, Idan and Markram, Henry and Tsodyks, Misha},
  journal={Journal of neurophysiology},
  volume={87},
  number={1},
  pages={140--148},
  year={2002},
  publisher={Am Physiological Soc}
}

@ARTICLE{fukushima88neocognitron,
  author = {Fukushima, K.},
  title = {Neocognitron: {A} Hierarchical Neural Network Capable of Visual Pattern
	Recognition },
  journal = {Neural Networks},
  year = {1988},
  volume = {1},
  pages = {119--130},
  keywords = {convolutional NN}
}

@ARTICLE{fukushima82neocognitron,
  author = {K. Fukushima and S. Miyake},
  title = {Neocognitron: A new algorithm for pattern recognition tolerant of
	deformations and shifts in position},
  journal = {Pattern Recognition},
  year = {1982},
  volume = {15(6)},
  pages = {455-469},
  keywords = {convolutional NN},
  owner = {fieres}
}

@ARTICLE{fukushima83neocognitron,
  author = {Fukushima, K. and Miyake, S. and Ito, T.},
  title = {Neocognitron: {A} neural network model for a mechanism of visual
	pattern recognition},
  journal = {IEEE Transactions on Systems, Man and Cybernetics},
  year = {1983},
  volume = {SMC-13},
  pages = {826--834},
  keywords = {convolutional NN}
}

@ARTICLE{Fuller2011,
  author = {Fuller, S.H. and Millett, L.I.},
  title = {Computing Performance: Game Over or Next Level?},
  journal = {Computer},
  year = {2011},
  volume = {44},
  pages = {31-38},
  number = {1},
  doi = {10.1109/MC.2011.15},
  issn = {0018-9162},
  keywords = {computer architecture;parallel processing;performance evaluation;computing
	architectures;computing performance;parallelism;sequential computing;single
	processor performance;Computational modeling;Hardware;Parallel processing;Program
	processors;Programming;Sequential analysis;High-performance computing;IT
	Roadmap;Moore's law;Parallel computing},
  owner = {simon},
  timestamp = {2013.05.07}
}

@INPROCEEDINGS{fung01proximal,
  author = {Fung, G. and Mangasarian, O. L.},
  title = {Proximal support vector machine classifiers},
  booktitle = {Proceedings of the Seventh {ACM} {SIGKDD} International Conference
	on Knowledge Discovery and Data Mining},
  year = {2001},
  editor = {Provost, F. and Srikant, R.},
  pages = {77--86},
  file = {fung01proximal.pdf:fung01proximal.pdf:PDF}
}

@ARTICLE{furber2020, crossref = {furber2012overview} }
@ARTICLE{furber2012overview,
  author = {Steve B. Furber and David R. Lester and Luis A. Plana and Jim D.
	Garside and Eustace Painkras and Steve Temple and Andrew D. Brown},
  title = {Overview of the {SpiNNaker} System Architecture},
  journal = {IEEE Transactions on Computers},
  year = {2012},
  volume = {99},
  number = {PrePrints},
  address = {Los Alamitos, CA, USA},
  doi = {http://doi.ieeecomputersociety.org/10.1109/TC.2012.142},
  issn = {0018-9340},
  publisher = {IEEE Computer Society}
}

@ARTICLE{fusi02hebbian,
  author = {Stefano Fusi},
  title = {Hebbian spike-driven synaptic plasticity for learning patterns of
	mean firing rates},
  journal = {Biol. Cybern.},
  year = {2002},
  volume = {87},
  pages = {459-470},
  number = {5-6},
  month = {December},
  abstract = {Synaptic plasticity is believed to underlie the formation of appropriate
	patterns of connectivity that stabilize stimulus-selective reverberations
	in the cortex. Here we present a general quantitative framework for
	studying the process of learning and memorizing of patterns of mean
	spike rates. General considerations based on the limitations of material
	(biological or electronic) synaptic devices show that most learning
	networks share the palimpsest property: old stimuli are forgotten
	to make room for the new ones. In order to prevent too-fast forgetting,
	one can introduce a stochastic mechanism for selecting only a small
	fraction of synapses to be changed upon the presentation of a stimulus.
	Such a mechanism can be easily implemented by exploiting the noisy
	fluctuations in the pre- and postsynaptic activities to be encoded.
	The spike-driven synaptic dynamics described here can implement such
	a selection mechanism to achieve slow learning, which is shown to
	maximize the performance of the network as an associative memory.},
  file = {fusi02hebbian.pdf:fusi02hebbian.pdf:PDF},
  keywords = {plasticity learning},
  owner = {mreuss},
  url = {\url{http://www.springerlink.com/link.asp?id=p30xv4ny5u6xlrtf}}
}

@BOOK{futuyma86evolutionary,
  title = {Evolutionary Biology},
  publisher = {Sinauer Associates, Inc.},
  year = {1986},
  author = {Futuyma, D. J.},
  address = {Sunderland, Massachusetts, USA},
  isbn = {3-7643-2200-4}
}

@ARTICLE{guetig2003,
  author = {G\"utig, R. and Aharonov, R. and Rotter, S. and Sompolinsky, Haim},
  title = {Learning Input Correlations through Nonlinear Temporally Asymmetric
	Hebbian Plasticity},
  journal = {The Journal of Neuroscience},
  year = 2003,
  volume = 23,
  pages = {3697--3714},
  number = 9,
  abstract = {Triggered by recent experimental results, temporally asymmetric Hebbian
	(TAH) plasticity is considered as a candidate model for the biological
	implementation of competitive synaptic learning, a key concept for
	the experience-based development of cortical circuitry. However,
	because of the well known positive feedback instability of correlation-based
	plasticity, the stability of the resulting learning process has remained
	a central problem. Plagued by either a runaway of the synaptic efficacies
	or a greatly reduced sensitivity to input correlations, the learning
	performance of current models is limited. Here we introduce a novel
	generalized nonlinear TAH learning rule that allows a balance between
	stability and sensitivity of learning. Using this rule, we study
	the capacity of the system to learn patterns of correlations between
	afferent spike trains. Specifically, we address the question of under
	which conditions learning induces spontaneous symmetry breaking and
	leads to inhomogeneous synaptic distributions that capture the structure
	of the input correlations. To study the efficiency of learning temporal
	relationships between afferent spike trains through TAH plasticity,
	we introduce a novel sensitivity measure that quantifies the amount
	of information about the correlation structure in the input, a learning
	rule capable of storing in the synaptic weights. We demonstrate that
	by adjusting the weight dependence of the synaptic changes in TAH
	plasticity, it is possible to enhance the synaptic representation
	of temporal input correlations while maintaining the system in a
	stable learning regime. Indeed, for a given distribution of inputs,
	the learning efficiency can be optimized.},
  eprint = {http://www.jneurosci.org/content/23/9/3697.full.pdf+html},
  url = {http://www.jneurosci.org/content/23/9/3697.abstract}
}

@ARTICLE{gabriel1980field,
  author = {Rupprecht Gabriel and Werner Leonhard and Craig J. Nordby},
  title = {Field-Oriented Control of a Standard {AC} Motor Using Microprocessors},
  journal = {IEEE Transactions on Industry Applications},
  year = 1980,
  volume = {IA-16},
  number = 2,
  pages = {186--192},
  doi = {10.1109/TIA.1980.4503770}
}

@INCOLLECTION{galluppi2010,
  author = {Galluppi, Francesco and Rast, Alexander and Davies, Sergio and Furber,
	Steve},
  title = {A General-Purpose Model Translation System for a Universal Neural
	Chip},
  booktitle = {Neural Information Processing. Theory and Algorithms},
  publisher = {Springer Berlin / Heidelberg},
  year = {2010},
  editor = {Wong, Kok and Mendis, B. and Bouzerdoum, Abdesselam},
  volume = {6443},
  series = {Lecture Notes in Computer Science},
  pages = {58-65},
  affiliation = {School of Computer Science, University of Manchester, Kilburn Building,
	Oxford Road, Manchester, M13 9PL United Kingdom},
  isbn = {978-3-642-17536-7},
  keyword = {Computer Science},
  url = {http://dx.doi.org/10.1007/978-3-642-17537-4_8}
}

@inproceedings{galluppi2012hierachical,
  title={A hierachical configuration system for a massively parallel neural hardware platform},
  author={Galluppi, Francesco and Davies, Sergio and Rast, Alexander and Sharp, Thomas and Plana, Luis A and Furber, Steve},
  booktitle={Proceedings of the 9th conference on Computing Frontiers},
  pages={183--192},
  year=2012
}

@article{galluppi2015framework,
  abstract = {Many of the precise biological mechanisms of synaptic plasticity remain elusive, but simulations of neural networks have greatly enhanced our understanding of how specific global functions arise from the massively parallel computation of neurons and local Hebbian or spike-timing dependent plasticity rules. For simulating large portions of neural tissue, this has created an increasingly strong need for large scale simulations of plastic neural networks on special purpose hardware platforms, because synaptic transmissions and updates are badly matched to computing style supported by current architectures. Because of the great diversity of biological plasticity phenomena and the corresponding diversity of models, there is a great need for testing various hypotheses about plasticity before committing to one hardware implementation. Here we present a novel framework for investigating different plasticity approaches on the SpiNNaker distributed digital neural simulation platform. The key innovation of the proposed architecture is to exploit the reconfigurability of the ARM processors inside SpiNNaker, dedicating a subset of them exclusively to process synaptic plasticity updates, while the rest perform the usual neural and synaptic simulations. We demonstrate the flexibility of the proposed approach by showing the implementation of a variety of spike- and rate-based learning rules, including standard Spike-Timing dependent plasticity (STDP), voltage-dependent STDP, and the rate-based BCM rule. We analyze their performance and validate them by running classical learning experiments in real time on a 4-chip SpiNNaker board. The result is an efficient, modular, flexible and scalable framework, which provides a valuable tool for the fast and easy exploration of learning models of very different kinds on the parallel and reconfigurable SpiNNaker system.},
  author = {Galluppi, Francesco and Lagorce, Xavier and Stromatias, Evangelos and Pfeiffer, Michael and Plana, Luis A and Furber, Steve B and Benosman, Ryad Benjamin},
  doi = {10.3389/fnins.2014.00429},
  issn = {1662-453X},
  journal = {Frontiers in Neuroscience},
  number = 429,
  title = {A framework for plasticity implementation on the SpiNNaker neural architecture},
  volume = 8,
  year = 2015
}

@article{gatliff2002embedding,
  title={Embedding with gnu: Newlib},
  author={Gatliff, Bill},
  journal={Embedded Systems Programming},
  volume={15},
  number={1},
  pages={12--17},
  year={2002},
  publisher={MILLER FREEMAN INC.}
}


@BOOK{gamme95designpatterns,
  title = {Design patterns: Elements of reusable object-oriented software},
  publisher = {Addisin-Wesley, Reading, Massachusetts},
  year = {1995},
  author = {E. Gamme and R. Helm and R. Johnson and J. Vlissides},
  owner = {fieres}
}

@INPROCEEDINGS{gamblin2015spack,
 author = {Gamblin, Todd and LeGendre, Matthew and Collette, Michael R. and Lee, Gregory L. and Moody, Adam and de Supinski, Bronis R. and Futral, Scott},
 title = {The Spack Package Manager: Bringing Order to HPC Software Chaos},
 booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
 series = {SC '15},
 year = {2015},
 isbn = {978-1-4503-3723-6},
 location = {Austin, Texas},
 pages = {40:1--40:12},
 doi = {10.1145/2807591.2807623},
 acmid = {2807623},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@BOOK{geiger1990vlsi,
  title = {{VLSI} Design Techniques for Analog and Digital Circuits},
  publisher = {McGraw-Hill},
  year = {1990},
  author = {R. L. Geiger and P. E. Allen and N. R. Strader}
}

@ARTICLE{genov03kerneltron,
  author = {Genov, R.},
  title = {Kerneltron: {S}upport vector ``machine'' in silicon},
  journal = {IEEE Transactions on Neural Networks},
  year = {2003},
  volume = {14},
  pages = {1426--1434},
  number = {5}
}

@ARTICLE{georgopoulos1986,
  author = {Georgopoulos, A. P. and Schwartz, A. B. and Kettner, R. E. },
  title = {Neuronal population coding of movement direction.},
  journal = {Science},
  year = {1986},
  volume = {233},
  pages = {1416--1419},
  number = {4771},
  month = {September},
  abstract = {Although individual neurons in the arm area of the primate motor cortex
	are only broadly tuned to a particular direction in three-dimensional
	space, the animal can very precisely control the movement of its
	arm. The direction of movement was found to be uniquely predicted
	by the action of a population of motor cortical neurons. When individual
	cells were represented as vectors that make weighted contributions
	along the axis of their preferred direction (according to changes
	in their activity during the movement under consideration) the resulting
	vector sum of all cell vectors (population vector) was in a direction
	congruent with the direction of movement. This population vector
	can be monitored during various tasks, and similar measures in other
	neuronal populations could be of heuristic value where there is a
	neural representation of variables with vectorial attributes.},
  issn = {0036-8075}
}

@ARTICLE{gerstein64random,
  author = {G Gerstein and B Mandelbrot},
  title = {Random walk models for the spike activity of a single neuron},
  journal = {Biophys J},
  year = {1964},
  volume = {4},
  pages = {41--68},
  month = {Jan},
  keywords = {Electrophysiology, Neurons, Research},
  language = {eng}
}

@ARTICLE{gerstner96coding,
  author = {Wulfram Gerstner},
  title = {A neuronal learning rule for sub-millisecond temporal coding},
  journal = {Nature},
  year = {1996},
  volume = {383},
  pages = {76--78}
}

@article{gerstner1997developmental,
  title={A developmental learning rule for coincidence tuning in the barn owl auditory system},
  author={Gerstner, Wulfram and Kempter, Richard and van Hemmen, J Leo and Wagner, Hermann},
  journal={Computational Neuroscience: trends in research},
  volume={1997},
  pages={665--669},
  year={1997},
  publisher={Citeseer}
}

@ARTICLE{gerstner00coding,
  author = {Wulfram Gerstner},
  title = {Coding properties of spiking neurons: reverse and cross-correlations},
  journal = {Neural Networks},
  year = {2000},
  volume = {14},
  pages = {599--610},
  number = {6-7},
  citeseerurl = {citeseer.nj.nec.com/gerstner01coding.html},
  keywords = {spiking}
}

@ARTICLE{gerstner1996neuronal,
  author = {Gerstner, W. and Kempter, R. and Van Hemmen, J.L. and Wagner, H.
	and others},
  title = {A neuronal learning rule for sub-millisecond temporal coding},
  journal = {Nature},
  year = {1996},
  volume = {383},
  pages = {76--78},
  number = {6595},
  owner = {simon},
  timestamp = {2013.01.16}
}

@BOOK{gerstner02book,
  title = {Spiking Neuron Models: Single Neurons, Populations, Plasticity},
  publisher = {Cambridge University Press},
  year = {2002},
  author = {Gerstner, Wulfram and Kistler, Werner},
  keywords = {spiking plasticity}
}

@ARTICLE{gerstner2009adex,
  author = {Wulrfram Gerstner and Romain Brette},
  title = {Adaptive exponential integrate-and-fire model},
  journal = {Scholarpedia},
  year = {2009},
  volume = {4},
  pages = {8427},
  number = {6},
  doi = {10.4249/scholarpedia.8427},
  timestamp = {2014.08.22},
  url = {http://www.scholarpedia.org/article/Adaptive_exponential_integrate-and-fire_model}
}

@BOOK{gerstner2014dynamics,
  title = {Neuronal Dynamics},
  publisher = {Cambridge University Press},
  year = 2014,
  author = {Gerstner, Wulfram and Kistler, Werner and Naud, Richard and
      Paninski, Liam},
}

@ARTICLE{gewaltig01_synfire,
  author = {Gewaltig, M. O. and Diesmann, M. and Aertsen, A. },
  title = {Propagation of cortical synfire activity: survival probability in
	single trials and stability in the mean.},
  journal = {Neural Netw},
  year = {2001},
  volume = {14},
  pages = {657--673},
  number = {6-7},
  abstract = {The synfire hypothesis states that under appropriate conditions volleys
	of synchronized spikes (pulse packets) can propagate through the
	cortical network by traveling along chains of groups of cortical
	neurons. Here, we present results from network simulations, taking
	full account of the variability in pulse packet realizations. We
	repeatedly stimulated a synfire chain of model neurons and estimated
	activity (a) and temporal jitter (sigma) of the spike response for
	each neuron group in the chain in many trials. The survival probability
	of the activity was assessed for each point in (a, sigma)-space.
	The results confirm and extend our earlier predictions based on single
	neuron properties and a deterministic state-space analysis [Diesmann,
	M., Gewaltig, M.-O., \& Aertsen, A. (1999). Stable propagation of
	synchronous spiking in cortical neural networks. Nature, 402, 529-533].},
  address = {Future Technology Research, Honda R\&D Europe (Deutschland) GmbH,
	Offenbach/Main, Germany. marc-oliver.gewaltig@hre-ftr.f.rd.honda.co.jp},
  citeulike-article-id = {489627},
  issn = {0893-6080},
  keywords = {networks, propagation, synfire},
  posted-at = {2006-02-02 14:37:48},
  priority = {2},
  url = {\url{http://view.ncbi.nlm.nih.gov/pubmed/11665761}}
}

@ARTICLE{Gewaltig:Nest, crossref={gewaltig2007nest}}
@ARTICLE{gewaltig2007nest,
  author = {Marc-Oliver Gewaltig and Markus Diesmann},
  title = {{NEST} ({NE}ural {S}imulation {T}ool)},
  journal = {Scholarpedia},
  year = {2007},
  volume = {2},
  pages = {1430},
  number = {4},
  doi = {10.4249/scholarpedia.1430},
}

@ARTICLE{scholarpedia_articlenest,
  author = {Gewaltig, Marc-Oliver and Diesmann, Markus},
  title = {{NEST} ({NE}ural {S}imulation {T}ool)},
  journal = {Scholarpedia},
  year = {2007},
  volume = {2},
  pages = {1430},
  number = {4},
  howpublished = {\url{http://www.scholarpedia.org/article/Nest}}
}

@MISC{githubgenpybind,
  author = {Johann Kl{\"a}hn},
  title = {{genpybind} software v0.2.1},
  year = {2020},
  publisher = {GitHub},
  journal = {Github repository},
  url = {https://github.com/kljohann/genpybind},
  doi = {10.5281/zenodo.3726274}
}

@MISC{githubelectronicvisions,
  author = {{E}lectronic {V}ision(s) {G}roup},
  title = {Open-Source Software},
  year = 2020,
  publisher = {GitHub},
  journal = {Github repositories},
  url = {https://github.com/electronicvisions/}
}

@MISC{githubomnibus,
  author = {Simon Friedmann},
  title = {Omnibus On-Chip Bus},
  year = 2015,
  publisher = {GitHub},
  journal = {Github repository},
  url = {https://github.com/electronicvisions/omnibus},
  note = {forked from \url{https://github.com/five-elephants/omnibus}}
}

@MISC{githubnux,
  author = {Simon Friedmann},
  title = {The Nux Processor v3.0},
  year = {2015},
  publisher = {GitHub},
  journal = {Github repository},
  url = {https://github.com/electronicvisions/nux},
  doi = {10.5281/zenodo.32146}
}

@MISC{githubppusoftware,
  author = {Simon Friedmann},
  title = {{PPU} software v1.0},
  year = {2015},
  publisher = {GitHub},
  journal = {Github repository},
  url = {https://github.com/electronicvisions/ppu-software},
  doi = {10.5281/zenodo.32147}
}

@MISC{githubhostarq,
  author = {Eric Müller and Moritz Schilling and Christian Mauch},
  title = {{HostARQ} Slow Control Transport Protocol},
  year = 2018,
  month = apr,
  publisher = {GitHub},
  journal = {Github repository},
  url = {https://github.com/electronicvisions/sctrltp},
  keywords = {own_software}
}

@ARTICLE{giulioni2012,
  AUTHOR={Giulioni, Massimiliano  and  Camilleri, Patrick  and  Mattia, Maurizio  and  Dante, Vittorio  and  Braun, Jochen  and  Del Giudice, Paolo},
  TITLE={Robust working memory in an asynchronously spiking neural network realized in neuromorphic VLSI},
  JOURNAL={Frontiers in Neuroscience},
  VOLUME=5,
  YEAR=2012,
  NUMBER=149,
  URL={http://www.frontiersin.org/neuromorphic_engineering/10.3389/fnins.2011.00149/abstract},
  DOI={10.3389/fnins.2011.00149},
  ISSN={1662-453X} ,
  ABSTRACT={We demonstrate bistable attractor dynamics in a spiking neural network implemented with neuromorphic VLSI hardware. The on-chip network consists of three interacting populations (two excitatory, one inhibitory) of leaky integrate-and-fire (LIF) neurons. One excitatory population is distinguished by strong synaptic self-excitation, which sustains meta-stable states of “high” and “low”-firing activity. Depending on the overall excitability, transitions to the “high” state may be evoked by external stimulation, or may occur spontaneously due to random activity fluctuations. In the former case, the “high” state retains a “working memory” of a stimulus until well after its release. In the latter case, “high” states remain stable for seconds, three orders of magnitude longer than the largest time-scale implemented in the circuitry. Evoked and spontaneous transitions form a continuum and may exhibit a wide range of latencies, depending on the strength of external stimulation and of recurrent synaptic excitation. In addition, we investigated “corrupted” “high” states comprising neurons of both excitatory populations. Within a “basin of attraction,” the network dynamics “corrects” such states and re-establishes the prototypical “high” state. We conclude that, with effective theoretical guidance, full-fledged attractor dynamics can be realized with comparatively small populations of neuromorphic hardware neurons.}
}

@article{goeltz2021fast,
  ids = {goeltz2019fast},
  author = {Julian G{\"o}ltz and Laura Kriener and Andreas Baumbach and Sebastian Billaudelle and Oliver Breitwieser and Benjamin Cramer and Dominik Dold and {\'A}kos Ferenc Kungl and Walter Senn and Johannes Schemmel and Karlheinz Meier and Mihai A. Petrovici},
  title = {Fast and energy-efficient neuromorphic deep learning with first-spike times},
  journal={Nature Machine Intelligence},
  volume=3,
  number=9,
  pages={823--835},
  year=2021,
  publisher={Nature Publishing Group},
  doi = {10.1038/s42256-021-00388-x},
}

@MISC{perl_97pdl,
  author = {Karl Glazebrook and Frossie Economou},
  title = {{PDL}: The {P}erl {D}ata {L}anguage},
  howpublished = {Dr. Dobb's Journal},
  month = {sep},
  year = {1997},
  owner = {mueller},
  timestamp = {2008.09.12},
  url = {http://www.ddj.com/184410442}
}

@MISC{opengl_redbook,
  author = {{GLP}rogramming},
  title = {{O}pen{GL} Programming Guide -- The Redbook},
  howpublished = {\url{http://www.glprogramming.com/red/}}
}

@MISC{gnumake_homepage,
  author = {{GNU}},
  title = {The {make} tool},
  howpublished = {\url{http://www.gnu.org/software/make/}}
}

@ARTICLE{goddard2001neuroml,
  author = {Goddard, N. H. and Hucka, M. and Howell, F. and Cornelis, H. and
	Shankar, K. and Beeman, D. },
  title = {Towards NeuroML: model description methods for collaborative modelling
	in neuroscience.},
  journal = {Philos Trans R Soc Lond B Biol Sci},
  year = {2001},
  volume = {356},
  pages = {1209--28},
  number = {1412},
  keywords = {bibtex-import},
  posted-at = {2007-01-04 07:23:27}
}

@INPROCEEDINGS{goldberg_icga87sharing,
  author = {Goldberg, David E.},
  title = {Genetic Algorithms with Sharing for Multimodal Function Approximation},
  booktitle = {Proceedings of the 2nd International Conference on Genetic Algorithms},
  year = {1987},
  editor = {J. J. Grefenstette},
  pages = {41--49},
  address = {Hillsdale, New Jersey},
  publisher = {Lawrence Erlbaum}
}

@INPROCEEDINGS{goldberg_icga89sizing,
  author = {Goldberg, David E.},
  title = {Sizing Populations for Serial and Parallel Genetic Algorithms},
  booktitle = {Proceedings of the Third International Conference on Genetic Algorithms},
  year = {1989},
  editor = {Schaffer, J. D.},
  pages = {70--79},
  publisher = {Morgan Kauffman},
  file = {goldberg_icga89sizing.pdf:goldberg_icga89sizing.pdf:PDF}
}

@BOOK{goldberg02design,
  title = {The Design of Innovation - Lessons from and for Competent Genetic
	Algorithms},
  publisher = {Kluwer Academic Publishers Group},
  year = {2002},
  author = {Goldberg, David E.},
  address = {the Netherlands},
  isbn = {1-4020-7098-5}
}

@BOOK{goldberg89genetic,
  title = {Genetic Algorithms in Search, Optimization and Machine Learning},
  publisher = {Addison Wesley Longman Inc.},
  year = {1989},
  author = {Goldberg, David E.},
  isbn = {0-201-15767-5}
}

@INPROCEEDINGS{goldberg_icga87finite,
  author = {Goldberg, D. E. and Segrest, P.},
  title = {Finite markov chain analysis of genetic algorithms},
  booktitle = {Proceedings of the 2nd International Conference on Genetic Algorithms},
  year = {1987},
  editor = {J. J. Grefenstette},
  pages = {1--8},
  address = {Hillsdale, New Jersey},
  publisher = {Lawrence Erlbaum}
}

@TECHREPORT{goldenholz02,
  author = {D. Goldenholz},
  title = {Liquid computig: A real effect},
  institution = {Boston University Department of Biomedical Engineering},
  year = {2002},
  keywords = {liquid}
}

@BOOK{golomb1981,
  title = {Shift Register Sequences},
  publisher = {Aegean Park Press},
  year = {1981},
  author = {Golomb, Solomon W.},
  address = {Laguna Hills, CA, USA},
  isbn = {0894120484}
}

@article{gotarredona2013stdp,
	abstract = "In this paper we review several ways of realizing asynchronous Spike-Timing Dependent Plasticity (STDP) using memristors as synapses. Our focus is on how to use individual memristors to implement synaptic weight multiplications, in a way such that it is not necessary to (a) introduce global synchronization and (b) to separate memristor learning phases from memristor performing phases. In the approaches described, neurons fire spikes asynchronously when they wish and memristive synapses perform computation and learn at their own pace, as it happens in biological neural systems. We distinguish between two different memristor physics, depending on whether they respond to the original ``moving wall'' or to the ``filament creation and annihilation'' models. Independent of the memristor physics, we discuss two different types of STDP rules that can be implemented with memristors: either the pure timing-based rule that takes into account the arrival time of the spikes from the pre- and the post-synaptic neurons, or a hybrid rule that takes into account only the timing of pre-synaptic spikes and the membrane potential and other state variables of the post-synaptic neuron. We show how to implement these rules in cross-bar architectures that comprise massive arrays of memristors, and we discuss applications for artificial vision.",
	author = "Serrano-Gotarredona, Teresa and Masquelier, Timoth{\'e}e and Prodromakis, Themistoklis and Indiveri, Giacomo and Linares-Barranco, Bernabe",
	doi = "10.3389/fnins.2013.00002",
	issn = "1662-453X",
	journal = "Frontiers in Neuroscience",
	number = "2",
	title = "{STDP and STDP Variations with Memristors for Spiking Neuromorphic Learning Systems}",
	url = "http://www.frontiersin.org/neuroscience/10.3389/fnins.2013.00002/abstract",
	volume = "7",
	year = "2013"
}

@ARTICLE{Gonzalez1996,
  author = {Ricardo Gonzalez and Mark Horowitz},
  title = {Energy Dissipation In General Purpose Microprocessors},
  journal = {IEEE Journal of Solid-State Circuits},
  year = {1996},
  volume = {31},
  pages = {1277-1284},
  number = {9},
  month = {September},
  owner = {simon},
  timestamp = {2013.04.14}
}

@ARTICLE{gonzalez-burgos2000,
  author = {González-Burgos, Guillermo and Barrionuevo, German and Lewis, David
	A.},
  title = {Horizontal Synaptic Connections in Monkey Prefrontal Cortex: An In
	Vitro Electrophysiological Study},
  journal = {Cerebral Cortex},
  year = {2000},
  volume = {10},
  pages = {82-92},
  number = {1},
  doi = {10.1093/cercor/10.1.82},
  eprint = {http://cercor.oxfordjournals.org/content/10/1/82.full.pdf+html},
  url = {http://cercor.oxfordjournals.org/content/10/1/82.abstract}
}

@ARTICLE{goodman2008,
  author = {Goodman, Dan and Brette, Romain},
  title = {Brian: a simulator for spiking neural networks in {Python}},
  journal = {Front. Neuroinform.},
  year = {2008},
  volume = {2},
  number = {5},
  abstract = {"Brian" is a new simulator for spiking neural networks, written in
	Python (http://brian.di.ens.fr). It is an intuitive and highly flexible
	tool for rapidly developing new models, especially networks of single-compartment
	neurons. In addition to using standard types of neuron models, users
	can define models by writing arbitrary differential equations in
	ordinary mathematical notation. Python scientific libraries can also
	be used for defining models and analysing data. Vectorisation techniques
	allow efficient simulations despite the overheads of an interpreted
	language. Brian will be especially valuable for working on non-standard
	neuron models not easily covered by existing software, and as an
	alternative to using Matlab or C for simulations. With its easy and
	intuitive syntax, Brian is also very well suited for teaching computational
	neuroscience.},
  keywords = {Python, spiking neurons, simulation, integrate and fire, teaching,
	neural networks, computational neuroscience, software}
}

@MASTERSTHESIS{gorel2013bachelorthesis,
  author   = {Alexander Gorel},
  title    = {Integration einer automatisierten analogen und erweiterbaren
Testumgebung zur Validierung und \"Uberwachung von Hardware und Software
Frameworks},
  year     = 2013,
  type     = {Bachelor thesis},
  school   = {Ruprecht-Karls-Universit{\"a}t Heidelberg}
}

@INPROCEEDINGS{gorges_icga89parallel,
  author = {Gorges-Schleuter, M.},
  title = {{ASPARAGOS} An Asynchronous Parallel Genetic Optimization Strategy},
  booktitle = {Proceedings of the Third International Conference on Genetic Algorithms},
  year = {1989},
  editor = {Schaffer, J. D.},
  pages = {422--427},
  address = {San Francisco},
  publisher = {Morgan Kaufmann}
}

@book{gough2005introduction,
  title={An Introduction to GCC: For the GNU Compilers Gcc and G++},
  author={Gough, Brian and Stallman, Richard M.},
  isbn={9780954161798},
  series={Network theory manual},
  url={https://books.google.de/books?id=yIGKQAAACAAJ},
  year={2005},
  publisher={Network Theory}
}

@MISC{gpl2_homepage,
  author = {{GPL 2009}},
  title = {{GNU} {G}eneral {P}ublic {L}icense 2.0},
  howpublished = {\url{http://www.gnu.org/licenses/gpl-2.0.html}}
}

@misc{gplv21,
  title        = {{GNU} General Public License},
  version      = {2.1},
  organization = {Free Software Foundation},
  url          = {http://www.gnu.org/licenses/gpl.html},
  pagination   = {section},
  language     = {english},
}

@MISC{gruebl13oral,
  author = {Andreas Gr\"ubl},
  title = {Personal Communication},
  month = {August},
  year = {2012},
  owner = {simon},
  timestamp = {2013.04.18}
}

@ARTICLE{gruebill2020bss2methods, crossref = {gruebl2020verification} }
@article{gruebl2020verification,
  title={Verification and Design Methods for the {BrainScaleS} Neuromorphic Hardware System},
  author={Gr{\"u}bl, Andreas and Billaudelle, Sebastian and Cramer, Benjamin and Karasenko, Vitali and Schemmel, Johannes},
  journal={Journal of Signal Processing Systems},
  volume=92,
  number=11,
  pages={1277--1292},
  year=2020,
  publisher={Springer},
  doi = {10.1007/s11265-020-01558-7},
}

@INPROCEEDINGS{graf1992NNchip,
  author = {H.P. Graf and R. Janow and D. Henderson and R. Lee},
  title = {Reconfigurable neural net chip with 32K connections},
  booktitle = {Proceedings Int. Conference of Information Processing Systems (NIPS1992)},
  year = {1992},
  pages = {1032--1038},
  file = {graf1992NNchip.pdf:graf1992NNchip.pdf:PDF},
  keywords = {vlsi, convolutional NN},
  owner = {fieres}
}

@INPROCEEDINGS{richard06openmpi,
  author = {Richard L. Graham and Galen M. Shipman and Brian W. Barrett and Ralph
	H. Castain and George Bosilca and Andrew Lumsdaine},
  title = {Open {MPI}: A High-Performance, Heterogeneous {MPI}},
  booktitle = {Proceedings, Fifth International Workshop on Algorithms, Models and
	Tools for Parallel Computing on Heterogeneous Networks},
  year = {2006},
  address = {Barcelona, Spain},
  month = {September}
}

@INPROCEEDINGS{granitto01stepwise,
  author = {Granitto, P. M. and Navone, H.D. and Ceccatto, H. A.},
  title = {A stepwise algorithm for construction of neural network ensembles},
  booktitle = {{VII} Argentine Congress of Computer Science},
  year = {2001},
  address = {Calafate, Argentina},
  file = {granitto01stepwise.pdf:granitto01stepwise.pdf:PDF}
}

@ARTICLE{kacher2020graphcore,
  title = {Graphcore c2 card performance for image-based deep learning application: A report},
  author = {Kacher, Ilyes and Portaz, Maxime and Randrianarivo, Hicham and Peyronnet, Sylvain},
  year = 2020,
  month = feb,
  journal = {arXiv preprint},
  archivePrefix = {arXiv},
  eprint = {2002.11670},
  primaryClass = {cs.CV}
}

@BOOK{gray1918,
  title = {Anatomy of the Human Body},
  publisher = {Public domain},
  year = {1918},
  author = {Henry Gray},
  volume = {20th US Edition}
}

@INPROCEEDINGS{gray01,
  author = {Paul R. Gray and Paul J. Hurst and Stephen H. Lewis and Robert G.
	Meyer},
  title = {Analysis and design of analog integrated circuits, Fourth Edition},
  year = {2001},
  publisher = {John Wiley \& Sons},
  isbn = {0-471-32168-0}
}

@ARTICLE{greenberg2008population,
  author = {David S. Greenberg and Arthur R. Houweling and Jason N D Kerr},
  title = {Population imaging of ongoing neuronal activity in the visual cortex of awake rats},
  journal = {Nature Neuroscience},
  year = {2008},
  volume = {11},
  pages = {749--751},
  doi = {10.1038/nn.2140}
}

@ARTICLE{greenwood97recurrent,
  author = {Greenwood, G. W.},
  title = {Training partially recurrent neural networks using evolutionary strategies},
  journal = {IEEE Transactions on Speech and Audio Processing},
  year = {1997},
  volume = {5},
  pages = {192--194},
  number = {2}
}

@INPROCEEDINGS{grefenstette92deception,
  author = {Grefenstette, J. J.},
  title = {Deception considered Harmful},
  booktitle = {Foundations of Genetic Algorithms},
  year = {1993},
  editor = {Whitley, L. D.},
  volume = {2},
  pages = {75--91}
}

@ARTICLE{grefenstette86parameters,
  author = {Grefenstette, J. J.},
  title = {Optimization of Control Parameters for Genetic Algorithms},
  journal = {IEEE Transactions on Systems, Man, and Cybernetics},
  year = {1986},
  volume = {SMC-16},
  pages = {122--128},
  number = {1}
}

@INPROCEEDINGS{grefenstette_icga89parallelism,
  author = {Grefenstette, J. J. and Baker, J. E.},
  title = {How genetic algorithms work: {A} critical look at implicit parallelism},
  booktitle = {Proceedings of the Third International Conference on Genetic Algorithms},
  year = {1989},
  editor = {Schaffer, J. D.},
  pages = {20--27},
  publisher = {Morgan Kaufmann}
}

@ARTICLE{gross2011,
  author = {Gross, G. W.},
  title = {Multielectrode arrays},
  journal = {Scholarpedia},
  year = {2011},
  volume = { 6},
  pages = {5749},
  number = {2}
}

@MANUAL{pci_standard,
  title = {PCI Local Bus Specification,? Revision 2.1},
  author = {PCI Special Interest Group},
  month = {Dec},
  year = {1995},
  groupsearch = {0},
  key = {pci 2.1},
  keywords = {spec}
}

@ARTICLE{gross2011multielectrode,
	AUTHOR = {Gross, G. W.},
	TITLE   = {Multielectrode arrays},
	YEAR= {2011},
	JOURNAL= {Scholarpedia},
	VOLUME  ={		 6},
	NUMBER  = {2},
	PAGES   = {5749}
}

@ARTICLE{gross1995aristotle,
	AUTHOR = {Charles G. Gross},
	TITLE = {Aristotle on the Brain},
	YEAR = {1995},
	JOURNAL= {The Neuroscientist},
	VOLUME = {1},
	NUMBER = {4},
	PAGES = {245-250}
}

@BOOK{grzywacz90velocity,
  title = {A model for the estimate of local velocity},
  publisher = {Springer Berlin / Heidelberg},
  year = {1990},
  author = {Norberto M. Grzywacz and Alan L. Yuille},
  volume = {427/1990},
  pages = {331--335},
  booktitle = {Computer Vision — ECCV 90}
}

@MISC{gruebl12oral,
  author = {Gr{\"u}bl, Andreas},
  title = {personal communication},
  year = {2012},
  address = {Kirchhoff Institut for Physics, University of Heidelberg, Germany}
}

@MISC{gruebl08oral,
  author = {Gr{\"u}bl, Andreas},
  title = {personal communication},
  year = {2008},
  address = {Kirchhoff Institut for Physics, University of Heidelberg, Germany}
}

@PHDTHESIS{gruebl_07thesis,
  author = {Andreas Gr{\"u}bl},
  title = {{VLSI} Implementation of a Spiking Neural Network},
  school = {Ruprecht-Karls-University, Heidelberg},
  year = {2007},
  note = {Document No. HD-KIP 07-10},
  abstract = {Within the scope of this thesis concepts and dedicated hardware have
	been developed that allow for building large scale hardware spiking
	neural networks. The work is based upon an analog VLSI model of a
	spiking neural network featuring an implementation of spike timing
	dependent plasticity (STDP) locally in each synapse. Analog network
	operation is carried out up to 105 times faster than real time and
	spikes are communicated as digital events. This work focuses on the
	digital hardware and the event transport. Along with digital logic
	for event processing and configuration purposes, the analog VLSI
	model has been integrated into a mixed-signal ASIC by means of an
	automated design flow. Furthermore, the accompanying controller has
	been realized in programmable logic, and a hardware platform capable
	of hosting multiple chips is presented. To extend the operation of
	the VLSI model to multiple chips, an event routing algorithm has
	been developed that enables the communication between neurons and
	synapses located on different chips, thereby providing correct temporal
	processing of events which is a basic requirement for investigating
	temporal plasticity. The functional performance of the event routing
	algorithm is shown in simulations. Furthermore, the functionality
	of the mixed- signal ASIC along with the hardware system and the
	feasibility of biologically realistic experiments is demonstrated
	. Due to its inherent fast and parallel operation the presented large
	scale physical model of a spiking neural network will serve as an
	experimentation tool for neuroscientists to complement numerical
	simulations of plasticity mechanisms within the visual cortex while
	facilitating intuitive and extensive parameter searches.},
  howpublished = {Ph.D. thesis, University of Heidelberg, HD-KIP-07-10},
  url = {http://www.kip.uni-heidelberg.de/Veroeffentlichungen/details.php?id=1788}
}

@MASTERSTHESIS{gruebl_03diplomathesis,
  author = {Gr{\"u}bl, Andreas},
  title = {Eine {FPGA}-basierte Platform f{\"u}r neuronala Netze},
  school = {University of Heidelberg},
  year = {2003},
  howpublished = {Diploma thesis (German), University of Heidelberg, HD-KIP-03-2},
  keywords = {vision}
}

@MISC{facets_d7-7,
  author = {Andreas Gr{\"u}bl and Sebastian Millner and Johannes Schemmel},
  title = {Design the final {ASIC} for the wafer-scale system},
  howpublished = {{FACETS} Deliverable D7-7},
  year = {2010},
  note = {University Heidelberg},
  keywords = {facets, deliverable},
  owner = {agruebl},
  timestamp = {2010.08.27}
}

@MISC{facets_d7-10,
  author = {Andreas Gr{\"u}bl and Johannes Schemmel},
  title = {Produce the network wafer and the associated post-processing mask},
  howpublished = {{FACETS} Deliverable D7-10},
  year = {2010},
  note = {University Heidelberg},
  keywords = {facets, deliverable},
  owner = {agruebl},
  timestamp = {2010.09.12}
}

@MISC{facets_d7-11,
  author = {Andreas Gr{\"u}bl and Johannes Schemmel},
  title = {Post-Process the Network Wafer},
  howpublished = {{FACETS} Deliverable D7-11},
  year = {2010},
  note = {University Heidelberg},
  keywords = {facets, deliverable},
  owner = {agruebl},
  timestamp = {2010.09.12}
}

@misc{gerrit,
title = {Gerrit Code Review},
year = 2020,
note = {accessed March 11, 2020},
howpublished = "\url{https://www.gerritcodereview.com/}"}

@ARTICLE{guerrero06programmable,
  author = {Guerrero-Rivera, Ruben and Morrison, Abigail and Diesmann, Markus
	and Pearce, Tim C.},
  title = {{Programmable Logic Construction Kits for Hyper-Real-Time Neuronal
	Modeling}},
  journal = {Neural Comp.},
  year = {2006},
  volume = {18},
  pages = {2651-2679},
  number = {11},
  abstract = {Programmable logic designs are presented that achieve exact integration
	of leaky integrate-and-fire soma and dynamical synapse neuronal models
	and incorporate spike-time dependent plasticity and axonal delays.
	Highly accurate numerical performance has been achieved by modifying
	simpler forward-Euler-based circuitry requiring minimal circuit allocation,
	which, as we show, behaves equivalently to exact integration. These
	designs have been implemented and simulated at the behavioral and
	physical device levels, demonstrating close agreement with both numerical
	and analytical results. By exploiting finely grained parallelism
	and single clock cycle numerical iteration, these designs achieve
	simulation speeds at least five orders of magnitude faster than the
	nervous system, termed here hyper-real-time operation, when deployed
	on commercially available field-programmable gate array (FPGA) devices.
	Taken together, our designs form a programmable logic construction
	kit of commonly used neuronal model elements that supports the building
	of large and complex architectures of spiking neuron networks for
	real-time neuromorphic implementation, neurophysiological interfacing,
	or efficient parameter space investigations.},
  eprint = {\url{http://neco.mitpress.org/cgi/reprint/18/11/2651.pdf}},
  keywords = {plasticity},
  url = {\url{http://neco.mitpress.org/cgi/content/abstract/18/11/2651}}
}

@article{gulledge2005synaptic,
  title={Synaptic integration in dendritic trees},
  author={Gulledge, Allan T and Kampa, Bj{\"o}rn M and Stuart, Greg J},
  journal={Journal of neurobiology},
  volume={64},
  number={1},
  pages={75--90},
  year={2005},
  publisher={Wiley Online Library}
}

@ARTICLE{gupta00organizing,
  author = {A. Gupta and Y. Wang and H. Markram},
  title = {Organizing Principles for a Diversity of Gabaergic Interneurons And
	Synapses in the Neocortex},
  journal = {Science},
  year = {2000},
  volume = {287},
  pages = {273}
}

@ARTICLE{guetig2006,
  author = {G{\"u}tig, Robert and Sompolinsky, Haim},
  title = {The tempotron: a neuron that learns spike timing-based decisions},
  journal = {Nat Neurosci},
  year = {2006},
  volume = {9},
  pages = {420--428},
  number = {3},
  month = mar,
  comment = {10.1038/nn1643},
  issn = {1097-6256},
  owner = {sjeltsch},
  publisher = {Nature Publishing Group},
  timestamp = {2010.08.04},
  url = {http://dx.doi.org/10.1038/nn1643}
}

@MISC{gutmann07diplomathesis,
  author = {Christian Gutmann},
  title = {{Implementation einer Gigabit-Ethernet-Schnittstelle zum Betrieb
	eines Künstlichen Neuronalen Netzwerkes}},
  howpublished = {Diploma thesis (German), University of Heidelberg, HD-KIP-07-08},
  year = {2007},
  keywords = {vision, spikey, stage1, software, MAC IP-Core, FPGA}
}

@ARTICLE{guyonneau05neurons,
  author = {Guyonneau, Rudy and VanRullen, Rufin and Thorpe, Simon J.},
  title = {Neurons Tune to the Earliest Spikes Through STDP},
  journal = {Neural Computation},
  year = {2005},
  volume = {17},
  pages = {859-879},
  number = {4},
  month = {April},
  abstract = {Spike timing-dependent plasticity (STDP) is a learning rule that modifies
	the strength of a neuron's synapses as a function of the precise
	temporal relations between input and output spikes. In many brains
	areas, temporal aspects of spike trains have been found to be highly
	reproducible. How will STDP affect a neuron's behavior when it is
	repeatedly presented with the same input spike pattern? We show in
	this theoretical study that repeated inputs systematically lead to
	a shaping of the neuron's selectivity, emphasizing its very first
	input spikes, while steadily decreasing the postsynaptic response
	latency. This was obtained under various conditions of background
	noise, and even under conditions where spiking latencies and firing
	rates, or synchrony, provided conflicting informations. The key role
	of first spikes demonstrated here provides further support for models
	using a single wave of spikes to implement rapid neural processing.},
  file = {guyonneau05neurons.pdf:guyonneau05neurons.pdf:PDF},
  keywords = {plasticity},
  owner = {mreuss}
}

@MISC{guettler10diplomathesis,
  author = {Maurice G{\"u}ttler},
  title = {Konzeptoptimierung und {E}ntwicklung einer hochintegrierten {L}eiterplatte},
  howpublished = {Diploma thesis (German), University of Heidelberg, HD-KIP-10-68},
  year = {2010},
  keywords = {PCB, brainscales}
}

@ARTICLE{habenschuss2012homeostasis,
  author = {Habenschuss, Stefan and Bill, Johannes and Nessler, Bernhard},
  title = {Homeostatic plasticity in Bayesian spiking networks as Expectation Maximization with posterior constraints},
  journal = {Advances in Neural Information Processing Systems},
  year = {2012},
  volume = {25},
}

@ARTICLE{habenschuss2013stdp,
  author = {Habenschuss, Stefan and Puhr, Helmut and Maass, Wolfgang},
  title = {Emergence of Optimal Decoding of Population Codes Through STDP},
  journal = {Neural Computation},
  year = 2013,
  volume = 25,
  pages = {1371-1407},
  number = 6,
  doi = {10.1162/NECO_a_00446},
}

@ARTICLE{habenschuss2013stochastic,
  author = {Stefan Habenschuss and Jonke Zeno and Wolfgang Maass},
  year = 2013,
  title = {Stochastic Computations in Cortical Microcircuit Models},
  journal = {PLoS Comput Biol},
  number = 9,
  volume = 11,
  pages = {e1003311},
  doi = {10.1371/journal.pcbi.1003311}
}

@ARTICLE{hafliger2007adaptive,
  author = {H\"afliger, P.},
  title = {Adaptive {WTA} with an analog {VLSI} neuromorphic learning chip},
  journal = {IEEE Transactions on Neural Networks},
  year = {2007},
  volume = {18},
  pages = {551-72},
  number = {2},
  keywords = {neuromorphic}
}

@ARTICLE{hananel2018BindsNet,
  author = {Hazan, Hananel and Saunders, Daniel J. and Khan, Hassaan and Patel, Devdhar and Sanghavi, Darpan T. and Siegelmann, Hava T. and Kozma, Robert},
  title = {{BindsNET}: A Machine Learning-Oriented Spiking Neural Networks Library in Python},
  journal = {Frontiers in Neuroinformatics},
  volume = 12,
  pages = 89,
  year = 2018,
  url = {https://www.frontiersin.org/article/10.3389/fninf.2018.00089},
  doi = {10.3389/fninf.2018.00089},
  issn = {1662--5196}
}

@article{Harris2012,
title = "Synaptic Energy Use and Supply ",
journal = "Neuron ",
volume = "75",
number = "5",
pages = "762 - 777",
year = "2012",
note = "",
issn = "0896-6273",
doi = "http://dx.doi.org/10.1016/j.neuron.2012.08.019",
url = "http://www.sciencedirect.com/science/article/pii/S0896627312007568",
author = "Julia J. Harris and Renaud Jolivet and David Attwell"
}

@ARTICLE{hasler2013,
 AUTHOR={Hasler, Jennifer  and  Marr, Harry Bo},
 TITLE={Finding a Roadmap to achieve Large Neuromorphic Hardware Systems},
 JOURNAL={Frontiers in Neuroscience},
 VOLUME={7},
 YEAR={2013},
 NUMBER={118},
 URL={http://www.frontiersin.org/neuromorphic_engineering/10.3389/fnins.2013.00118/abstract},
 DOI={10.3389/fnins.2013.00118},
 ISSN={1662-453X},
 ABSTRACT={Neuromorphic systems are gaining increasing importance in an era where CMOS digital computing techniques are meeting hard physical limits. These silicon systems mimic extremely energy efficient neural computing structures, potentially both for solving engineering applications as well as understanding neural computation.  Towards this end, the authors provide a glimpse at what the technology evolution roadmap looks like for these systems so that Neuromorphic engineers may gain the same benefit of anticipation and foresight that IC designers gained from Moore's law many years ago.   Scaling of energy efficiency, performance, and size will be discussed as well as how the implementation and application space of Neuromorphic systems are expected to evolve over time.  }
}

@manual{hbp2014_sp9spec,
	author = {{HBP SP9 partners}},
	title = {Neuromorphic Platform Specification},
	abstract = { This document provides the technical specifications for
	             the Neuromorphic Computing Platform of the Human Brain
		     Project. For each of the two complementary large-scale
		     hardware implementations detailed technical descriptions
		     of the architecture, the user view and the electronic com-
		     ponents are given. In addition the support software re-
		     quired for the execution of experiments on the platform is
		     described and benchmark tasks for neuromorphic comput-
		     ing are proposed. The document closes with a list of key
		     performance indicators and a time planning for the plat-
		     form construction.},
	year = 2014,
	month = mar,
	day = 31,
	organization = {Human Brain Project}
}

@ARTICLE{ho2000,
  author = {H\^{o}, N. and Destexhe, A. },
  title = {Synaptic background activity enhances the responsiveness of neocortical
	pyramidal neurons.},
  journal = {J Neurophysiol},
  year = {2000},
  volume = {84},
  pages = {1488--1496},
  number = {3},
  month = {Sep},
  abstract = {Neocortical pyramidal neurons in vivo are subject to an intense synaptic
	background activity but little is known of how this activity affects
	cellular responsiveness and what function it may serve. These issues
	were examined in morphologically reconstructed neocortical pyramidal
	neurons in which synaptic background activity was simulated based
	on recent measurements in cat parietal cortex. We show that background
	activity can be decomposed into two components: a tonically active
	conductance and voltage fluctuations. Previous studies have mostly
	focused on the conductance effect, revealing that background activity
	is responsible for a decrease in responsiveness, which imposes severe
	conditions of coincidence of inputs necessary to discharge the cell.
	It is shown here, in contrast, that responsiveness is enhanced if
	voltage fluctuations are taken into account; in this case the model
	can produce responses to inputs that would normally be subthreshold.
	This effect is analyzed by dissecting and comparing the different
	components of background activity, as well as by evaluating the contribution
	of parameters such as the dendritic morphology, the distribution
	of leak currents, the value of axial resistivity, the densities of
	voltage-dependent currents, and the release parameters underlying
	background activity. Interestingly, the model's optimal responsiveness
	was obtained when voltage fluctuations were of the same order as
	those measured intracellularly in vivo. Possible consequences were
	also investigated at the population level, where the presence of
	background activity allowed networks of pyramidal neurons to instantaneously
	detect inputs that are small compared with the classical detection
	threshold. These results suggest, at the single-cell level, that
	the presence of voltage fluctuations has a determining influence
	on cellular responsiveness and that these should be taken into account
	in models of background activity. At the network level, we predict
	that background activity provides the necessary drive for detecting
	events that would normally be undetectable. Experiments are suggested
	to explore this possible functional role for background activity.},
  citeulike-article-id = {3184734},
  keywords = {action-potentials, alpha-amino-3-hydroxy-5-methyl-4-isoxazolepropionic-acid,
	animals, cats, computer-simulation, file-import-08-09-03, models,
	neocortex, neural-networks-computer, neurological, parietal-lobe,
	predicti, pyramidal-cells, reproducibility-of-results, sensory-thresholds,
	synapses, synaptic-transmission, ve-value-of-tests},
  posted-at = {2008-09-03 10:42:00},
  priority = {0}
}

@ARTICLE{Hafliger2007,
  author = {Hafliger, P.},
  title = {Adaptive WTA With an Analog VLSI Neuromorphic Learning Chip},
  journal = {Neural Networks, IEEE Transactions on},
  year = {2007},
  volume = {18},
  pages = {551-572},
  number = {2},
  doi = {10.1109/TNN.2006.884676},
  issn = {1045-9227},
  keywords = {Hebbian learning;VLSI;convolution;image processing;neural chips;Hebbian
	learning;analog VLSI neuromorphic learning chip;convolution AER vision
	architecture for real time;spike based learning rule;spike signal
	image processing;very large scale integration chip;Computational
	modeling;Computer architecture;Convolution;Hebbian theory;Image processing;Neuromorphics;Neurons;Signal
	processing;Statistics;Very large scale integration;Classification;competitive
	Hebbian learning;neuromorphic electronics;winner-take-all (WTA);Analog-Digital
	Conversion;Artificial Intelligence;Biomimetic Materials;Computers,
	Analog;Equipment Design;Equipment Failure Analysis;Game Theory;Neural
	Networks (Computer);Pattern Recognition, Automated;Semiconductors;Signal
	Processing, Computer-Assisted},
  owner = {simon},
  timestamp = {2013.05.03}
}

@ARTICLE{haider06neocortical,
  author = {Bilal Haider and Alvaro Duque and Andrea R Hasenstaub and David A
	McCormick},
  title = {Neocortical network activity in vivo is generated through a dynamic
	balance of excitation and inhibition},
  journal = {J Neurosci},
  year = {2006},
  volume = {26},
  pages = {4535--45},
  number = {17},
  month = {Apr},
  abstract = {The recurrent excitatory and inhibitory connections between and within
	layers of the cerebral cortex are fundamental to the operation of
	local cortical circuits. Models of cortical function often assume
	that recurrent excitation and inhibition are balanced, and we recently
	demonstrated that spontaneous network activity in vitro contains
	a precise balance of excitation and inhibition; however, the existence
	of a balance between excitation and inhibition in the intact and
	spontaneously active cerebral cortex has not been directly tested.
	We examined this hypothesis in the prefrontal cortex in vivo, during
	the slow (<1 Hz) oscillation in ketamine-xylazine-anesthetized ferrets.
	We measured persistent network activity (Up states) with extracellular
	multiple unit and local field potential recording, while simultaneously
	recording synaptic currents in nearby cells. We determined the reversal
	potential and conductance change over time during Up states and found
	that the body of Up state activity exhibited a steady reversal potential
	(-37 mV on average) for hundreds of milliseconds, even during substantial
	(21 nS on average) changes in membrane conductance. Furthermore,
	we found that both the initial and final segments of the Up state
	were characterized by significantly more depolarized reversal potentials
	and concomitant increases in excitatory conductance, compared with
	the stable middle portions of Up states. This ongoing temporal evolution
	between excitation and inhibition, which exhibits remarkable proportionality
	within and across neurons in active local networks, may allow for
	rapid transitions between relatively stable network states, permitting
	the modulation of neuronal responsiveness in a behaviorally relevant
	manner.},
  affiliation = {Department of Neurobiology, Kavli Institute for Neuroscience, Yale
	University School of Medicine, New Haven, Connecticut 06510, USA.}
}

@article{halldorsson1997greed,
  title={Greed is good: Approximating independent sets in sparse and bounded-degree graphs},
  author={Halld{\'o}rsson, Magn{\'u}s M and Radhakrishnan, Jaikumar},
  journal={Algorithmica},
  volume={18},
  number={1},
  pages={145--163},
  year={1997},
  publisher={Springer}
}

@ARTICLE{han98cmos,
  author = {Han, G. and S{\'a}nchez, E.},
  title = {{CMOS} transconductance multipliers: {A} tutorial},
  journal = {IEEE Transactions on Circuits and Systems {II}; Analog and Digital
	Signal Processing},
  year = {1998},
  volume = {45},
  pages = {1550--1563},
  number = {12}
}

@INPROCEEDINGS{hancock_ppsn92recombination,
  author = {Hancock, P. J. B.},
  title = {Recombination operators for the design of neural nets by genetic
	algorithm},
  booktitle = {Proceedings of the Conference on Parallel Problem Solving from Nature},
  year = {1992},
  editor = {M{\"a}nner, R. and Manderick, B.},
  volume = {2},
  pages = {441--450},
  publisher = {Elsevier Science Publishers B.V.}
}

@INPROCEEDINGS{hancock_cmss88representations,
  author = {Hancock, P. J. B.},
  title = {Data representations in neural nets: an empirical study},
  booktitle = {Proceedings of the 1988 Connectionist Models Summer School},
  year = {1988},
  editor = {D.S. Touretzky and G. Hinton and T. Sejnowski},
  pages = {11--20},
  publisher = {Morgan Kaufmann Publishers Inc.},
  groupsearch = {0}
}

@INPROCEEDINGS{hancock_cogann92genetic,
  author = {Hancock, P. J. B.},
  title = {Genetic algorithms and permutation problems: {A} comparison of recombination
	operators for neural net structure specification},
  booktitle = {Proceedings of the IEEE Workshop on Combinations of Genetic Algorithms
	and Neural Network},
  year = {1992},
  editor = {Whitley, D.},
  pages = {108--121},
  publisher = {IEEE Press}
}

@ARTICLE{hansel95,
  author = {D. Hansel and G. Mato and C. Meunier},
  title = {Synchrony in excitatory neural networks},
  journal = {Neural Comput.},
  year = {1995},
  volume = {7},
  pages = {307--337},
  number = {2},
  address = {Cambridge, MA, USA},
  doi = {http://dx.doi.org/10.1162/neco.1995.7.2.307},
  issn = {0899-7667},
  publisher = {MIT Press}
}

@MISC{harik97linkage,
  author = {Harik, G.R.},
  title = {Learning Gene Linkage to Efficiently Solve Problems of Bounded Difficulty
	Using Genetic Algorithms},
  howpublished = {{\it PhD thesis}},
  year = {1997},
  address = {Ann Arbor, MI, USA},
  file = {harik97inkage.pdf:harik97inkage.pdf:PDF},
  institution = {University of Michigan}
}

@INPROCEEDINGS{harp89genetic,
  author = {Harp, S. A. and Samad, T. and Guha, A.},
  title = {Towards the Genetic Synthesis of Neural Networks},
  booktitle = {Proceedings of the Third International Conference on Genetic Algorithms},
  year = {1989},
  editor = {Schaffer, J. D.},
  pages = {360--369},
  publisher = {Morgan Kaufmann}
}

@MISC{tc65nm_backend_team,
  author = {Andreas Hartel and Gvidas Sidlauskas and Andreas Gr\"ubl},
  title = {tc65nm back-end},
  year = {2011},
  note = {personal communication},
  owner = {simon},
  timestamp = {2013.05.02}
}

@phdthesis{hartel2016phd,
  author = "Hartel, Andreas",
  school = {Ruprecht-Karls-Universit{\"a}t Heidelberg},
  title = "Implementation and Characterization of Mixed-Signal Neuromorphic {ASIC}s",
  year = 2016
}

@INPROCEEDINGS{hartmann2010,
  author = {Hartmann, S. and Schiefer, S. and Scholze, S. and Partzsch, J. and
	Mayr, C. and Henker, S. and Schuffny, R.},
  title = {Highly integrated packet-based AER communication infrastructure with
	3Gevent/S throughput},
  booktitle = {Electronics, Circuits, and Systems (ICECS), 2010 17th IEEE International
	Conference on},
  year = {2010},
  pages = {950--953},
  month = {dec.},
  doi = {10.1109/ICECS.2010.5724670},
  keywords = {3Gevent-S throughput;AER protocol serialization;FPGA design;PCB design;address
	event representations;communication infrastructure design;event storage;event
	transmission speed;highly-integrated packet-based AER communication
	infrastructure;large-scale neuromorphic VLSI systems;mechanical integration;neural
	communication;off-the-shelf chip connect protocols;packet-based synchronous
	protocol;parallel asynchronous transmission;serial event communication;waferscale
	neuromorphic system;VLSI;field programmable gate arrays;logic design;neural
	nets;printed circuit design;protocols;}
}

@INPROCEEDINGS{hasler07,
  author = {Hasler, P. and Kozoil, S. and Farquhar, E. and Basu, A.},
  title = {Transistor Channel Dendrites implementing HMM classifiers},
  booktitle = {Circuits and Systems, 2007. ISCAS 2007. IEEE International Symposium
	on},
  year = {2007},
  pages = {3359 -3362},
  month = {may},
  abstract = {Recently the authors presented transistor channel models of biological
	channels and the resulting implementation towards building spiking
	nodes, synapses, and dendrites. The authors also discussed how to
	build reconfigurable dendrites using programmable analog techniques.
	With all of this technology components available, the authors begin
	to address the question of the computation model possible using a
	dendrite element, as well as a network of dendrite elements. The
	authors discuss the connection between a dendrite element and a hidden
	Markov model (HMM) classifier branch, as well as a network of dendrites
	and somas to create an HMM classifier typical of what is used in
	speech recognition systems. The authors present simulation and experimental
	results for the branch elements; the authors also present initial
	results for a small dendrite based classifier structure to show the
	similarities to the HMM paradigm.},
  doi = {10.1109/ISCAS.2007.378287},
  keywords = {HMM classifiers;biological channels;hidden Markov model;programmable
	analog techniques;reconfigurable dendrites;speech recognition systems;spiking
	nodes;transistor channel models;MOSFET;analogue integrated circuits;dendrites;field
	programmable analogue arrays;hidden Markov models;semiconductor device
	models;speech recognition;}
}

@BOOK{hastings2001analog,
  title = {The Art of Analog Layout},
  publisher = {Prentice-Hall Inc.},
  year = {2001},
  author = {Alan Hastings},
  address = {Upper Saddle River, New Jersey, USA},
  isbn = {0-13-087061-1}
}

@ARTICLE{hawkins2016,
  author={Hawkins, Jeff and Ahmad, Subutai},
  title={Why Neurons Have Thousands of Synapses, a Theory of Sequence Memory in Neocortex},
  journal={Frontiers in Neural Circuits},
  volume={10},
  pages={23},
  year={2016},
  url={http://journal.frontiersin.org/article/10.3389/fncir.2016.00023},
  doi={10.3389/fncir.2016.00023},
  issn={1662-5110},
  abstract={Pyramidal neurons represent the majority of excitatory neurons in the neocortex. Each pyramidal neuron receives input from thousands of excitatory synapses that are segregated onto dendritic branches. The dendrites themselves are segregated into apical, basal, and proximal integration zones, which have different properties. It is a mystery how pyramidal neurons integrate the input from thousands of synapses, what role the different dendrites play in this integration, and what kind of network behavior this enables in cortical tissue. It has been previously proposed that non-linear properties of dendrites enable cortical neurons to recognize multiple independent patterns. In this paper we extend this idea in multiple ways. First we show that a neuron with several thousand synapses segregated on active dendrites can recognize hundreds of independent patterns of cellular activity even in the presence of large amounts of noise and pattern variation. We then propose a neuron model where patterns detected on proximal dendrites lead to action potentials, defining the classic receptive field of the neuron, and patterns detected on basal and apical dendrites act as predictions by slightly depolarizing the neuron without generating an action potential. By this mechanism, a neuron can predict its activation in hundreds of independent contexts. We then present a network model based on neurons with these properties that learns time-based sequences. The network relies on fast local inhibition to preferentially activate neurons that are slightly depolarized. Through simulation we show that the network scales well and operates robustly over a wide range of parameters as long as the network uses a sparse distributed code of cellular activations. We contrast the properties of the new network model with several other neural network models to illustrate the relative capabilities of each. We conclude that pyramidal neurons with thousands of synapses, active dendrites, and multiple integration zones create a robust and powerful sequence memory. Given the prevalence and similarity of excitatory neurons throughout the neocortex and the importance of sequence memory in inference and behavior, we propose that this form of sequence memory may be a universal property of neocortical tissue.}
}

@INPROCEEDINGS{hay_ire60markI,
  author = {Hay, J. C. and Martin, F. C. and Wightman, C. W.},
  title = {The {MARK I} perceptron - design and performance},
  booktitle = {{IRE} National Convention Record},
  year = {1960},
  volume = {2},
  pages = {78--87}
}

@article{hay2011,
    author = {Hay, Etay AND Hill, Sean AND Sch{\"u}rmann, Felix AND Markram, Henry AND Segev, Idan},
    journal = {PLoS Comput Biol},
    publisher = {Public Library of Science},
    title = {Models of Neocortical Layer 5b Pyramidal Cells Capturing a Wide Range of Dendritic and Perisomatic Active Properties},
    year = {2011},
    month = {07},
    volume = {7},
    url = {http://dx.doi.org/10.1371%2Fjournal.pcbi.1002107},
    pages = {e1002107},
    abstract = {
        <title>Author Summary</title>
        <p>The pyramidal cell of layer 5b in the mammalian neocortex extends its dendritic tree to all six layers of cortex, thus receiving inputs from the entire cortical column and supplying the major output of the column to other brain areas. L5b pyramidal cells have been the subject of extensive experimental and modeling studies, yet realistic models of these cells that faithfully reproduce both their perisomatic Na<sup>+</sup> and dendritic Ca<sup>2+</sup> firing behaviors are still lacking. Using an automated algorithm and a large body of experimental data, we generated a set of models that faithfully replicate a range of active dendritic and perisomatic properties of L5b pyramidal cells, as well as the experimental variability of the properties. Furthermore, we show a useful way to analyze model parameters with our sets of models, which enabled us to identify some of the mechanisms responsible for the dynamic properties of L5b pyramidal cells as well as mechanisms that are sensitive to morphological changes. This framework can be used to develop a database of faithful models for other neuron types. The models we present can serve as a powerful tool for theoretical investigations of the contribution of single-cell dynamics to network activity and its computational capabilities.</p>
      },
    number = {7},
    doi = {10.1371/journal.pcbi.1002107}
}

@ARTICLE{hayashi07circuit,
  author = {Hayashi, Yugo and Saeki, Katsutoshi and Sekine, Yoshifumi},
  title = {A synaptic circuit of a pulse-type hardware neuron model with STDP},
  journal = {International Congress Series},
  year = {2007},
  volume = {1301},
  pages = {132--135},
  month = jul,
  abstract = {A number of recent studies of neural networks have been conducted
	with the purpose of applying engineering to the brain. We have been
	attempting to construct artificial neural networks. In the present
	study, we focus on STDP (spike timing dependent synaptic plasticity)
	and construct a neural network from a pulse-type hardware neuron
	model (P-HNM) with STDP. In addition, we propose a simple synaptic
	circuit of the P-HNM with STDP, which is composed of MOSFETs, and
	construct a simple neural network in order to verify the robustness
	with respect to phase fluctuation. We show that a pulse-type hardware
	neural network with STDP synapses has a learning function that has
	robustness with respect to phase fluctuation.},
  booktitle = {Brain-Inspired IT III. Invited and selected papers of the 3rd International
	Conference on Brain-Inspired Information Technology "BrainIT 2006"
	held in Hibikino, Kitakyushu, Japan between 27 and 29 September 2006},
  keywords = {STDP, Pulse-type hardware neuron model, Phase fluctuation, Synaptic
	weight},
  owner = {mreuss},
  timestamp = {2007.06.14},
  url = {\url{http://www.sciencedirect.com/science/article/B7581-4NYJD61-14/2/9b04dfd28d975967fdee19a5bc626257}}
}

@BOOK{haykin99neuralnets,
  title = {Neural Networks: A Comprehensive Foundation},
  publisher = {Prentice Hall},
  year = {1999},
  author = {Haykin, Simon},
  address = {Upper Saddle River, New Jersey},
  isbn = {0-13-273350-1}
}

@ARTICLE{hass2007,
  author = {Ha{\ss}, Joachim and Blaschke, Stefan and Rammsayer, Thomas and Herrmann,
	J M. },
  title = {A neurocomputational model for optimal temporal processing.},
  journal = {Journal of computational neuroscience},
  year = {2008},
  month = {April},
  abstract = {Humans can estimate the duration of intervals of time, and psychophysical
	experiments show that these estimations are subject to timing errors.
	According to standard theories of timing, these errors increase linearly
	with the interval to be estimated (Weber's law), and both at longer
	and shorter intervals, deviations from linearity are reported. This
	is not easily reconciled with the accumulation of neuronal noise,
	which would only lead to an increase with the square root of the
	interval. Here, we offer a neuronal model which explains the form
	of the error function as a result of a constrained optimization process.
	The model consists of a number of synfire chains with different transmission
	times, which project onto a set of readout neurons. We show that
	an increase in the transmission time corresponds to a superlinear
	increase of the timing errors. Under the assumption of a fixed chain
	length, the experimentally observed error function emerges from optimal
	selection of chains for each given interval. Furthermore, we show
	how this optimal selection could be implemented by competitive spike-timing
	dependent plasticity in the connections from the chains to the readout
	network, and discuss implications of our model on selective temporal
	learning and possible neural architectures of interval timing.},
  address = {Bernstein Center for Computational Neuroscience G\"{o}ttingen, Bunsenstr.
	10, 37073, G\"{o}ttingen, Germany, joachim@nld.ds.mpg.de.},
  citeulike-article-id = {2653916},
  doi = {http://dx.doi.org/10.1007/s10827-008-0088-4},
  issn = {0929-5313},
  keywords = {network\_dynamics, stdp, temporal\_coding, time},
  posted-at = {2008-04-11 14:15:53},
  priority = {2},
  url = {http://dx.doi.org/10.1007/s10827-008-0088-4}
}

@article{he2019mlframeworks,
author = {He, Horace},
title = {The State of Machine Learning Frameworks in 2019},
journal = {The Gradient},
year = 2019,
howpublished = {\url{https://thegradient.pub/state-of-ml-frameworks-2019-pytorch-dominates-research-tensorflow-dominates-industry}},
}

@BOOK{hebb2002organization,
  title = {The Organization of Behavior: A Neuropsychological Theory},
  publisher = {Taylor \& Francis},
  year = {2002},
  author = {Hebb, D.O.},
  isbn = {9780805843002},
  lccn = {2002018867}
}

@BOOK{hebb49organbehav,
  title = {The Organization of Behaviour},
  publisher = {Wiley},
  year = {1949},
  author = {Hebb, Donald O.},
  address = {New York},
  keywords = {learning}
}

@TECHREPORT{hechtnielsen04,
  author = {Robert Hecht-Nielsen},
  title = {Perceptrons},
  institution = {Institute for Neural Computation, University of California, San Diego},
  year = {2004},
  keywords = {NN}
}

@BOOK{hechtnielsen89neurocomputing,
  title = {Neurocomputing},
  publisher = {Addison-Wesley},
  year = {1989},
  author = {Hecht-Nielsen, R.},
  keywords = {NN}
}

@MISC{heemskerk95overview,
  author = {Heemskerk, J. N. H.},
  title = {Overview of Neural Hardware},
  howpublished = {In: Neurocomputers for Brain-Style Processing. Design, Implementation
	and Application, {\it PhD thesis}},
  year = {1995},
  institution = {Unit of Experimental and Theoretical Psychology, Leiden University,
	The Netherlands}
}

@MISC{vanheesch97doxygen,
  author = {van Heesch, D.},
  title = {The doxygen documentation system},
  howpublished = {\url{http://www.stack.nl/\~{}dimitri/doxygen}}
}

@MISC{facets_d7-12,
  author = {University Heidelberg and TU Dresden},
  title = {Complete a FACETS stage 2 hardware system},
  howpublished = {{FACETS} Deliverable D7-12},
  year = {2010},
  keywords = {facets, deliverable},
  owner = {smillner},
  timestamp = {2010.08.31}
}

@MISC{facets_d7-9,
  author = {University Heidelberg and TU Dresden},
  title = {Complete a wafer module consisting of the system {PCB}, the communication
	boards containing digital network {ASIC}s and the communication {FPGA},
	as well as the associated mechanical components},
  howpublished = {{FACETS} Deliverable D7-9},
  year = {2010},
  keywords = {facets, deliverable},
  owner = {smillner},
  timestamp = {2010.08.31}
}

@ARTICLE{helias08structural,
  author = {Helias, Moritz and Rotter, Stefan and Gewaltig, Marc-Oliver and Diesmann,
	Markus},
  title = {Structural plasticity controlled by calcium based correlation detection},
  journal = {Front. Neuroinform.},
  year = {2008},
  volume = {2},
  number = {7},
  abstract = {Hebbian learning in cortical networks during development and adulthood
	relies on the presence of a mechanism to detect correlation between
	the presynaptic and the postsynaptic spiking activity. Recently,
	the calcium concentration in spines was experimentally shown to be
	a correlation sensitive signal with the necessary properties: it
	is confined to the spine volume, it depends on the relative timing
	of pre- and postsynaptic action potentials, and it is independent
	of the spine's location along the dendrite. NMDA receptors are a
	candidate mediator for the correlation dependent calcium signal.
	Here, we present a quantitative model of correlation detection in
	synapses based on the calcium influx through NMDA receptors under
	realistic conditions of irregular pre- and postsynaptic spiking activity
	with pairwise correlation. Our analytical framework captures the
	interaction of the learning rule and the correlation dynamics of
	the neurons. We find that a simple thresholding mechanism can act
	as a sensitive and reliable correlation detector at physiological
	firing rates. Furthermore, the mechanism is sensitive to correlations
	among afferent synapses by cooperation and competition. In our model
	this mechanism controls synapse formation and elimination. We explain
	how synapse elimination leads to firing rate homeostasis and show
	that the connectivity structure is shaped by the correlations between
	neighboring inputs.},
  keywords = {structural plasticity, correlation detection, synaptic death, synaptogenesis,
	silent synapses, STDP, Calcium calmodulin dependent kinase II (CaMKII),
	synaptic cooperativity}
}

@ARTICLE{helias2012supercomputers,
    author={Helias, Moritz  and  Kunkel, Susanne  and  Masumoto, Gen  and  Igarashi, Jun  and  Eppler, Jochen Martin  and  Ishii, Shin  and  Fukai, Tomoki  and  Morrison, Abigail  and  Diesmann, Markus},
    title={Supercomputers ready for use as discovery machines for neuroscience},
    journal={Frontiers in Neuroinformatics},
    volume={6},
    year={2012},
    number={26},
    URL={http://www.frontiersin.org/neuroinformatics/10.3389/fninf.2012.00026/abstract},
    DOI={10.3389/fninf.2012.00026},
    ISSN={1662-5196},
    abstract={NEST is a widely used tool to simulate biological spiking neural networks. Here we explain the
improvements, guided by a mathematical model of memory consumption, that enable us to exploit
for the first time the computational power of the K supercomputer for neuroscience. Multi-threaded
components for wiring and simulation combine 8 cores per MPI process to achieve excellent scaling.
K is capable of simulating networks corresponding to a brain area with 10^8 neurons and 10^12 synapses
in the worst case scenario of random connectivity; for larger networks of the brain its hierarchical
organization can be exploited to constrain the number of communicating computer nodes. We
discuss the limits of the software technology, comparing maximum-?lling scaling plots for K and
the JUGENE BG/P system. The usability of these machines for network simulations has become
comparable to running simulations on a single PC. Turn-around times in the range of minutes even
for the largest systems enable a quasi-interactive working style and render simulations on this scale
a practical tool for computational neuroscience.}
}

@ARTICLE{hellwig2000,
  author = {Hellwig, Bernhard},
  title = {A quantitative analysis of the local connectivity between pyramidal
	neurons in layers 2/3 of the rat visual cortex},
  journal = {Biological Cybernetics},
  year = {2000},
  volume = {82},
  pages = {111-121},
  note = {10.1007/PL00007964},
  affiliation = {Max-Planck-Institut für biologische Kybernetik, Spemannstr. 38, D-72076
	Tübingen, Germany DE},
  issn = {0340-1200},
  issue = {2},
  keyword = {Biomedical and Life Sciences},
  publisher = {Springer Berlin / Heidelberg},
  url = {http://dx.doi.org/10.1007/PL00007964}
}

@ARTICLE{hendrickson2011capabilities,
  author = {Hendrickson, E.B. and Edgerton, J.R. and Jaeger, D.},
  title = {The capabilities and limitations of conductance-based compartmental
	neuron models with reduced branched or unbranched morphologies and
	active dendrites},
  journal = {Journal of computational neuroscience},
  year = {2011},
  volume = {30},
  pages = {301--321},
  number = {2},
  publisher = {Springer}
}

@BOOK{hennessy2007,
  title = {{Computer architecture: a quantitative approach}},
  publisher = {{Morgan Kaufmann}},
  year = {2007},
  author = {John L. Hennessy and David A. Patterson},
  address = {Amsterdam}
}

@MISC{henning_date02,
  author = {Henning, Eckhard and Sommer, Ralf and Charlack, Lisa},
  title = {An Automated Approach for Sizing Complex Analog Circuits in a Simulation-Based
	Flow},
  howpublished = {Conference and Exhibition on Design Automation \& Test in Europe
	(DATE 2002)},
  month = mar,
  year = {2002},
  address = {Le Palais de Congres, Paris, France},
  groupsearch = {0}
}

@BOOK{herlihy2008,
  title = {The Art of Multiprocessor Programming},
  publisher = {Morgan Kaufmann},
  year = {2008},
  author = {Herlihy, Maurice and Shavit, Nir},
  month = {March},
  day = {03},
  howpublished = {Paperback},
  isbn = {0123705916},
  posted-at = {2008-10-20 06:41:42},
  priority = {2}
}

@BOOK{hertz91theory,
  title = {Introduction to the theory of neural computation},
  publisher = {Addison Wesley Publishing Company},
  year = {1991},
  author = {Hertz, John and Krogh, Anders and Palmer, Richard G.},
  address = {Redwood City, CA},
  isbn = {0-201-50395-6}
}

@INPROCEEDINGS{hesser_ppsn91mutation,
  author = {Hesser, J. and M{\"a}nner, R.},
  title = {Towards an optimal Mutation Probability for Genetic Algorithms},
  booktitle = {Proceedings of the 1st International Conference on Parallel Problem
	Solving from Nature},
  year = {1991},
  editor = {Schwefel, H.-P. and M{\"a}nner, R.},
  volume = {496},
  pages = {23--32},
  publisher = {Springer Verlag},
  howpublished = {Lecture Notes in Computer Science}
}

@mastersthesis{hornung2020ba,
  author   = {Moritz Hornung},
  title    = {Adapting the Cortical Microcircuit Model for the BrainScaleS-1 hardware},
  school   = {Universit{\"a}t Heidelberg},
  year     = {2020},
  type     = {Bachelor thesis},
}

@MISC{ucikdd,
  author = {Hettich, S. and Bay, S. D.},
  title = {The {UCI} {KDD} Archive},
  howpublished = {University of California, Department of Information and Computer
	Science, Irvine, USA, \texttt{http://kdd.ics.uci.edu}},
  year = {1999},
  groupsearch = {0},
  institution = {Irvine, CA: University of California, Department of Information and
	Computer Science}
}

@INPROCEEDINGS{hickmann2020nervana,
  author = {Hickmann, Brian and Chen, Jieasheng and Rotzin, Michael and Yang, Andrew and Urbanski, Maciej and Avancha, Sasikanth},
  booktitle = {2020 {IEEE} 27th Symposium on Computer Arithmetic ({ARITH})},
  title = {Intel Nervana Neural Network Processor-T ({NNP-T}) Fused Floating Point Many-Term Dot Product},
  year = 2020,
  pages = {133--136},
  doi = {10.1109/ARITH48897.2020.00029}
}

@ARTICLE{hilditch_mi69,
  author = {C.J. Hilditch},
  title = {Linear skeletons from square cupboards},
  journal = {Machine Intelligence},
  year = {1969},
  volume = {4},
  pages = {403-420}
}

@INBOOK{hines03neuron,
  pages = {769-773},
  title = {The NEURON simulation environment.},
  publisher = {M.A. Arbib},
  year = {2003},
  author = {M.L. Hines and N.T. Carnevale},
  booktitle = {The Handbook of Brain Theory and Neural Networks},
  howpublished = {Cambridge, MA: MIT Press},
  key = {hines03neuron},
  keywords = {neuron,simulator}
}

@MISC{neuronhomepage,
  author = {Michael Hines and John W. Moore and Ted Carnevale},
  title = {NEURON},
  year = {2008},
  url = {\url{http://neuron.duke.edu}}
}

@MISC{neuronhomepage2,
  author = {Michael Hines and John W. Moore and Ted Carnevale},
  title = {NEURON},
  year = {2008},
  url = {http://neuron.duke.edu}
}

@ARTICLE{hines_modeldb2004,
  author = {ML Hines and T Morse and M Migliore and NT Carnevale and GM Shepherd},
  title = {{M}odel{DB}: A Database to Support Computational Neuroscience},
  journal = {Journal of Computational Neuroscience},
  year = {2004},
  volume = {17},
  pages = {7--11},
  number = {1}
}

@ARTICLE{hines06neuron, crossref = {carnevale2006neuron} }
@BOOK{carnevale2006neuron,
  author = {Carnevale, Nicholas T and Hines, Michael L},
  title = {The {NEURON} Book},
  publisher = {Cambridge University Press},
  year = {2006},
  address = {Cambridge, UK},
  isbn = {978-0521843218},
  doi = {10.1017/CBO9780511541612},
}

@ARTICLE{hines09,
  author = {Hines, Michael L. and Davison, Andrew P. and Muller, Eilif},
  title = {{NEURON and Python}},
  journal = {Front. Neuroinform.},
  year = {2009},
  abstract = {The NEURON simulation program now allows Python to be used, alone
	or in combination with NEURON's traditional Hoc interpreter. Adding
	Python to NEURON has the immediate benefit of making available a
	very extensive suite of analysis tools written for engineering and
	science. It also catalyzes NEURON software development by offering
	users a modern programming tool that is recognized for its flexibility
	and power to create and maintain complex programs. At the same time,
	nothing is lost because all existing models written in Hoc, including
	GUI tools, continue to work without change and are also available
	within the Python context. An example of the benefits of Python availability
	is the use of the XML module in implementing NEURON's Import3D and
	CellBuild tools to read MorphML and NeuroML model specifications.},
  keywords = {Python, simulation environment, computational neuroscience}
}

@MASTERSTHESIS{hinrichs2014bachelorthesis,
  author   = {David Hinrichs},
  title    = {Software Development in the Context of Dendrite Membrane Simulation},
  year     = 2014,
  type     = {Bachelor thesis},
  school   = {Rupercht-Karls-Universit{\"a}t Heidelberg}
}

@INPROCEEDINGS{hinterding_ieeeecep97adaptation,
  author = {Hinterding, R. and Michalewicz, Z. and Eiben, A. E.},
  title = {Adaptation in Evolutionary Computation: {A} Survey},
  booktitle = {{IEEECEP}: Proceedings of The {IEEE} Conference on Evolutionary Computation,
	{IEEE} World Congress on Computational Intelligence},
  year = {1997},
  file = {hinterding_ieeeecep97adaptation.pdf:hinterding_ieeeecep97adaptation.pdf:PDF}
}

@article{hinton1986learning,
  title={Learning and relearning in Boltzmann machines},
  author={Hinton, Geoffrey E and Sejnowski, Terrance J},
  journal={MIT Press, Cambridge, Mass},
  volume={1},
  pages={282--317},
  year={1986}
}

@article{hinton2002training,
  title={Training products of experts by minimizing contrastive divergence},
  author={Hinton, Geoffrey E},
  journal={Neural computation},
  volume={14},
  number={8},
  pages={1771--1800},
  year={2002},
  publisher={MIT Press}
}

@article{hinton2006fast,
  title={A fast learning algorithm for deep belief nets},
  author={Hinton, Geoffrey E and Osindero, Simon and Teh, Yee-Whye},
  journal={Neural computation},
  volume={18},
  number={7},
  pages={1527--1554},
  year={2006},
  publisher={MIT Press}
}

@article{hinton2006reducing,
  title={Reducing the dimensionality of data with neural networks},
  author={Hinton, Geoffrey E and Salakhutdinov, Ruslan R},
  journal={Science},
  volume={313},
  number={5786},
  pages={504--507},
  year={2006},
  publisher={American Association for the Advancement of Science}
}

@article{hinton2007learning,
  title={Learning multiple layers of representation},
  author={Hinton, Geoffrey E},
  journal={Trends in cognitive sciences},
  volume={11},
  number={10},
  pages={428--434},
  year={2007},
  publisher={Elsevier}
}

@article{hinton2010practical,
  title={A practical guide to training restricted Boltzmann machines},
  author={Hinton, Geoffrey},
  journal={Momentum},
  volume={9},
  number={1},
  year={2010}
}

@ARTICLE{hinton2012deep,
  author={Hinton, G. and Li Deng and Dong Yu and Dahl, G.E. and Mohamed, A. and Jaitly, N. and Senior, A. and Vanhoucke, V. and Nguyen, P. and Sainath, T.N. and Kingsbury, B.},
  journal={Signal Processing Magazine, IEEE},
  title={Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups},
  year={2012},
  volume={29},
  number={6},
  pages={82-97},
  keywords={Gaussian processes;feedforward neural nets;hidden Markov models;speech recognition;Gaussian mixture models;HMM states;acoustic modeling;deep neural networks;feed-forward neural network;hidden Markov models;posterior probabilities;speech recognition;temporal variability;Acoustics;Automatic speech recognition;Data models;Gaussian processes;Hidden Markov models;Neural networks;Speech recognition;Training},
  doi={10.1109/MSP.2012.2205597},
  ISSN={1053-5888},
  month={Nov}
}

@ARTICLE{hirsch1991,
  author = {Hirsch, JA and Gilbert, CD},
  title = {Synaptic physiology of horizontal connections in the cat's visual
	cortex},
  journal = {The Journal of Neuroscience},
  year = {1991},
  volume = {11},
  pages = {1800-1809},
  number = {6},
  eprint = {http://www.jneurosci.org/content/11/6/1800.full.pdf+html},
  url = {http://www.jneurosci.org/content/11/6/1800.abstract}
}

@MISC{hock09diplomathesis,
  author = {Hock, Matthias},
  title = {Test of Components for a Wafer-Scale Neuromorphic Hardware System},
  howpublished = {Diploma thesis, University of Heidelberg, HD-KIP-09-37, \url{http://www.kip.uni-heidelberg.de/Veroeffentlichungen/details.php?id=1935}},
  year = {2009}
}

@INPROCEEDINGS{hock13analogmemory,
	author = {Hock, M. and Hartel, A. and Schemmel, J. and Meier, K.},
	booktitle = {Circuit Theory and Design (ECCTD), 2013 European Conference on},
	title = {An analog dynamic memory array for neuromorphic hardware},
	year = 2013,
	pages = {1-4},
	keywords = {analogue storage;content-addressable storage;mixed
		analogue-digital integrated circuits;neural chips;ramp
			generators;analog dynamic memory array;analog
			voltages;content-addressable memory;highly-configurable large-scale
			neuromorphic hardware;mixed-signal low-power process;power
			consumption;size 65 nm;voltage ramp
			generator;Arrays;Capacitors;Hardware;Logic
			gates;Programming;Radiation detectors;Transistors},
	doi = {10.1109/ECCTD.2013.6662229},
	month = sep
}

@PHDTHESIS{hock14phd,
  author = {Matthias Hock},
  title = {Modern Semiconductor Technologies for Neuromorphic Hardware},
  year = {2014},
  school = {Ruprecht-Karls-Universit{\"a}t Heidelberg},
}

@MISC{route65,
  author = {Matthias Hock and Andreas Hartel and Johannes Schemmel},
  title = {The route65 prototype chip},
  month = {April},
  year = {2013},
  note = {personal communication},
  owner = {simon},
  timestamp = {2013.05.05}
}

@ARTICLE{hodgkin52,
  author = {Alan Lloyd Hodgkin and Andrew F. Huxley},
  title = {A quantitative description of membrane current and its application
	to conduction and excitation in nerve.},
  journal = {J Physiol},
  year = {1952},
  volume = {117},
  pages = {500--544},
  number = {4},
  month = {August},
  citeulike-article-id = {851137},
  issn = {0022-3751},
  keywords = {action-potential, differential-equations, modeling, neuron},
  posted-at = {2007-11-19 03:11:10},
  priority = {4},
  url = {http://view.ncbi.nlm.nih.gov/pubmed/12991237}
}

@ARTICLE{hodgkin1958croonian,
  author = {Alan Lloyd Hodgkin},
  title = {The Croonian Lecture: Ionic Movements and Electrical Activity in Giant Nerve Fibres},
  volume = {148},
  number = {930},
  pages = {1-37},
  year = {1958},
  journal = {Proceedings of the Royal Society B}
}

@BOOK{hohmann01zahntechnik,
  title = {Lehrbuch der Zahntechnik, Band I},
  publisher = {Quintessenz Verlags-GmbH},
  year = {2001},
  author = {Hohmann, A. and Hielscher, W.},
  address = {Berlin},
  isbn = {3-87652-122-X}
}

@MISC{hohmann_05phdthesis,
  author = {Hohmann, Steffen},
  howpublished = {{\it PhD thesis}, University of Heidelberg, in preparation},
  year = {2005},
  keywords = {vision}
}

@INPROCEEDINGS{hohmann_ijcnn04,
  author = {S.G. Hohmann and J. Fieres and K. Meier and J. Schemmel and T. Schmitz
	and F. Sch{\"u}rmann },
  title = {Training Fast Mixed-Signal Neural Networks for Data Classification},
  booktitle = {Proceedings of the 2004 International Joint Conference on Neural
	Networks (IJCNN'04)},
  year = {2004},
  pages = {2647--2652},
  publisher = {IEEE Press},
  key = {hohmann_ijcnn04},
  keywords = {vision, learning}
}

@INPROCEEDINGS{hohmann_gecco02,
  author = {Hohmann, S. and Schemmel, J. and Sch{\"u}rmann, F. and Meier, K.},
  title = {Exploring the Parameter Space of a Genetic Algorithm for Training
	an Analog Neural Network},
  booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference
	{GECCO} 2002},
  year = {2002},
  editor = {Langdon, W.B. et. al.},
  pages = {375--382},
  month = {July},
  publisher = {Morgan Kaufmann Publishers},
  key = {hohmann_gecco2002},
  keywords = {vision}
}

@INPROCEEDINGS{hohmann_ijcnn03,
  author = {Steffen G. Hohmann and Johannes Schemmel and Felix Sch{\"u}rmann
	and Karlheinz Meier},
  title = {Predicting Protein Cellular Localization Sites with a Hardware Analog
	Neural Network},
  booktitle = {Proceedings of the Int.\ Joint Conf.\ on Neural Networks},
  year = {2003},
  pages = {381--386},
  month = {July},
  publisher = {IEEE Press},
  isbn = {0-7803-7899-7},
  key = {hohmann_ijcnn03},
  keywords = {vision}
}

@ARTICLE{holland73genetic,
  author = {Holland, J. H.},
  title = {Genetic algorithms and the optimal allocation of trials},
  journal = {SIAM J.\ of Computing},
  year = {1973},
  volume = {2},
  pages = {88--105}
}

@ARTICLE{holler91vlsi,
  author = {Holler, M.},
  title = {{VLSI} implementation of learning and memory systems: {A} review},
  journal = {Advances in Neural Information Processing Systems},
  year = {1991},
  volume = {3},
  editor = {Lippmann, R. P.}
}

@ARTICLE{holmes90insights,
  author = {W. R. Holmes and W. B. Levy},
  title = {Insights into associative long-term potentiation from computational
	models of {NMDA} receptor-mediated calcium influx and intracellular
	Calcium concentration changes},
  journal = {Journal of Neurophysiology},
  year = {1990},
  volume = {63},
  pages = {1148-1168},
  keywords = {ltp }
}

@MISC{holt-membrane,
  author = {Gary R. Holt and Christof Koch and Rodney J. Douglas and Misha Mahowald},
  title = {The Membrane Time Constant and Firing Rate Dynamics},
  abstract = {The subthreshold membrane time constant \o{} governs how quickly the
	membrane potential approaches equilibrium, and has been used to estimate
	how quickly a neuron can respond to its inputs. However, spiking
	neurons do not have an equilibrium voltage; subthreshold dynamics
	do not apply to their firing rates. In fact, a spiking neuron can
	respond much faster than \o{} . For current step inputs, a non-adapting
	spiking neuron reaches its final firing after a single interspike
	interval, and an ensemble of such neurons can respond arbitrarily
	fast. Firing rate dynamics are controlled by postsynaptic conductances,
	adaptation, and other processes in the cell, rather than passive
	properties of the membrane. The spiking mechanism can speed up neuronal
	responses, so that information can be passed to successive stages
	of a feedforward network in considerably less time than \o{} . Introduction
	Computations in the nervous system can be considered on many different
	organizational scales, from the filtering p...},
  citeseerurl = {citeseer.nj.nec.com/70702.html},
  keywords = {spiking}
}

@BOOK{Holz1999,
  title = {Basic Neurochemistry: Molecular, Cellular and Medical Aspects},
  publisher = {Lippincott-Raven},
  year = {1999},
  editor = {Siegel, George J. and Agranoff, Bernard W. and Albers, R. Wayne and
	Fisher, Stephen K. and Uhler, Michael D.},
  author = {Holz, Ronald W. and Fisher, Stephen K.},
  owner = {simon},
  timestamp = {2013.04.29},
  url = {http://www.ncbi.nlm.nih.gov/books/NBK27911/}
}

@ARTICLE{hopfield84graded,
  author = {Hopfield, J. J.},
  title = {Neurons with graded response have collective computational properties
	like those of two-state neurons},
  journal = {Proceedings of the National Academy of Sciences},
  year = {1984},
  volume = {81},
  pages = {3088--3092},
  booktitle = {Proceedings of the National Academy of Sciences}
}

@ARTICLE{hopfield82collective,
  author = {Hopfield, J. J.},
  title = {Neural networks and physical systems with emergent collective computational
	abilities},
  journal = {Proceedings of the National Academy of Sciences},
  year = {1982},
  volume = {79},
  pages = {2554--2558},
  booktitle = {Proceedings of the National Academy of Sciences}
}

@ARTICLE{hopfield04learning,
  author = {Hopfield, J. J. and Brody, Carlos D.},
  title = {{Learning rules and network repair in spike-timing-based computation
	networks}},
  journal = {PNAS},
  year = {2004},
  volume = {101},
  pages = {337-342},
  number = {1},
  month = {January},
  abstract = {Plasticity in connections between neurons allows learning and adaptation,
	but it also allows noise to degrade the function of a network. Ongoing
	network self-repair is thus necessary. We describe a method to derive
	spike-timing-dependent plasticity rules for self-repair, based on
	the firing patterns of a functioning network. These plasticity rules
	for self-repair also provide the basis for unsupervised learning
	of new tasks. The particular plasticity rule derived for a network
	depends on the network and task. Here, self-repair is illustrated
	for a model of the mammalian olfactory system in which the computational
	task is that of odor recognition. In this olfactory example, the
	derived rule has qualitative similarity with experimental results
	seen in spike-timing-dependent plasticity. Unsupervised learning
	of new tasks by using the derived self-repair rule is demonstrated
	by learning to recognize new odors. },
  eprint = {http://www.pnas.org/cgi/reprint/101/1/337.pdf},
  file = {hopfield04learning.pdf:hopfield04learning.pdf:PDF},
  keywords = {learning plasticity spiking},
  url = {\url{http://www.pnas.org/cgi/content/abstract/101/1/337}}
}

@INPROCEEDINGS{hopfield98computing,
  author = {John J. Hopfield and Carlos D. Brody and Sam Roweis},
  title = {Computing with Action Potentials},
  booktitle = {Advances in Neural Information Processing Systems},
  year = {1998},
  editor = {Michael I. Jordan and Michael J. Kearns and Sara A. Solla},
  volume = {10},
  publisher = {The {MIT} Press},
  citeseerurl = {citeseer.nj.nec.com/hopfield98computing.html},
  keywords = {spiking}
}

@BOOK{horak03,
  title = {Telecommunications and data communications handbook},
  publisher = {Wiley-Interscience},
  year = {2007},
  author = {Ray Horak}
}

@ARTICLE{hornik89approx,
  author = {Hornik, K and Stinchcombe, M. and White, H.},
  title = {Multilayer Feedforward Networks are Universal Approximators},
  journal = {Neural Networks},
  year = {1989},
  volume = {4},
  pages = {359--366},
  number = {2}
}

@INPROCEEDINGS{horton_ismb97better,
  author = {Horton, Paul and Nakai, Kenta},
  title = {Better Prediction of Protein Cellular Localization Sites with the
	{\it k} Nearest Neighbors Classifier},
  booktitle = {Proceedings of the 5th International Conference on Intelligent Systems
	in Molecular Biology},
  year = {1997},
  pages = {147--152},
  publisher = {AAAIPress},
  file = {horton_ismb97better.pdf:horton_ismb97better.pdf:PDF}
}

@INPROCEEDINGS{horton_ismb96probabilistic,
  author = {Horton, Paul and Nakai, Kenta},
  title = {A Probabilistic Classification System for Predicting the Cellular
	Localization Sites of Proteins},
  booktitle = {Proceedings of the 4th International Conference on Intelligent Systems
	for Molecular Biology},
  year = {1996},
  pages = {109--115},
  publisher = {AAAIPress},
  file = {horton_ismb96probabilistic.pdf:horton_ismb96probabilistic.pdf:PDF}
}

@BOOK{hromkovic04algorithmics,
  title = {Algorithmics for Hard Problems},
  publisher = {Springer Verlag},
  year = {2004},
  author = {Hromkovi{\u c}, J.},
  address = {Berlin, Heidelberg, New York},
  isbn = {3-540-44134-4}
}

@MANUAL{bsim3v2.2,
  title = {The BSIM3v3.2 MOSFET Model},
  author = {Chenming Hu and Weidong Liu and Xiaodong Jin},
  month = {Dec},
  year = {1998}
}

@article{hu2014memristor,
  title={Memristor crossbar-based neuromorphic computing system: A case study},
  author={Hu, Miao and Li, Hai and Chen, Yiran and Wu, Qing and Rose, Garrett S and Linderman, Richard W},
  journal={IEEE transactions on neural networks and learning systems},
  volume=25,
  number=10,
  pages={1864--1878},
  year=2014,
  publisher={IEEE}
}

@ARTICLE{huang04plasticity,
  author = {Huang, Yan-You and Pittenger, Christopher and Kandel, Eric R.},
  title = {{A form of long-lasting, learning-related synaptic plasticity in
	the hippocampus induced by heterosynaptic low-frequency pairing}},
  journal = {PNAS},
  year = {2004},
  volume = {101},
  pages = {859-864},
  number = {3},
  abstract = {The late, transcription- and translation-dependent phase of long-term
	synaptic potentiation (L-LTP) at the Schaffer collateral synapse
	of the hippocampus is an experimental model of the synaptic plasticity
	underlying long-lasting memory formation. L-LTP is typically induced
	by homosynaptic tetanic stimulation; but associative forms of learning
	are likely to require the heterosynaptic pairing of stimuli. Here
	we describe L-LTP elicited by such heterosynaptic pairing at the
	Schaffer collateral synapse in mice. We find that repeated stimulation
	of one pathway at low frequency (0.2 Hz), which does not by itself
	induce synaptic potentiation, will produce long-lasting synaptic
	plasticity when paired with a brief conditioning burst applied to
	an independent afferent pathway. The induction of heterosynaptic
	L-LTP is associative and critically depends on the precise time interval
	of pairing: simultaneous, conjunctional pairing induces L-LTP; in
	contrast, delayed pairing induces short-lasting early-phase LTP.
	Heterosynaptically induced early-phase LTP could be depotentiated
	by repeatedly presenting unpaired test stimuli, whereas L-LTP could
	not. This heterosynaptically induced L-LTP requires PKA and protein
	synthesis. In addition, heterosynaptically induced L-LTP is impaired
	in transgenic mice that express KCREB (a dominant negative inhibitor
	of adenosine 3'5'-cyclic monophosphate response element-binding protein-mediated
	transcription) in the hippocampus. These mice have previously been
	shown to be impaired in spatial memory but have normal L-LTP as induced
	by a conventional homosynaptic tetanic protocol. These data suggest
	that at least in some instances this L-LTP-inducing protocol may
	better model behaviorally relevant information storage and the in
	vivo mechanisms underlying long-lasting memories. },
  eprint = {http://www.pnas.org/cgi/reprint/101/3/859.pdf},
  file = {huang04plasticity.pdf:huang04plasticity.pdf:PDF},
  keywords = {learning plasticity},
  url = {\url{http://www.pnas.org/cgi/content/abstract/101/3/859}}
}

@ARTICLE{hubel65cortex,
  author = {D. Hubel and T. Wiesel},
  title = {Receptive fields and functional architecture in two non-striate visual
	areas (18 and 19) of the cat},
  journal = {J. Neurophysiology},
  year = {1965},
  volume = {28},
  pages = {229-289},
  owner = {fieres}
}

@ARTICLE{hubel62cortex,
  author = {D. Hubel and T. Wiesel},
  title = {Receptive fields, binocular interaction and functional architecture
	in the cat's visual cortex},
  journal = {J.Physiology},
  year = {1962},
  volume = {160},
  pages = {106-154},
  owner = {fieres}
}

@BOOK{huettel_04fmri,
  title = {{F}unctional {M}agnetic {R}esonance {I}maging},
  publisher = {Sinauer Associates},
  year = {2004},
  author = {Scott A. Huettel and Allen W. Song and Gregory McCarthy},
  owner = {mueller},
  timestamp = {2008.09.07}
}

@ARTICLE{Matplotlib2007,
  author = {John D. Hunter},
  title = {Matplotlib: A {2D} Graphics Environment},
  journal = {IEEE Computing in Science and Engineering},
  year = {2007},
  volume = {9},
  pages = {90--95},
  number = {3}
}

@ARTICLE{hunsberger2015, crossref = {hunsberger2015spiking} }
@ARTICLE{hunsberger2015spiking,
   author = {{Hunsberger}, E. and {Eliasmith}, C.},
    title = {Spiking Deep Networks with {LIF} Neurons},
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1510.08829},
 primaryClass = "cs.LG",
 keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
     year = 2015,
    month = oct,
   adsurl = {http://adsabs.harvard.edu/abs/2015arXiv151008829H},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@INPROCEEDINGS{husbands_aisb94distributed,
  author = {Husbands, P.},
  title = {Distributed co-evolutionary genetic algorithms for multi-criteria
	and multi-constraint optimisation},
  booktitle = {Evolutionary Computing:\ Proceedings of the {AISB} workshop},
  year = {1994},
  editor = {Fogarty, T. C.},
  pages = {150--165},
  address = {Berlin, Heidelberg, New York},
  publisher = {Springer Verlag}
}

@MISC{husmann_2011internship,
  author = {Husmann, Kai},
  title = {Internship -- {HMF} Transmitter},
  year = {2011},
  month = nov,
  institution = {Kirchhoff-Institut f\"ur Physik, Ruprecht-Karls-Universit\"at Heidelberg},
  url = {http://www.kip.uni-heidelberg.de/cms/fileadmin/groups/vision/Downloads/Internship_Reports/report_khusmann.pdf}
}

@mastersthesis{husmann_2012bscthesis,
  author   = {Kai-Hajo Husmann},
  title    = {Handling Spike Data in an Accelerated Neuromorphic System},
  school   = {Ruprecht-Karls-Universit\"at Heidelberg},
  year     = {2012},
  type     = {BSc Thesis}
}

@article{huynh2022implementing,
      title={Implementing Spiking Neural Networks on Neuromorphic Architectures: A Review},
      author={Phu Khanh Huynh and M. Lakshmi Varshika and Ankita Paul and Murat Isik and Adarsha Balaji and Anup Das},
      year=2022,
      eprint={2202.08897},
      archivePrefix={arXiv},
      journal={arXiv preprint},
      primaryClass={cs.NE}
}

@MISC{facets_d7-4,
  author = {Dan Husmann},
  title = {Verify the manufacturability of the wafer-scale system by producing
	and testing a prototype wafer},
  howpublished = {{FACETS} Deliverable D7-4},
  year = {2008},
  note = {University Heidelberg},
  keywords = {facets, deliverable},
  owner = {agruebl},
  timestamp = {2010.09.14}
}

@MISC{facets_wsi,
  author = {Dan Husmann and Holger Zoglauer},
  title = {A Wafer-Scale-Intagration System{(WSI)}},
  howpublished = {{FACETS} project internal documentation},
  year = {2010},
  abstract = {The system delivers the mechanical and electrical platform for the
	realization of a large neuronal network in hardware},
  keywords = {facets, documentation},
  owner = {agruebl}
}

@MISC{bluegene_web,
  author = {{IBM}},
  title = {System Blue Gene Solution},
  howpublished = {\url{ibm.com/systems/deepcomputing/bluegene/ }},
  year = {2010}
}

@MISC{ppc405_2005,
  author = {{IBM}},
  title = {PPC405Fx Embedded Processor Core User’s Manual},
  month = {January},
  year = {2005},
  owner = {simon},
  timestamp = {2013.04.15}
}

@MISC{IBM1998,
  author = {Microcontroller Applications {IBM}},
  title = {Developing PowerPC Embedded Application Binary Interface (EABI) Compliant
	Programs},
  month = {September},
  year = {1998},
  note = {Version 1.0},
  owner = {simon},
  timestamp = {2013.03.27}
}

@MISC{IBM1998a,
  author = {Microelectronics Division {IBM}},
  title = {The PowerPC 405 Core},
  howpublished = {Whitepaper},
  month = {November},
  year = {1998},
  owner = {simon},
  timestamp = {2013.03.28}
}

@INPROCEEDINGS{ide2002random,
    author = {Ide, J. S. and Cozman, F. G.},
    title = {Random {G}eneration of {B}ayesian {N}etworks},
    booktitle = {In Brazilian Symp. On Artificial Intelligence},
    year = {2002},
    pages = {366--375},
    publisher = {Springer-Verlag}
}

@TECHREPORT{ieee_gbe,
  author = {{IEEE 802.3 Ethernet Working Group}},
  title = {{802.3ab (1000Base-T)}},
  institution = {},
  year = 1999,
  type = {{IEEE} Standard}
}

@TECHREPORT{ieee_10gbe,
  author = {{IEEE 802.3 Ethernet Working Group}},
  title = {802.3ak ({10GBASE-CX4})},
  institution = {},
  year = 2004,
  type = {{IEEE} Standard}
}

@proceedings{ieee2014specialissue,
  title={Engineering Intelligent Electronic Systems Based on Computational Neuroscience},
  volume={102: 5},
  number={5},
  year={2014},
  series={Proceedings of the IEEE},
  editor ={McDonnell, Mark D and Boahen, Kwabena and Ijspeert, Auke and Sejnowski, Terrence J},
  month =     {May},
  note = {Special Issue},
}

@TECHREPORT{ieee2004posix,
  author = {IEEE},
  title = {Standard for information technology - portable operating system interface
	({POSIX}). Shell and utilities},
  institution = {IEEE},
  year = {2004},
  booktitle = {IEEE Std 1003.1, 2004 Edition. The Open Group Technical Standard.
	Base Specifications, Issue 6. Includes IEEE Std 1003.1-2001, IEEE
	Std 1003.1-2001/Cor 1-2002 and IEEE Std 1003.1-2001/Cor 2-2004. Shell
	and Utilities},
  journal = {IEEE Std 1003.1, 2004 Edition. The Open Group Technical Standard.
	Base Specifications, Issue 6. Includes IEEE Std 1003.1-2001, IEEE
	Std 1003.1-2001/Cor 1-2002 and IEEE Std 1003.1-2001/Cor 2-2004. Shell
	and Utilities},
  keywords = {ieee, operating, posix, shell, specification, system, systems, thread,
	threaded, threads, unix},
  owner = {bruederl},
  posted-at = {2008-06-18 10:50:13},
  priority = {0},
  timestamp = {2008.09.04},
  url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1309816}
}

@MISC{tokenring,
  author = {{IEEE}},
  title = {{IEEE} {T}oken {R}ing standards},
  howpublished = {\url{http://www.ieee802.org/5/}}
}

@MANUAL{plx_company,
  title = {PLX 9054 Datasheet},
  author = {PLX Technology Inc.},
  address = {870 Maude Avenue, Sunnyvale, CA 94085, USA},
  annote = {plx chip on darkwing},
  groupsearch = {0},
  keywords = {spec}
}

@MISC{incf_software_homepage,
  author = {{INCF Software Database}},
  title = {Website},
  year = {2008},
  url = {http://software.incf.net}
}

@inproceedings{indiveri2003low,
  title={A low-power adaptive integrate-and-fire neuron circuit},
  author={Indiveri, Giacomo},
  booktitle={ISCAS (4)},
  pages={820--823},
  year={2003}
}

@ARTICLE{indiveri2008_vlsi,
  author = {Indiveri, Giacomo},
  title = {Neuromorphic VLSI Models of Selective Attention: From Single Chip
	Vision Sensors to Multi-chip Systems},
  journal = {Sensors},
  year = {2008},
  volume = {8},
  pages = {5352--5375},
  number = {9},
  issn = {1424-8220},
  url = {http://www.mdpi.com/1424-8220/8/9/5352}
}

@ARTICLE{indiveri2000_vlsi,
  author = {Giacomo Indiveri},
  title = {Modeling Selective Attention Using a Neuromorphic Analog VLSI Device},
  journal = {Neural Comput.},
  year = {2000},
  volume = {12},
  pages = {2857--2880},
  number = {12},
  address = {Cambridge, MA, USA},
  doi = {http://dx.doi.org/10.1162/089976600300014755},
  issn = {0899-7667},
  publisher = {MIT Press}
}

@ARTICLE{indiveri2008modeling,
  author = {Indiveri, G.},
  title = {Modeling selective attention using a neuromorphic analog VLSI device},
  journal = {Neural computation},
  year = {2000},
  volume = {12},
  pages = {2857--2880},
  number = {12},
  publisher = {MIT Press}
}

@ARTICLE{indiveri2009artificial,
  author = {Indiveri, G. and Chicca, E. and Douglas, R.},
  title = {Artificial cognitive systems: From {VLSI} networks of spiking neurons
	to neuromorphic cognition},
  journal = {Cognitive Computation},
  year = {2009},
  volume = {1},
  pages = {119--127},
  number = {2},
  month = {Mar}
}

@ARTICLE{indiveri_tnn2006,
  author = {Indiveri, G. and Chicca, E. and Douglas, R.},
  title = {A {VLSI} array of low-power spiking neurons and bistable synapses
	with spike-timing dependent plasticity},
  journal = {IEEE Transactions on Neural Networks},
  year = {2006},
  volume = {17},
  pages = {211--221},
  number = {1},
  month = {Jan}
}

@ARTICLE{indiveri2011, crossref = {indiveri2011neuromorphic} }
@ARTICLE{indiveri2011neuromorphic,
  author = {Giacomo Indiveri and Bernabe Linares-Barranco and Tara Julia Hamilton
	and Andr{\'e} van Schaik and Ralph Etienne-Cummings and Tobi Delbruck
	and Shih-Chii Liu and Piotr Dudek and Philipp H{\"a}fliger and Sylvie
	Renaud and Johannes Schemmel and Gert Cauwenberghs and John Arthur
	and Kai Hynna and Fopefolu Folowosele and Sylvain Saighi and Teresa
	Serrano-Gotarredona and Jayawan Wijekoon and Yingxue Wang and Kwabena
	Boahen},
  title = {Neuromorphic silicon neuron circuits},
  journal = {Frontiers in Neuroscience},
  year = {2011},
  volume = {5},
  number = {0},
  abstract = {Hardware implementations of spiking neurons can be extremely useful
	for a large variety of applications, ranging from high-speed modeling
	of large-scale neural systems to real-time behaving systems, to bidirectional
	brain-machine interfaces. The specific circuit solutions used to
	implement silicon neurons depend on the application requirements.
	In this paper we describe the most common building blocks and techniques
	used to implement these circuits, and present an overview of a wide
	range of neuromorphic silicon neurons, which implement different
	computational models, ranging from biophysically realistic and conductance
	based Hodgkin-Huxley models to bi-dimensional generalized adaptive
	Integrate and Fire models. We compare the different design methodologies
	used for each silicon neuron design described, and demonstrate their
	features with experimental results, measured from a wide range of
	fabricated VLSI chips.},
  doi = {10.3389/fnins.2011.00073},
  issn = {1662-453X},
  url = {http://www.frontiersin.org/Journal/Abstract.aspx?s=755&name=neuromorphic engineering&ART_DOI=10.3389/fnins.2011.00073}
}

@INPROCEEDINGS{indiveri2010,
  author = {Indiveri, G. and Stefanini, F. and Chicca, E.},
  title = {Spike-based learning with a generalized integrate and fire silicon
	neuron},
  booktitle = {Circuits and Systems (ISCAS), Proceedings of 2010 IEEE International
	Symposium on},
  year = {2010},
  pages = {1951 -1954},
  month = {30 2010-june 2},
  abstract = {Spike-based learning circuits have been typically used in conjunction
	with linear integrate-and-flre neurons. As a new class of current-mode
	conductance-based silicon neurons has been recently developed, it
	is important to evaluate how the spike-based learning circuits perform,
	when interfaced to these new types of neuron circuits. Here, we describe
	a VLSI implementation of a current-mode conductance-based neuron,
	connected to synaptic circuits with spike-based learning capabilities.
	The conductance-based silicon neuron has built-in spike-frequency
	adaptation, refractory period mechanisms, and plasticity eligibility
	control circuits. The synaptic circuits exhibits realistic dynamics
	in the post-synaptic currents and comprise local spike-based learning
	circuits, controlled by the global post-synaptic eligibility circuits.
	We present experimental results which characterize the conductance-based
	neuron circuit properties and the spike-based learning circuits connected
	to it.},
  doi = {10.1109/ISCAS.2010.5536980},
  keywords = {VLSI implementation;current-mode conductance;integrate-and-fire silicon
	neuron;plasticity eligibility control circuit;refractory period mechanism;spike-based
	learning circuit;spike-frequency adaptation;synaptic circuit;VLSI;electric
	admittance;neural nets;}
}

@ARTICLE{islam03constructive,
  author = {Islam, M. M. and Yao, X. and Murase, K.},
  title = {A constructive algorithm for training cooperative neural network
	ensembles},
  journal = {IEEE Transactions on Neural Networks},
  year = {2003},
  volume = {14},
  pages = {820--834},
  number = {4},
  file = {islam03constructive.pdf:islam03constructive.pdf:PDF}
}

@MISC{ist_homepage,
  author = {{IST}},
  title = {{I}nformation {S}ociety {T}echnologies -- Website},
  howpublished = {\url{http://cordis.europa.eu/ist}},
  year = {2009}
}

@MISC{itrs2007pids,
  author = {{ITRS}},
  title = {International Technology Roadmap for Semiconductors},
  howpublished = {\url{http://www.itrs.net/Links/2007ITRS/2007\_Chapters/2007\_PIDS.pdf}},
  year = {2007},
  owner = {jo},
  timestamp = {2009.10.26}
}

@ARTICLE{izhikevich2007solving,
  author = {Izhikevich, E.M.},
  title = {Solving the distal reward problem through linkage of STDP and dopamine
	signaling},
  journal = {Cerebral Cortex},
  year = {2007},
  volume = {17},
  pages = {2443--2452},
  number = {10},
  owner = {simon},
  publisher = {Oxford Univ Press},
  timestamp = {2013.01.05}
}

@ARTICLE{Izhikevich2007,
  author = {Izhikevich, Eugene M.},
  title = {Solving the Distal Reward Problem through Linkage of STDP and Dopamine
	Signaling},
  journal = {Cerebral Cortex},
  year = {2007},
  volume = {17},
  pages = {2443-2452},
  number = {10},
  abstract = {In Pavlovian and instrumental conditioning, reward typically comes
	seconds after reward-triggering actions, creating an explanatory
	conundrum known as “distal reward problem”: How does the brain know
	what firing patterns of what neurons are responsible for the reward
	if 1) the patterns are no longer there when the reward arrives and
	2) all neurons and synapses are active during the waiting period
	to the reward? Here, we show how the conundrum is resolved by a model
	network of cortical spiking neurons with spike-timing–dependent plasticity
	(STDP) modulated by dopamine (DA). Although STDP is triggered by
	nearly coincident firing patterns on a millisecond timescale, slow
	kinetics of subsequent synaptic plasticity is sensitive to changes
	in the extracellular DA concentration during the critical period
	of a few seconds. Random firings during the waiting period to the
	reward do not affect STDP and hence make the network insensitive
	to the ongoing activity—the key feature that distinguishes our approach
	from previous theoretical studies, which implicitly assume that the
	network be quiet during the waiting period or that the patterns be
	preserved until the reward arrives. This study emphasizes the importance
	of precise firing patterns in brain dynamics and suggests how a global
	diffusive reinforcement signal in the form of extracellular DA can
	selectively influence the right synapses at the right time.},
  doi = {10.1093/cercor/bhl152},
  eprint = {http://cercor.oxfordjournals.org/content/17/10/2443.full.pdf+html},
  owner = {simon},
  timestamp = {2012.12.14},
  url = {http://cercor.oxfordjournals.org/content/17/10/2443.abstract}
 }

@ARTICLE{izhikevich03spikes,
  author = {Izhikevich, Eugene M.},
  title = {{Simple Model of Spiking Neurons}},
  journal = {IEEE Transactions on Neural Networks},
  year = {2003},
  volume = {14},
  pages = {1569-1572},
  abstract = {We present a model that reproduces spiking and bursting behavior of
	known types of cortical neurons. The model combines the biologically
	plausibility of Hodgkin-Huxley-type dynamics and the computational
	efficiency of integrate-and-fire neurons. Using this model, one can
	simulate tens of thousands of spiking cortical neurons in real time
	(1 ms resolution) using a desktop PC. },
  eprint = {http://www.izhikevich.org/publications/spkes.pdf},
  keywords = {Bursting, cortex, Hodgkin–Huxley, PCNN, quadratic in- tegrate-and-fire,
	spiking, thalamus },
  url = {http://www.izhikevich.org/publications/spikes.htm}
}

@ARTICLE{izhikevich04whichmod,
  author = {Izhikevich, Eugene M.},
  title = {{Which Model to Use for Cortical Spiking Neurons?}},
  journal = {IEEE Transactions on Neural Networks},
  year = {2004},
  volume = {15},
  pages = {1063-1070},
  abstract = {We discuss the biological plausibility and computational efficiency
	of some of the most useful models of spiking and bursting neurons.
	We compare their applicability to large-scale simulations of cortical
	neural networks.},
  eprint = {http://www.izhikevich.org/publications/whichmod.pdf},
  keywords = {Terms—Chaos, Hodgkin–Huxley, pulse-coupled neural network (PCNN),
	quadratic integrate-and-fire (I&F), spike-timing },
  url = {http://www.izhikevich.org/publications/whichmod.htm}
}

@ARTICLE{izhikevich05polychronization,
  author = {Izhikevich, Eugene M.},
  title = {{Polychronization: Computation with Spikes}},
  journal = {Neural Comp.},
  year = {2005},
  volume = {18},
  pages = {245-282},
  number = {2},
  abstract = {We present a minimal spiking network that can polychronize, that is,
	exhibit reproducible time-locked but not synchronous firing patterns
	with millisecond precision, as in synfire braids. The network consists
	of cortical spiking neurons with axonal conduction delays and spike-timing-dependent
	plasticity (STDP); a ready-to-use MATLAB code is included. It exhibits
	sleeplike oscillations, gamma (40 Hz) rhythms, conversion of firing
	rates to spike timings, and other interesting regimes. Due to the
	interplay between the delays and STDP, the spiking neurons spontaneously
	self-organize into groups and generate patterns of stereotypical
	polychronous activity. To our surprise, the number of coexisting
	polychronous groups far exceeds the number of neurons in the network,
	resulting in an unprecedented memory capacity of the system. We speculate
	on the significance of polychrony to the theory of neuronal group
	selection (TNGS, neural Darwinism), cognitive neural computations,
	binding and gamma rhythm, mechanisms of attention, and consciousness
	as "attention to memories."},
  eprint = {http://neco.mitpress.org/cgi/reprint/18/2/245.pdf},
  keywords = {spiking plasticity},
  url = {http://neco.mitpress.org/cgi/content/abstract/18/2/245}
}

@BOOK{izhikevich2007dynamicalsystems,
  title = {Dynamical Systems in Neuroscience},
  publisher = {MIT Press, Cambridge},
  year = {2007},
  author = {Izhikevich, Eugene M.},
}

@MISC{matlab,
  author = {{T}he {M}athworks {I}nc.},
  title = {{MATLAB}},
  howpublished = {version 6, release 12.1},
  address = {3 Apple Hill Drive, Natick, MA, USA},
  groupsearch = {0},
  key = {matlab},
  keywords = {spec}
}

@BOOK{jaehne05imageprocessing,
  title = {Digital Image Processing (6th ed.)},
  publisher = {Springer Verlag Berlin, Heidelberg, New York},
  year = {2005},
  author = {B. J\a"{a}hne},
  owner = {fieres}
}

@ARTICLE{jabri92perturbation,
  author = {Jabri, M. and Flower, B.},
  title = {Weight perturbation: {A}n optimal architecture and learning technique
	for analog {VLSI} feedforward and recurrent multilayer networks},
  journal = {IEEE Transactions on Neural Networks},
  year = {1992},
  volume = {3},
  pages = {154--157},
  number = {1},
  file = {jabri92perturbation.pdf:jabri92perturbation.pdf:PDF}
}

@ARTICLE{jacobs88adaption,
  author = {Jacobs, R.A.},
  title = {Increased rates of convergence through learning rate adaption},
  journal = {Neural Networks},
  year = {1988},
  volume = {1},
  pages = {295-307}
}

@ARTICLE{jacobs91decomposition,
  author = {Jacobs, R. A. and Jordan, M. I. and Barto, A. G.},
  title = {Task decomposition through competition in a modular connectionist
	architecture: {T}he what and where vision tasks},
  journal = {Cognitive Science},
  year = {1991},
  volume = {15},
  pages = {219--250},
  file = {jacobs91decomposition.pdf:jacobs91decomposition.pdf:PDF}
}

@ARTICLE{jacobs91mixtures,
  author = {Jacobs, R. A. and Jordan, M. I. and Nowlan, S. J. and Hinton, G.
	E.},
  title = {Adaptive mixtures of local experts},
  journal = {Neural Computation},
  year = {1991},
  volume = {3},
  pages = {79--87},
  file = {jacobs91mixtures.pdf:jacobs91mixtures.pdf:PDF}
}

@ARTICLE{jacobson1995,
  author = {Jacobson, V.},
  title = {Congestion avoidance and control},
  journal = {SIGCOMM Comput. Commun. Rev.},
  year = {1995},
  volume = {25},
  pages = {157--187},
  number = {1},
  address = {New York, NY, USA},
  issn = {0146-4833},
  publisher = {ACM}
}

@MISC{jacobson1988,
  author = {Van Jacobson and Michael J. Karels},
  title = {Congestion avoidance and control},
  year = {1988}
}

@TECHREPORT{jaeger01echo,
  author = {H. Jaeger},
  title = {The ``echo state'' approach to analysing and training recurrent neural
	networks},
  institution = {German National Research Center for Information Technology},
  year = {2001},
  number = {GMD Report 148},
  file = {jaeger01echo.pdf:jaeger01echo.pdf:PDF},
  keywords = {lq liquid},
  url = {ftp://borneo.gmd.de/pub/indy/publications_herbert/EchoStatesTechRep.pdf}
}

@ARTICLE{jaeger07optimization,
  author = {Jaeger, Herbert and Lukosevicius, Mantas and Popovici, Dan and Siewert,
	Udo},
  title = {Optimization and applications of echo state networks with leaky-
	integrator neurons},
  journal = {Neural Networks},
  year = {2007},
  volume = {20},
  pages = {335--352},
  number = {3},
  month = apr,
  abstract = {Standard echo state networks (ESNs) are built from simple additive
	units with a sigmoid activation function. Here we investigate ESNs
	whose reservoir units are leaky integrator units. Units of this type
	have individual state dynamics, which can be exploited in various
	ways to accommodate the network to the temporal characteristics of
	a learning task. We present stability conditions, introduce and investigate
	a stochastic gradient descent method for the optimization of the
	global learning parameters (input and output feedback scalings, leaking
	rate, spectral radius) and demonstrate the usefulness of leaky-integrator
	ESNs for (i) learning very slow dynamic systems and replaying the
	learnt system at different speeds, (ii) classifying relatively slow
	and noisy time series (the Japanese Vowel dataset -- here we obtain
	a zero test error rate), and (iii) recognizing strongly time-warped
	dynamic patterns.},
  booktitle = {Echo State Networks and Liquid State Machines},
  keywords = {Recurrent neural networks, Pattern generation, Speaker classification,,
	liquid},
  owner = {mreuss},
  timestamp = {2007.06.14},
  url = {http://www.sciencedirect.com/science/article/B6T08-4NMWR88-9/2/213836000d0950215e4d665e6070472b}
}

@ARTICLE{jaeger07specialissue,
  author = {Jaeger, Herbert and Maass, Wolfgang and Principe, Jose},
  title = {Special issue on echo state networks and liquid state machines},
  journal = {Neural Networks},
  year = {2007},
  volume = {20},
  pages = {287--289},
  number = {3},
  month = apr,
  booktitle = {Echo State Networks and Liquid State Machines},
  keywords = {liquid},
  owner = {mreuss},
  timestamp = {2007.06.14}
}

@inproceedings{jaitly2011learning,
  title={Learning a better representation of speech soundwaves using restricted Boltzmann machines},
  author={Jaitly, Navdeep and Hinton, Geoffrey},
  booktitle={Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on},
  pages={5884--5887},
  year={2011},
  organization={IEEE}
}

@BOOK{jang97neurofuzzy,
  title = {Neuro-Fuzzy and Soft Computing},
  publisher = {Prentice-Hall},
  year = {1997},
  author = {J.-S.\ R.\ Jang and C.\-T.\ Sun and E.\ Mizutani},
  keywords = {learning},
  owner = {fieres}
}

@INPROCEEDINGS{jenkins2015continous,
author = {Armenise, Valentina},
title = {Continuous Delivery with Jenkins: Jenkins Solutions to Implement Continuous Delivery},
year = {2015},
publisher = {IEEE Press},
booktitle = {Proceedings of the Third International Workshop on Release Engineering},
pages = {24–27},
numpages = {4},
location = {Florence, Italy},
series = {RELENG '15}
}

@inproceedings{ji2016neutrams,
  title={NEUTRAMS: Neural network transformation and co-design under neuromorphic hardware constraints},
  author={Ji, Yu and Zhang, YouHui and Li, ShuangChen and Chi, Ping and Jiang, CiHang and Qu, Peng and Xie, Yuan and Chen, WenGuang},
  booktitle={2016 49th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)},
  pages={1--13},
  year=2016,
  organization={IEEE}
}

@INPROCEEDINGS{jimenez_ijcnn98dynamically,
  author = {Jimenez, D. and Walsh, N.},
  title = {Dynamically weighted ensemble neural networks for classification},
  booktitle = {Proceedings of the 1998 International Joint Conference on Neural
	Networks ({IJCNN})},
  year = {1998}
}

@ARTICLE{jin02vsd,
  author = {Jin, W. and Zhang, R. J. and Wu, J. Y. },
  title = {Voltage-sensitive dye imaging of population neuronal activity in
	cortical tissue.},
  journal = {Journal of neuroscience methods},
  year = {2002},
  volume = {115},
  pages = {13--27},
  number = {1},
  month = {March},
  abstract = {Voltage-sensitive dyes (VSDs) and optical imaging are useful for studying
	spatiotemporal patterns of population neuronal activity in cortical
	tissue. Using a photodiode array and absorption dyes we were able
	to detect neuronal activity in single trials before it could be detected
	by local field potential (LFP) recordings. Simultaneous electrical
	and optical recordings from the same tissue also showed that VSD
	and LFP signals have different waveforms during different activities,
	suggesting that they are sensitive to different aspects of the synchronization
	across the population. Noise, dye bleaching, phototoxicity and optical
	filter selection are important to the quality of the VSD signal and
	are discussed in this report. With optimized signal-to-noise ratio
	(S/N) and total recording time, we can optically monitor approximately
	500 locations in an area of 1 mm(2) of cortical tissue with a sensitivity
	comparable to that of LFP electrodes. The total recording time and
	S/N of fluorescence and absorption dyes are also compared. At S/N
	of 8-10, absorption dye NK3630 allows a total recording time of 15-30
	min, which can be divided into hundreds of 4-8 s recording trials
	over several hours, long enough for many kinds of experiments. In
	conclusion, the VSD method provides a reliable way for examining
	neuronal activity and pharmacological properties of synapses in brain
	slices.},
  keywords = {optical\_imaging, spatiotemporal\_patterns, voltage\_sensitive\_imaging}
}

@INPROCEEDINGS{joachims_ecml98textsvm,
  author = {Joachims, T.},
  title = {Text categorization with support vector machines: learning with many
	relevant features},
  booktitle = {Proceedings of {ECML}-98, 10th European Conference on Machine Learning},
  year = {1998},
  editor = {Claire N{\'{e}}dellec and C{\'{e}}line Rouveirol},
  pages = {137--142},
  address = {Chemnitz, DE},
  publisher = {Springer Verlag, Heidelberg, DE},
  file = {joachims_ecml98textsvm.pdf:joachims_ecml98textsvm.pdf:PDF},
  url = {citeseer.ist.psu.edu/article/joachims98text.html}
}

@ARTICLE{johannson07towards,
  author = {Christopher Johansson and Anders Lansner},
  title = {Towards cortex sized artificial neural systems.},
  journal = {Neural Networks},
  year = {2007},
  volume = {20},
  pages = {48-61},
  number = {1},
  abstract = { We propose, implement, and discuss an abstract model of the mammalian
	neocortex. This model is instantiated with a sparse recurrently connected
	neural network that has spiking leaky integrator units and continuous
	Hebbian learning. First we study the structure, modularization, and
	size of neocortex, and then we describe a generic computational model
	of the cortical circuitry. A characterizing feature of the model
	is that it is based on the modularization of neocortex into hypercolumns
	and minicolumns. Both a floating- and fixed-point arithmetic implementation
	of the model are presented along with simulation results. We conclude
	that an implementation on a cluster computer is not communication
	but computation bounded. A mouse and rat cortex sized version of
	our model executes in 44\% and 23\% of real-time respectively. Further,
	an instance of the model with 1.6 � 106 units and 2 � 1011 connections
	performed noise reduction and pattern completion. These implementations
	represent the current frontier of large-scale abstract neural network
	simulations in terms of network size and running speed. }
}

@ARTICLE{johansson06attractor,
  author = {Johansson, C and Rehn, M and Lansner, A},
  title = {Attractor neural networks with patchy connectivity},
  journal = {Neurocomputing},
  year = {2006},
  volume = {69},
  pages = {627-633},
  number = {7-9},
  month = {Jan},
  abstract = {The neurons in the mammalian visual cortex are arranged in columnar
	structures, and the synaptic contacts of the pyramidal neurons in
	layer II/III are clustered into patches that are sparsely distributed
	over the surrounding cortical surface. Here, we use an attractor
	neural-network model of the cortical circuitry and investigate the
	effects of patchy connectivity, both on the properties of the network
	and the attractor dynamics. An analysis of the network shows that
	the signal-to-noise ratio of the synaptic potential sums are improved
	by the patchy connectivity, which results in a higher storage capacity.
	This analysis is performed for both the Hopfield and Willshaw learning
	rules and the results are confirmed by simulation experiments.},
  keywords = {Attractor neural network; Patchy connectivity; Clustered connections;
	Neocortex; Small world network; Hypercolumn}
}

@INPROCEEDINGS{johansson02,
  author = {Christopher Johansson and Anders S and Anders Lansner},
  title = {A neural network with hypercolumns},
  booktitle = {In LNCS: Vol. 2415. Proceedings of the international conference on
	artificial neural networks},
  year = {2002}
}

@BOOK{johnsanalog,
  title = {Analog integrated Circuit },
  publisher = {John Wiley and Sons, Inc},
  year = {1997},
  author = {David Johns and Ken Martin},
  isbn = {0-471-14448-7}
}

@article{joshi2020accurate,
  author = {Vinay Joshi and Manuel Le Gallo and Simon Haefeli and Irem Boybat and S. R. Nandakumar and Christophe Piveteau and Martino Dazzi and Bipin Rajendran and Abu Sebastian and Evangelos Eleftheriou},
  title = {Accurate deep neural network inference using computational phase-change memory},
  publisher = {Springer Science and Business Media {LLC}},
  journal = {Nature Communications},
  volume = {11},
  number = {1},
  month = may,
  year = 2020,
  doi = {10.1038/s41467-020-16108-9}
}

@ARTICLE{jolivet08benchmark,
  author = {Renaud Jolivet and Ryota Kobayashi and Alexander Rauch and Richard
	Naud and Shigeru Shinomoto and Wulfram Gerstner},
  title = {A benchmark test for a quantitative assessment of simple neuron models},
  journal = {Journal of Neuroscience Methods},
  year = {2008},
  volume = {169},
  pages = {417 -- 424},
  number = {2},
  abstract = { Several methods and algorithms have recently been proposed that allow
	for the systematic evaluation of simple neuron models from intracellular
	or extracellular recordings. Models built in this way generate good
	quantitative predictions of the future activity of neurons under
	temporally structured current injection. It is, however, difficult
	to compare the advantages of various models and algorithms since
	each model is designed for a different set of data. Here, we report
	about one of the first attempts to establish a benchmark test that
	permits a systematic comparison of methods and performances in predicting
	the activity of rat cortical pyramidal neurons. We present early
	submissions to the benchmark test and discuss implications for the
	design of future tests and simple neurons models. },
  issn = {0165-0270}
}

@MISC{Scipy2001,
  author = {Eric Jones and Travis Oliphant and Pearu Peterson},
  title = {{SciPy}: Open source scientific tools for {Python}},
  year = {2001},
  url = {http://www.scipy.org/}
}

@ARTICLE{jones92lemma,
  author = {Jones, L. K.},
  title = {A simple lemma on greedy approximation in Hilbert space and convergence
	rates for projection pursuit regression and neural network training},
  journal = {Annals of Statistics},
  year = {1992},
  volume = {20},
  pages = {608--613},
  number = {1},
  file = {jones92lemma.pdf:jones92lemma.pdf:PDF}
}

@ARTICLE{jones2014, crossref = {jones2014computer} }
@ARTICLE{jones2014computer,
  author = {Jones, N.},
  title = {Computer science: The learning machines},
  journal = {Nature},
  year = {2014},
  volume = {505},
  pages = {146--148},
  doi = {10.1038/505146}
}

@techreport{jordan2014neural,
  title={Neural Networks as Sources of uncorrelated Noise for functional neural Systems},
  author={Jordan, Jakob and Bytschok, Ilya and Tetzlaff, Tom and Pfeil, Thomas and Breitwieser, Oliver and Bill, Johannes and Diesmann, Markus and Gruebl, Andreas and Schemmel, Johannes and Petrovici, Mihai and others},
  year={2014},
  institution={Computational and Systems Neuroscience}
}

@article{jordan2018extremely,
  title={Extremely Scalable Spiking Neuronal Network Simulation Code: From Laptops to Exascale Computers},
  author={Jordan, Jakob and Ippen, Tammo and Helias, Moritz and Kitayama, Itaru and Sato, Mitsuhisa and Igarashi, Jun and Diesmann, Markus and Kunkel, Susanne},
  journal={Frontiers in Neuroinformatics},
  pages=2,
  year=2018,
  publisher={Frontiers},
  volume=12,
  url={https://www.frontiersin.org/article/10.3389/fninf.2018.00002},
  doi={10.3389/fninf.2018.00002},
  issn={1662--5196},
}

@ARTICLE{joshi07decisions,
  author = {Joshi, Prashant},
  title = {From memory-based decisions to decision-based movements: A model
	of interval discrimination followed by action selection},
  journal = {Neural Networks},
  year = {2007},
  volume = {20},
  pages = {298--311},
  number = {3},
  month = apr,
  abstract = {The interval discrimination task is a classical experimental paradigm
	that is employed to study working memory and decision making and
	typically involves four phases. First, the subject receives a stimulus,
	then holds it in the working memory, then makes a decision by comparing
	it with another stimulus and finally acts on this decision, usually
	by pressing one of the two buttons corresponding to the binary decision.
	This article demonstrates that simple linear readouts from generic
	neural microcircuits that send feedback of their activity to the
	circuit, can be trained using identical learning mechanisms to perform
	quite separate tasks of decision making and generation of subsequent
	motor commands. In this sense, the neurocomputational algorithm presented
	here is able to integrate the four computational stages into a single
	unified framework. The algorithm is tested using two-interval discrimination
	and delayed-match-to-sample experimental paradigms as benchmarks.},
  booktitle = {Echo State Networks and Liquid State Machines},
  keywords = {Working memory, Decision making, Action selection, Tuned feedback,
	liquid},
  owner = {mreuss},
  timestamp = {2007.06.14},
  url = {http://www.sciencedirect.com/science/article/B6T08-4NMWR88-1/2/2f77cdf64ce372185ada7c43fae07975}
}

@INPROCEEDINGS{jouppi2017datacenter,
  title={In-datacenter performance analysis of a tensor processing unit},
  author={Jouppi, Norman P and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and others},
  booktitle={Proceedings of the 44th Annual International Symposium on Computer Architecture},
  pages={1--12},
  year={2017}
}

@ARTICLE{jung79eeg,
  author = {Jung, R. and Berger, W. and Berger, H. },
  title = {{F}iftieth anniversary of {H}ans {B}erger's publication of the electroencephalogram.
	{H}is first records in 1924--1931 (author's transl)},
  journal = {Arch Psychiatr Nervenkr},
  year = {1979},
  volume = {227},
  pages = {279--300},
  month = {Dec},
  abstract = { For the fiftieth anniversary of Berger's first EEG publication, some
	of his early recordings obtained between 1924 and 1931 are discussed
	and illustrated. Examples of his protocols from the Freiburg Berger
	Archives are reproduced. Three types of Berger's early investigations
	are described: (1) String-galvanometer recordings obtained between
	1924 and 1926, mainly from trephined patients with cerebral diseases,
	which usually showed brain waves slowed to 6--8 per second; (2) Direct
	recordings from the cortex and white matter proving the cortical
	origin of the EEG in 1930; (3) Typical unpublished EEG recordings
	of epileptics and of petit-mal attacks obtained in 1930 and 1931.
	Berger's first six papers published between 1929 and 1933 described
	nearly all the main EEG findings of cerebral diseases and the EEG
	alterations of normals during attention, sleep, and narcosis, but
	they did not report on convulsive potentials in the EEGs of epileptics.
	Berger had, however, obtained excellent records of epileptic EEG
	features, here depicted in Figs. 4 through 7. These remained unpublished
	until 1933 and 1938, because Berger suspected that they contained
	artifacts caused by blinks and facial movements which he had recorded
	in his controls (Fig. 4). Only in 1933, after other authors had described
	large amplitudes of convulsive potentials in the cortex of animals,
	did Berger publish parts of the EEGs of a petit-mal attack and of
	focal attacks in progressive paresis. In 1938, Berger presented the
	EEG of the beginning of a petit-mal attack with large 3/s spikes
	and waves recorded in 1931 which were similar to those described
	by Gibbs and coworkers in 1935. In 1933 and 1938, Berger interpreted
	the abnormal brain potentials of epileptics as signs of a preconvulsive
	state of the forebrain and suggested that the periods of 3/s waves
	were cortical correlates of an epileptic absence. }
}

@MANUAL{jungo_company,
  title = {{WinDriver}},
  author = {{Jungo Ltd}},
  address = {1 Hamachshev Street, P.O.Box 8493, Netanya 42504, Israel},
  year = {2007},
  annote = {our device drivers},
  groupsearch = {0},
  keywords = {spec}
}

@MISC{kalarickal98comparison,
  author = {George J. Kalarickal and Jonathan A. Marshall},
  title = {Comparison of Generalized Hebbian Rules for Long-Term Synaptic Plasticity},
  abstract = {A large variety of synaptic plasticity rules have been used in models
	of excitatory synaptic plasticity (Brown et al., 1990). These rules
	are generalizations of the Hebbian rule and have some properties
	consistent with experimental data on long-term excitatory synaptic
	plasticity, but they also have some properties inconsistent with
	experimental data. For example, the BCM rule (Bear et al., 1987;
	Bienenstock et al., 1982) produces homosynaptic potentiation and
	depression, which has been observed experimentally (Artola et al.,
	1990; Dudek & Bear, 1992; Kirkwood et al., 1993; Fr'egnac et al.,
	1994; Yang & Faber, 1991). But the BCM rule is also inconsistent
	with some experimental results; e.g., the BCM rule cannot produce
	heterosynaptic depression (Abraham & Goddard, 1983; Lynch et al.,
	1977). In addition, long-term synaptic plasticity in inhibitory pathways
	has been emphasized in some models of cortical function (Marshall,
	1990abc, 1995a; Sirosh et al., 1996), but experimental data on in...},
  citeseerurl = {citeseer.nj.nec.com/kalarickal98comparison.html},
  keywords = {spiking learning}
}

@MISC{kalarickal98role,
  author = {George J. Kalarickal and Jonathan A. Marshall},
  title = {The Role of Afferent Excitatory and Lateral Inhibitory Synaptic Plasticity
	in Visual Cortical Ocular Dominance Plasticity},
  citeseerurl = {citeseer.nj.nec.com/kalarickal98role.html},
  keywords = {spiking learning}
}

@ARTICLE{kampa04kinetics,
  author = {Björn M. Kampa and John Clements and Peter Jonas and Greg J. Stuart},
  title = {Kinetics of Mg2+ unblock of NMDA receptors: implications for spike-timing
	dependent synaptic plasticity},
  journal = {J. Physiol.},
  year = {2004},
  volume = {556},
  pages = {337-345},
  number = {2},
  file = {kampa04kinetics.pdf:kampa04kinetics.pdf:PDF},
  keywords = {spiking plasticity},
  owner = {mreuss}
}

@ARTICLE{kampa04cortical,
  author = {B. M. Kampa and J. J. Letzkus and G. J. Stuart},
  title = {Cortical feed-forward networks for binding different streams of sensory
	information},
  journal = {Nature Neuroscience},
  year = {2006},
  volume = {9},
  pages = {1472-1473},
  number = {12}
}

@BOOK{kandal2000,
  title = {Principles of Neural Science},
  publisher = {McGraw-Hill},
  year = {2000},
  author = {E. R. Kandel and J. H. Schwartz and T. M. Jessell},
  address = {New York},
  edition = {4},
  owner = {mueller},
  timestamp = {2008.09.07}
}

@article{kanerva2009hyperdimensional,
  title={Hyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors},
  author={Kanerva, Pentti},
  journal={Cognitive Computation},
  volume={1},
  number={2},
  pages={139--159},
  year={2009},
  publisher={Springer}
}

@INPROCEEDINGS{kaplan_ijcnn2009,
  author = {Kaplan, Bernhard and Br\"uderle, Daniel and Schemmel, Johannes and
	Meier, Karlheinz},
  title = {High-Conductance States on a Neuromorphic Hardware System},
  booktitle = {Proceedings of the 2009 International Joint Conference on Neural
	Networks (IJCNN)},
  year = {2009},
  keywords = {neuromorphic}
}

@article{kappel2014stdp,
  title={STDP installs in winner-take-all circuits an online approximation to hidden markov model learning},
  author={Kappel, David and Nessler, Bernhard and Maass, Wolfgang},
  journal={PLoS computational biology},
  volume={10},
  number={3},
  pages={e1003511},
  year={2014},
  publisher={Public Library of Science}
}

@article{kappel2015network,
  title={Network Plasticity as Bayesian Inference},
  author={Kappel, David and Habenschuss, Stefan and Legenstein, Robert and Maass, Wolfgang},
  journal={arXiv preprint},
  url = {http://arxiv.org/abs/1504.05143},
  year=2015
}

@article{kasabov2013dynamic,
  title={Dynamic evolving spiking neural networks for on-line spatio-and spectro-temporal pattern recognition},
  author={Kasabov, Nikola and Dhoble, Kshitij and Nuntalid, Nuttapod and Indiveri, Giacomo},
  journal={Neural Networks},
  volume={41},
  pages={188--201},
  year={2013},
  publisher={Elsevier}
}

@ARTICLE{kaski94,
  author = {Samuel Kaski and Teuvo Kohonen},
  title = {Winner-take-all networks for physiological models of competitive
	learning},
  journal = {Neural Networks},
  year = {1994},
  volume = {7},
  pages = {973-984},
  number = {6-7},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  ee = {http://dx.doi.org/10.1016/S0893-6080(05)80154-6}
}

@BOOK{Katz1969,
  title = {The release of neural transmitter substances},
  publisher = {Liverpool University Press},
  year = {1969},
  author = {Katz, Bernard},
  volume = {10},
  owner = {simon},
  timestamp = {2013.05.03}
}

@masterthesis{katz2018low,
  title = {A low cost modular actuator for dynamic robots},
  author = {Katz, Benjamin G.},
  year = 2018,
  school = {Massachusetts Institute of Technology},
  type = {Masters Thesis}
}

@ARTICLE{kepecs02stdp,
  author = {Adam Kepecs and Mark C.W. van Rossum and Sen Song and Jesper Tegner},
  title = {Spike-timing-dependent plasticity: common themes and divergent vistas},
  journal = {Biol. Cybern.},
  year = {2002},
  volume = {87},
  pages = {446 - 458},
  number = {5-6},
  month = {December},
  abstract = {Recent experimental observations of spike-timing-dependent synaptic
	plasticity (STDP) have revitalized the study of synaptic learning
	rules. The most surprising aspect of these experiments lies in the
	observation that synapses activated shortly after the occurrence
	of a postsynaptic spike are weakened. Thus, synaptic plasticity is
	sensitive to the temporal ordering of pre- and postsynaptic activation.
	This temporal asymmetry has been suggested to underlie a range of
	learning tasks. In the first part of this review we highlight some
	of the common themes from a range of findings in the framework of
	predictive coding. As an example of how this principle can be used
	in a learning task, we discuss a recent model of cortical map formation.
	In the second part of the review, we point out some of the differences
	in STDP models and their functional consequences. We discuss how
	differences in the weight-dependence, the time-constants and the
	non-linear properties of learning rules give rise to distinct computational
	functions. In light of these computational issues raised, we review
	current experimental findings and suggest further experiments to
	resolve some controversies.},
  file = {kepecs02stdp.pdf:kepecs02stdp.pdf:PDF},
  keywords = {plasticity},
  owner = {mreuss},
  url = {http://springerlink.metapress.com/link.asp?id=qh0ju6hc19mm070m}
}

@BOOK{kernighan78clanguage,
  title = {The {C} Programming Language},
  publisher = {Prentice-Hall International Inc.},
  year = {1978},
  author = {Kernighan, B. W. and Ritchie, D. M.},
  address = {London},
  isbn = {0-13-110163-3}
}

@TECHREPORT{Kernighan1977,
  author = {Brian W. Kernighan and Dennis M. Ritchie},
  title = {The M4 Macro Processor},
  institution = {Bell Laboratories},
  year = {1977},
  address = {Murray Hill, New Jersey 07974},
  month = {July},
  owner = {simon},
  timestamp = {2013.02.19}
}

@ARTICLE{kerr2005,
  author = {Kerr, J. N. and Greenberg, D. and Helmchen, F.},
  title = {Imaging input and output of neocortical networks in vivo.},
  journal = {Proc Natl Acad Sci U S A},
  year = {2005},
  volume = {102},
  pages = {14063--14068},
  number = {39},
  month = {September},
  abstract = {Neural activity manifests itself as complex spatiotemporal activation
	patterns in cell populations. Even for local neural circuits, a comprehensive
	description of network activity has been impossible so far. Here
	we demonstrate that two-photon calcium imaging of bulk-labeled tissue
	permits dissection of local input and output activities in rat neocortex
	in vivo. Besides astroglial and neuronal calcium transients, we found
	spontaneous calcium signals in the neuropil that were tightly correlated
	to the electrocorticogram. This optical encephalogram (OEG) is shown
	to represent bulk calcium signals in axonal structures, thus providing
	a measure of local input activity. Simultaneously, output activity
	in local neuronal populations could be derived from action potential-evoked
	calcium transients with single-spike resolution. By using these OEG
	and spike activity measures, we characterized spontaneous activity
	during cortical Up states. We found that (i) spiking activity is
	sparse (<0.1 Hz); (ii) on average, only approximately 10\% of neurons
	are active during each Up state; (iii) this active subpopulation
	constantly changes with time; and (iv) spiking activity across the
	population is evenly distributed throughout the Up-state duration.
	Furthermore, the number of active neurons directly depended on the
	amplitude of the OEG, thus optically revealing an input-output function
	for the local network. We conclude that spontaneous activity in the
	neocortex is sparse and heterogeneously distributed in space and
	time across the neuronal population. The dissection of the various
	signal components in bulk-loaded tissue as demonstrated here will
	enable further studies of signal flow through cortical networks.},
  address = {Department of Cell Physiology, Max Planck Institute for Medical Research,
	Jahnstrasse 29, D-69120 Heidelberg, Germany.},
  citeulike-article-id = {350009},
  doi = {http://dx.doi.org/10.1073/pnas.0506029102},
  issn = {0027-8424},
  keywords = {cortical\_dynamics, to\_read},
  posted-at = {2005-10-13 19:46:26},
  priority = {2},
  url = {http://dx.doi.org/10.1073/pnas.0506029102}
}

@INPROCEEDINGS{keulen94neural,
  author = {E. van Keulen and S. Colak and H. Withagen and H. Hegt},
  title = {Neural network hardware performance criteria},
  booktitle = {Proceedings of the IEEE International Conference on Neural Networks
	1994},
  year = {1994},
  pages = {1885--1888},
  keywords = {vlsi, NN},
  owner = {fieres}
}

@INPROCEEDINGS{vankeulen_icnn94performance,
  author = {van Keulen, E. and Colak, S. and Withagen, H. and Hegt, H.},
  title = {Neural network hardware performance criteria},
  booktitle = {Proceedings of the IEEE International Conference on Neural Networks},
  year = {1994},
  pages = {1885--1888},
  month = {June}
}

@INPROCEEDINGS{keymeulen_gecco2000,
  author = {Didier Keymeulen and Gerhard Klimeck and Ricardo Zebulum and Adrian
	Stoica and Carlos Salazar-Lazaro},
  title = {EHWPack: a Parallel Software/Hardware Environment for Evolvable Hardware},
  booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference
	(GECCO-2000)},
  year = {2000},
  editor = {Darrell Whitley et. al.},
  pages = {538},
  address = {Las Vegas, Nevada, USA},
  month = {10-12 July},
  publisher = {San Francisco, CA 94104, USA},
  groupsearch = {0},
  isbn = {1-55860-708-0},
  keywords = {eh},
  notes = {A joint meeting of the ninth International Conference on Genetic Algorithms
	(ICGA-2000) and the fifth Annual Genetic Programming Conference (GP-2000)
	Part of whitley_2000_GECCO}
}

@INPROCEEDINGS{keymeulen_iirw00,
  author = {Keymeulen, Didier and Stoica, Adrian and Zebulum, Ricardo and Jin,
	Yili and Duong, Vu},
  title = {Fault-Tolerant Approaches based on Evolvable Hardware and Using a
	Reconfigurable Electronic Devices},
  booktitle = {Proc.\ of the IEEE Int.\ Integrated Reliability Workshop},
  year = {2000},
  pages = {32--39},
  address = {Lake Tahoe, CA, USA},
  month = Oct,
  publisher = {IEEE Press},
  annote = {XNOR intrinsic -> real chip evolution},
  groupsearch = {0},
  keywords = {eh}
}

@INPROCEEDINGS{Khan2008,
  author = {Khan, MM and Lester, DR and Plana, Luis A and Rast, A and Jin, X
	and Painkras, E and Furber, Stephen B},
  title = {SpiNNaker: mapping neural networks onto a massively-parallel chip
	multiprocessor},
  booktitle = {Neural Networks, 2008. IJCNN 2008.(IEEE World Congress on Computational
	Intelligence). IEEE International Joint Conference on},
  year = {2008},
  pages = {2849--2856},
  organization = {IEEE},
  owner = {simon},
  timestamp = {2013.04.15}
}

@article{kim2022moneta,
  author = {Daehyun Kim and Biswadeep Chakraborty and Xueyuan She and Edward Lee and Beomseok Kang and Saibal Mukhopadhyay},
  title = {{MONETA}: A Processing-In-Memory-Based Hardware Platform for the Hybrid Convolutional Spiking Neural Network With Online Learning},
  journal = {Frontiers in Neuroscience},
  publisher = {Frontiers Media {SA}},
  year = 2022,
  month = apr,
  volume = 16,
  doi = {10.3389/fnins.2022.775457}
}

@INPROCEEDINGS{kinget2007device,
  author = {Kinget, P.R.},
  title = {Device mismatch: an analog design perspective},
  booktitle = {Circuits and Systems, 2007. ISCAS 2007. IEEE International Symposium
	on},
  year = {2007},
  pages = {1245--1248},
  organization = {IEEE},
  owner = {simon},
  timestamp = {2013.02.08}
}

@ARTICLE{kirkpatrick83annealing,
  author = {Kirkpatrick, S. and Gelatt, C. D. and Vecchi, M. P.},
  title = {Optimization by Simulated Annealing},
  journal = {Science},
  year = {1983},
  volume = {220},
  pages = {671--680},
  number = {4598}
}

@ARTICLE{kistler97,
  author = {Kistler, Werner and Gerstner, Wulfram and van Hemmen, J. Leo},
  title = {Reduction of the {H}odgkin-{H}uxley Equations to a Single-Variable
	Threshold Model},
  journal = {Neural Computation},
  year = {1997},
  volume = {9},
  pages = {1015--1045}
}

@ARTICLE{kistler02phenomenological,
  author = {Kistler, Werner M.},
  title = {Spike-timing dependent synaptic plasticity: a phenomenological framework},
  journal = {Biol. Cybern.},
  year = {2002},
  volume = {87},
  pages = {416-427},
  number = {5-6},
  month = {December},
  abstract = { In this paper a phenomenological model of spike-timing dependent
	synaptic plasticity (STDP) is developed that is based on a Volterra
	series-like expansion. Synaptic weight changes as a function of the
	relative timing of pre- and postsynaptic spikes are described by
	integral kernels that can easily be inferred from experimental data.
	The resulting weight dynamics can be stated in terms of statistical
	properties of pre- and postsynaptic spike trains. Generalizations
	to neurons that fire two different types of action potentials, such
	as cerebellar Purkinje cells where synaptic plasticity depends on
	correlations in two distinct presynaptic fibers, are discussed. We
	show that synaptic plasticity, together with strictly local bounds
	for the weights, can result in synaptic competition that is required
	for any form of pattern formation. This is illustrated by a concrete
	example where a single neuron equipped with STDP can selectively
	strengthen those synapses with presynaptic neurons that reliably
	deliver precisely timed spikes at the expense of other synapses which
	transmit spikes with a broad temporal distribution. Such a mechanism
	may be of vital importance for any neuronal system where information
	is coded in the timing of individual action potentials.},
  file = {kistler02phenomenological.pdf:kistler02phenomenological.pdf:PDF},
  keywords = {plasticity spiking},
  owner = {mreuss},
  url = {http://www.springerlink.com/link.asp?id=h2kjbbnlt8c0dfrf}
}

@ARTICLE{kitano90graph,
  author = {Kitano, H.},
  title = {Designing neural networks using genetic algorithms with graph generation
	system},
  journal = {Complex Systems},
  year = {1990},
  volume = {4},
  pages = {461--476},
  number = {4}
}

@PHDTHESIS{kleider17phd,
  author = {Mitja Kleider},
  title = {Neuron Circuit Characterization in a Neuromorphic System},
  year = {2017},
  school = {Universit{\"a}t Heidelberg},
  note = {HD-KIP 17-135},
  url = {http://www.kip.uni-heidelberg.de/Veroeffentlichungen/details.php?id=3657}
}

@INPROCEEDINGS{klein2021addressing,
  author = {Klein, Bernhard and Kuhn, Lisa and Weis, Johannes and Emmel, Arne and Stradmann, Yannik and Schemmel, Johannes and Fr{\"o}ning, Holger},
  title = {Towards Addressing Noise and Static Variations of Analog Computations Using Efficient Retraining},
  booktitle = {Machine Learning and Principles and Practice of Knowledge Discovery in Databases},
  year = 2021,
  publisher = {Springer International Publishing},
  address = {Cham},
  pages = {409--420},
  isbn = {978-3-030-93736-2},
  doi = {10.1007/978-3-030-93736-2_32}
}

@article{knight2018gpus,
  title={{GPU}s outperform current {HPC} and neuromorphic solutions in terms of speed and energy when simulating a highly-connected cortical model},
  author={Knight, James C and Nowotny, Thomas},
  journal={Frontiers in neuroscience},
  volume=12,
  pages=941,
  year=2018,
  publisher={Frontiers}
}

@article{knight2021larger,
  title={Larger {GPU}-accelerated brain simulations with procedural connectivity},
  author={Knight, James C and Nowotny, Thomas},
  journal={Nature Computational Science},
  volume=1,
  number=2,
  pages={136--142},
  year=2021,
  publisher={Nature Publishing Group}
}

@article{knill1991apparent,
  title={Apparent surface curvature affects lightness perception},
  author={Knill, David C and Kersten, Daniel},
  journal={Nature},
  volume={351},
  number={6323},
  pages={228--230},
  year={1991}
}

@article{knill2004bayesian,
  title={The Bayesian brain: the role of uncertainty in neural coding and computation},
  author={Knill, David C and Pouget, Alexandre},
  journal={TRENDS in Neurosciences},
  volume={27},
  number={12},
  pages={712--719},
  year={2004},
  publisher={Elsevier}
}

@BOOK{knodel1998linder,
  title = {Linder Biologie},
  publisher = {Schroeder Verlag, Hannover},
  year = {1998},
  author = {Knodel, Hans}
}

@article{knott2006spine,
  title={Spine growth precedes synapse formation in the adult neocortex in vivo},
  author={Knott, Graham W and Holtmaat, Anthony and Wilbrecht, Linda and Welker, Egbert and Svoboda, Karel},
  journal={Nature Neuroscience},
  volume={9},
  number={9},
  pages={1117--1124},
  year={2006},
  publisher={Nature Publishing Group}
}

@INBOOK{knuth97binarysearch,
  chapter = {6.2.1},
  pages = {409--426},
  title = {Sorting and Searching},
  publisher = {Addison-Wesley},
  year = {1997},
  author = {Donald E. Knuth},
  volume = {3},
  series = {The Art of Computer Programming},
  type = {Section},
  address = {Reading, Massachusetts},
  edition = {Third}
}

@ARTICLE{kobayashi09,
  author = {Ryota Kobayashi and Yasuhiro Tsubo and Shigeru Shinomoto},
  title = {Made-to-order spiking neuron model equipped with a multi-timescale
	adaptive threshold},
  journal = {Frontiers in Computational Neuroscience},
  year = {2009},
  volume = {3},
  number = {0},
  abstract = {Information is transmitted in the brain through various kinds of neurons
	that respond differently to the same signal. Full characteristics
	including cognitive functions of the brain should ultimately be comprehended
	by building simulators capable of precisely mirroring spike responses
	of a variety of neurons. Neuronal modeling that had remained on a
	qualitative level has recently advanced to a quantitative level,
	but is still incapable of accurately predicting biological data and
	requires high computational cost. In this study, we devised a simple,
	fast computational model that can be tailored to any cortical neuron
	not only for reproducing but also for predicting a variety of spike
	responses to greatly fluctuating currents. The key features of this
	model are a multi-time scale adaptive threshold predictor and a nonresetting
	leaky integrator. This model is capable of reproducing a rich variety
	of neuronal spike responses, including regular spiking, intrinsic
	bursting, fast spiking, and chattering, by adjusting only three adaptive
	threshold parameters. This model can express a continuous variety
	of the firing characteristics in a three-dimensional parameter space
	rather than just those identified in the conventional discrete categorization.
	Both high flexibility and low computational cost would help to model
	the real brain function faithfully and examine how network properties
	may be influenced by the distributed characteristics of component
	neurons. Ryota Kobayashi and Yasuhiro Tsubo contributed equally to
	this work.},
  doi = {10.3389/neuro.10.009.2009},
  issn = {1662-5188}
}

@BOOK{koch99biophys,
  title = {Biophysics of Computation: Information Processing in Single Neurons},
  publisher = {Oxford University Press},
  year = {1999},
  author = {Koch, Christoph}
}

@ARTICLE{Koch2000Role,
  author = {Koch, C. and Segev, I.},
  title = {{The role of single neurons in information processing.}},
  journal = {Nat Neurosci},
  year = {2000},
  volume = {3 Suppl},
  pages = {1171--1177},
  month = nov,
  abstract = {{Neurons carry out the many operations that extract meaningful information
	from sensory receptor arrays at the organism's periphery and translate
	these into action, imagery and memory. Within today's dominant computational
	paradigm, these operations, involving synapses, membrane ionic channels
	and changes in membrane potential, are thought of as steps in an
	algorithm or as computations. The role of neurons in these computations
	has evolved conceptually from that of a simple integrator of synaptic
	inputs until a threshold is reached and an output pulse is initiated,
	to a much more sophisticated processor with mixed analog-digital
	logic and highly adaptive synaptic elements.}},
  address = {Computation and Neural Systems Program, Division of Biology, California
	Institute of Technology, Pasadena 91125, USA. koch@klab.caltech.edu},
  doi = {10.1038/81444},
  issn = {1097-6256},
  keywords = {neuroscience},
  pmid = {11127834},
  posted-at = {2011-07-28 12:18:02},
  priority = {0}
}

@ARTICLE{kock2009spiking,
    author = {Christiaan P.J. de Kock and Bert Sakmann},
    title = {Spiking in primary somatosensory cortex during natural whisking in awake head-restrained rats is cell-type specific},
    journal = {PNAS},
    volume = {38},
    year = {2009},
    pages = {16446--16450},
    doi = {10.1073/pnas.0904143106}
}

@INPROCEEDINGS{kohavi95study,
  author = {Kohavi, R.},
  title = {A Study of Cross-Validation and Bootstrap for Accuracy Estimation
	and Model Selection},
  booktitle = {The 1995 International Joint Conference on Artificial Intelligence
	{IJCAI}},
  year = {1995},
  pages = {1137--1145},
  address = {Montreal, Quebec, Canada},
  month = {August},
  file = {kohavi95study.pdf:kohavi95study.pdf:PDF}
}

@article{Ko2010481,
title = "Wafer-level bonding/stacking technology for 3D integration ",
journal = "Microelectronics Reliability ",
volume = "50",
number = "4",
pages = "481 - 488",
year = "2010",
note = "International Symposium on Reliability of Optoelectronics for Space / Advances in Wafer Level Packaging ",
issn = "0026-2714",
doi = "http://dx.doi.org/10.1016/j.microrel.2009.09.015",
url = "http://www.sciencedirect.com/science/article/pii/S0026271409003655",
author = "Cheng-Ta Ko and Kuan-Neng Chen",
abstract = "Enhanced transmission speeds, lower power consumption, better performance, and smaller form factors are reported as advantages in many devices and applications when using 3D integration. One core technique for performing 3D interconnection is stacked bonding. In this paper, wafer-level bonding technologies are reviewed and described in detail, including bonding materials and bonding conditions. The corresponding 3D integration technologies and platforms developed world-wide are also organized and addressed. "
}

@ARTICLE{kohonen82som,
  author = {Kohonen, Tuevo},
  title = {Self-organized formation of topologically correct feature maps},
  journal = {Biological Cybernetics},
  year = {1982},
  volume = {43},
  pages = {59--69}
}

@PHDTHESIS{Koke2017,
  author = {Christoph Koke},
  title = {Device Variability in Synapses of Neuromorphic Circuits},
  year = 2017,
  month = {February},
  url = {http://www.ub.uni-heidelberg.de/archiv/22742},
  school = {Ruprecht-Karls University Heidelberg}
}

@MISC{kononov13oral,
  author = {Alexander Kononov},
  title = {Personal Communication},
  year = {2013},
  owner = {simon},
  timestamp = {2013.04.17}
}

@MISC{kononov12oral,
  author = {Kononov, Alex},
  title = {personal communication},
  year = {2012},
  address = {Kirchhoff Institut f{\"u}r Physik, Universit{\"a}t Heidelberg, Deutschland}
}

@MASTERSTHESIS{kononov11diplomathesis,
  author = {Kononov, Alex},
  title = {Testing of an Analog Neuromorphic Network Chip},
  school = {Ruprecht-Karls-Universit{\"a}t Heidelberg},
  type = {Diploma thesis},
  year = {2011},
  note = {HD-KIP-11-83},
  keywords = {facets, floationg gate}
}

@INPROCEEDINGS{Koolwal2009,
  author = {Koolwal,Kushal},
  title = {Investigating latency effects of the Linux real-time Preemption Patches
	(PREEMPT RT) on AMD’s GEODE LX Platform},
  booktitle = {Eleventh Real-Time Linux Workshop},
  year = {2009},
  owner = {simon},
  timestamp = {2012.10.28}
}

@MISC{korcsak2015bachelorthesis,
  author = {TAgnes Korcsak-Gorzo},
  title = {Firing States of Recurrent Leaky Integrate-and-Fire Networks},
  month = {January},
  year = {2015},
  note = {Bachelor thesis}
}

@Article{kording2004bayesian,
  Author         = {K{\"o}rding, Konrad and Wolpert, Daniel},
  Title          = {Bayesian integration in sensorimotor learning},
  Journal        = {Nature},
  Volume         = {427},
  Number         = {6971},
  Pages          = {244--247},
  publisher      = {Nature Publishing Group},
  year           = 2004
}

@BOOK{koza99genetic,
  title = {Genetic Programming III - Darwinian Invention and Problem Solving},
  publisher = {Morgan Kaufmann Publishers},
  year = {1999},
  author = {Koza, J. R. and Bennet III, F. H. and Andre, D. and Keane, M. A.},
  address = {San Francisko, CA, USA},
  isbn = {1-55860-543-6}
}

@ARTICLE{kramer_ieeemicro96,
  author = {Kramer, A. H.},
  title = {Array-Based Analog Computation},
  journal = {IEEE Micro},
  year = {1996},
  volume = {16},
  pages = {20--29},
  number = {5},
  month = Oct,
  annote = {Summary/Overview, analog computing},
  groupsearch = {0},
  keywords = {vlsi}
}

@INPROCEEDINGS{rutenbar_dac99,
  author = {Krasnicki, Michael J and Phelps, Rodney and Rutenbar, Rob A and Carley,
	L Richard},
  title = {{MAELSTROM}: Efficient Simulation-Based Synthesis for Custom Analog
	Cells},
  booktitle = {Proceedings of the 1999 ACM/IEEE Design Automation Conference (DAC
	99)},
  year = {1999},
  pages = {945--950},
  address = {New Orleans, LA, USA},
  month = jun,
  groupsearch = {0},
  howpublished = {CD-ROM: ISBN: 1-58113-109-7}
}

@ARTICLE{kremkow2010gating,
  author = {Kremkow, Jens and Aertsen, Ad and Kumar, Arvind},
  title = {Gating of signal propagation in spiking neural networks by balanced
	and correlated excitation and inhibition.},
  journal = {The Journal of neuroscience},
  year = {2010},
  volume = {30},
  pages = {15760--15768},
  number = {47},
  month = nov,
  abstract = { Both ongoing and natural stimulus driven neuronal activity are dominated
	by transients. Selective gating of these transients is mandatory
	for proper brain function and may, in fact, form the basis of millisecond-fast
	decision making and action selection. Here we propose that neuronal
	networks may exploit timing differences between correlated excitation
	and inhibition (temporal gating) to control the propagation of spiking
	activity transients. When combined with excitation-inhibition balance,
	temporal gating constitutes a powerful mechanism to control the propagation
	of mixtures of transient and tonic neural activity components. },
  day = {24},
  doi = {10.1523/JNEUROSCI.3874-10.2010},
  issn = {1529-2401},
  keywords = {excitation, inhibition, propagation},
  pmid = {21106815},
  posted-at = {2010-11-29 17:37:53},
  priority = {2},
  url = {http://dx.doi.org/10.1523/JNEUROSCI.3874-10.2010}
}

@ARTICLE{kremkow2010functional,
  author = {Kremkow, J. and Perrinet, L.U. and Masson, G.S. and Aertsen, A.},
  title = {Functional consequences of correlated excitatory and inhibitory conductances
	in cortical networks.},
  journal = {J Comput Neurosci},
  year = {2010},
  volume = {28},
  pages = {579--594},
  abstract = {Neurons in the neocortex receive a large number of excitatory and
	inhibitory synaptic inputs. Excitation and inhibition dynamically
	balance each other, with inhibition lagging excitation by only few
	milliseconds. To characterize the functional consequences of such
	correlated excitation and inhibition, we studied models in which
	this correlation structure is induced by feedforward inhibition (FFI).
	Simple circuits show that an effective FFI changes the integrative
	behavior of neurons such that only synchronous inputs can elicit
	spikes, causing the responses to be sparse and precise. Further,
	effective FFI increases the selectivity for propagation of synchrony
	through a feedforward network, thereby increasing the stability to
	background activity. Last, we show that recurrent random networks
	with effective inhibition are more likely to exhibit dynamical network
	activity states as have been observed in vivo. Thus, when a feedforward
	signal path is embedded in such recurrent network, the stabilizing
	effect of effective inhibition creates an suitable substrate for
	signal propagation. In conclusion, correlated excitation and inhibition
	support the notion that synchronous spiking may be important for
	cortical processing.}
}

@INPROCEEDINGS{kristensen_ijcnn03eukariotic,
  author = {Kristensen, T. and Patel, R.},
  title = {Classification of Eukariotic and Prokariotic Cells by a Backpropagation
	Network},
  booktitle = {Proceedings of the International Joint Conference on Neural Networks},
  year = {2003},
  pages = {1718--1723},
  publisher = {IEEE Press},
  howpublished = {ISBN 0-7803-7899-7}
}

@ARTICLE{krogh95ensembles,
  author = {Krogh, A. and Vedelsby, J.},
  title = {Neural network ensembles, cross validation and active learning},
  journal = {Advances in Neural Information Processing Systems},
  year = {1995},
  volume = {7},
  pages = {299--314},
  address = {Cambridge, {MA}},
  editor = {Tesauro, G. and Touretzky, D. and Leen, T.},
  file = {krogh95ensembles.pdf:krogh95ensembles.pdf:PDF},
  publisher = {MIT Press}
}

@ARTICLE{kubota07biophysical,
  author = {Kubota, Shigeru and Kitajima, Tatsuo},
  title = {Biophysical model of spike-timing-dependent plasticity based on temporal
	factors of intracellular Ca2+},
  journal = {International Congress Series},
  year = {2007},
  volume = {1301},
  pages = {127--131},
  month = jul,
  abstract = {In this study, we propose a biophysical plasticity model that incorporates
	the temporal factors of spine Ca2+ signals and demonstrates that
	spike-timing-dependent plasticity (STDP) is attributable to the Ca2+
	entry through NMDA receptors (NMDARs). We also analyze the effects
	of the developmental change in NMDAR kinetics on the STDP curve.
	The results show that the change in NMDAR kinetics functions as a
	metaplasticity that contributes to potentiating immature synapses
	during early development.},
  booktitle = {Brain-Inspired IT III. Invited and selected papers of the 3rd International
	Conference on Brain-Inspired Information Technology "BrainIT 2006"
	held in Hibikino, Kitakyushu, Japan between 27 and 29 September 2006},
  keywords = {Spike-timing-dependent plasticity, Calcium, Metaplasticity, Neuroscience},
  owner = {mreuss},
  timestamp = {2007.06.14},
  url = {http://www.sciencedirect.com/science/article/B7581-4NYJD61-13/2/6fef5b17443b5486a72583d308217311}
}

@mastersthesis{kugele2018csp,
  author   = {Alexander Kugele},
  title    = {Solving the Constraint Satisfaction Problem Sudoku on Neuromorphic Hardware},
  school   = {Ruprecht-Karls-Universit{\"a}t Heidelberg},
  year     = {2018},
  type     = {Master's Thesis}
}

@ARTICLE{kumar08conditions,
  author = {Kumar, Arvind and Rotter, Stefan and Aertsen, Ad},
  title = {Conditions for propagating synchronous spiking and asynchronous firing
	rates in a cortical network model},
  journal = {J Neurosci},
  year = {2008},
  volume = {28},
  pages = {5268-80},
  number = {20}
}

@ARTICLE{kumar08highconductance,
  author = {Arvind Kumar and Sven Schrader and Ad Aertsen and Stefan Rotter},
  title = {The high-conductance state of cortical networks},
  journal = {Neural Computation},
  year = {2008},
  volume = {20},
  pages = {1--43},
  number = {1},
  month = {Jan},
  timestamp = {2007.05.30}
}

@article{kumar10spiking,
    abstract = {
                The brain is a highly modular structure. To exploit modularity, it is necessary that spiking activity can propagate from one module to another while preserving the information it carries. Therefore, reliable propagation is one of the key properties of a candidate neural code. Surprisingly, the conditions under which spiking activity can be propagated have received comparatively little attention in the experimental literature. By contrast, several computational studies in the last decade have addressed this issue. Using feedforward networks ({FFNs}) as a generic network model, they have identified two dynamical activity modes that support the propagation of either asynchronous (rate code) or synchronous (temporal code) spiking. Here, we review the dichotomy of asynchronous and synchronous propagation in {FFNs}, propose their integration into a single extended conceptual framework and suggest experimental strategies to test our hypothesis.
            },
    author = {Kumar, Arvind and Rotter, Stefan and Aertsen, Ad},
    day = {01},
    doi = {10.1038/nrn2886},
    issn = {1471-0048},
    journal = {Nature reviews. Neuroscience},
    keywords = {neural-network, propagation, spiking-model},
    month = sep,
    number = {9},
    pages = {615--627},
    pmid = {20725095},
    posted-at = {2011-03-29 16:47:49},
    priority = {5},
    publisher = {Nature Publishing Group},
    title = {Spiking activity propagation in neuronal networks: reconciling different perspectives on neural coding.},
    url = {http://dx.doi.org/10.1038/nrn2886},
    volume = {11},
    year = {2010}
}

@ARTICLE{kungl2019accelerated,
AUTHOR={Kungl, Akos F. and Schmitt, Sebastian and Klähn, Johann and Müller, Paul and Baumbach, Andreas and Dold, Dominik and Kugele, Alexander and Müller, Eric and Koke, Christoph and Kleider, Mitja and Mauch, Christian and Breitwieser, Oliver and Leng, Luziwei and Gürtler, Nico and Güttler, Maurice and Husmann, Dan and Husmann, Kai and Hartel, Andreas and Karasenko, Vitali and Grübl, Andreas and Schemmel, Johannes and Meier, Karlheinz and Petrovici, Mihai A.},
TITLE={Accelerated Physical Emulation of Bayesian Inference in Spiking Neural Networks},
JOURNAL={Frontiers in Neuroscience},
VOLUME={13},
PAGES={1201},
YEAR={2019},
URL={https://www.frontiersin.org/article/10.3389/fnins.2019.01201},
DOI={10.3389/fnins.2019.01201},
ISSN={1662-453X},
ABSTRACT={The massively parallel nature of biological information processing plays an important role due to its superiority in comparison to human-engineered computing devices. In particular, it may hold the key to overcoming the von Neumann bottleneck that limits contemporary computer architectures. Physical-model neuromorphic devices seek to replicate not only this inherent parallelism, but also aspects of its microscopic dynamics in analog circuits emulating neurons and synapses. However, these machines require network models that are not only adept at solving particular tasks, but that can also cope with the inherent imperfections of analog substrates. We present a spiking network model that performs Bayesian inference through sampling on the BrainScaleS neuromorphic platform, where we use it for generative and discriminative computations on visual data. By illustrating its functionality on this platform, we implicitly demonstrate its robustness to various substrate-specific distortive effects, as well as its accelerated capability for computation. These results showcase the advantages of brain-inspired physical computation and provide important building blocks for large-scale neuromorphic applications.}
}

@article{kurata2006potassium,
title = "A structural interpretation of voltage-gated potassium channel inactivation ",
journal = "Progress in Biophysics and Molecular Biology ",
volume = "92",
number = "2",
pages = "185 - 208",
year = "2006",
note = "",
issn = "0079-6107",
doi = "http://dx.doi.org/10.1016/j.pbiomolbio.2005.10.001",
url = "http://www.sciencedirect.com/science/article/pii/S0079610705000635",
author = "Harley T. Kurata and David Fedida"
}

@article{kurtzer2017singularity,
    author = {Kurtzer, Gregory M. AND Sochat, Vanessa AND Bauer, Michael W.},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {Singularity: Scientific containers for mobility of compute},
    year = {2017},
    month = {05},
    volume = {12},
    pages = {1-20},
    abstract = {Here we present Singularity, software developed to bring containers and reproducibility to scientific computing. Using Singularity containers, developers can work in reproducible environments of their choosing and design, and these complete environments can easily be copied and executed on other platforms. Singularity is an open source initiative that harnesses the expertise of system and software engineers and researchers alike, and integrates seamlessly into common workflows for both of these groups. As its primary use case, Singularity brings mobility of computing to both users and HPC centers, providing a secure means to capture and distribute software and compute environments. This ability to create and deploy reproducible environments across these centers, a previously unmet need, makes Singularity a game changing development for computational science.},
    number = {5},
    doi = {10.1371/journal.pone.0177459}
}

@INPROCEEDINGS{kussul06image,
  author = {E. Kussul and T. Baidyk and D. Wunhsch II and O. Makeyev and A. Martin},
  title = {Image recognition systems based on random local descriptors},
  booktitle = {Proceedings of the 2006 International Joint Conference on Neural
	Networks (IJCNN 2006)},
  year = {2006},
  pages = {4722--4727},
  publisher = {IEEE Press},
  keywords = {convolutional NN},
  owner = {fieres}
}

@MASTERSTHESIS{kutny18bachelorthesis,
  author   = {Daniel Kutny},
  title    = {Development of a Modern Monitoring Platform for the BrainScaleS System},
  year     = 2018,
  type     = {Bachelor thesis},
  school   = {Ruprecht-Karls-Universit{\"a}t Heidelberg}
}

@ARTICLE{kwan_eletter92,
  author = {Kwan, HK.},
  title = {Simple sigmoid-like activation function suitable for digital hardware
	implementation},
  journal = {IEE Electronics Letters},
  year = {1992},
  volume = {28},
  pages = {1379-1380},
  number = {15}
}

@MISC{neuroanalysis_homepage,
  author = {Laboratory of Neuroinformatics, Weill Medical College, Cornell University},
  title = {{N}euro{A}nalysis Website},
  howpublished = {\url{http://neuroanalysis.org}},
  year = {2008}
}

@MISC{neurodatabase_homepage,
  author = {Laboratory of Neuroinformatics, Weill Medical College, Cornell University},
  title = {{N}euro{D}ata{B}ase Website},
  howpublished = {\url{http://neurodatabase.org}},
  year = {2008}
}

@BOOK{laing2009stochastic,
  title = {Stochastic Methods in Neuroscience},
  publisher = {Oxford University Press},
  year = {2009},
  author = {Carlo Laing and Gabriel J. Lord}
}

@BOOK{laker94,
  title = {Design of Analog Integrated Circuits and Systems},
  publisher = {McGraw-Hill,Inc},
  year = {1994},
  author = {Kenneth R. Laker and Willy M. C. Sansen},
  isbn = {007036060}
}

@ARTICLE{lampl_1999,
  author = {Ilan Lampl and I Reichova and David Ferster},
  title = {Synchronous membrane potential fluctuations in neurons of the cat
	visual cortex},
  journal = {Neuron},
  year = {1999},
  volume = {22},
  pages = {361--74},
  number = {2},
  month = {Feb},
  abstract = {We have recorded intracellularly from pairs of neurons less than 500
	microm distant from one another in V1 of anesthetized cats. Cross-correlation
	of spontaneous fluctuations in membrane potential revealed significant
	correlations between the cells in each pair. This synchronization
	was not dependent on the occurrence of action potentials, indicating
	that it was not caused by mutual interconnections. The cells were
	synchronized continuously rather than for brief epochs. Much weaker
	correlations were found between the EEG and intracellular potentials,
	suggesting local, rather than global, synchrony. The highest correlation
	occurred among cells with similar connectivity from the LGN and similar
	receptive fields. During visual stimulation, correlations increased
	when both cells responded to the stimulus and decreased when neither
	cell responded.},
  affiliation = {Department of Neurobiology and Physiology, Northwestern University,
	Evanston, Illinois 60208, USA. i-lampl@nwu.edu}
}

@BOOK{lamport94latexe,
  title = {{L}a{T}e{X} -- A Document Preparation System},
  publisher = {Addison-Wesley},
  year = {1994},
  author = {L. Lamport},
  edition = {second},
  note = {Updated for {L}a{T}e{X}e}
}

@ARTICLE{Lamprecht2004,
  author = {Lamprecht, Raphael and LeDoux, Joseph},
  title = {Structural plasticity and memory},
  journal = {Nature Reviews Neuroscience},
  year = {2004},
  volume = {5},
  pages = {45--54},
  number = {1},
  owner = {simon},
  publisher = {Nature Publishing Group},
  timestamp = {2013.04.24}
}

@INPROCEEDINGS{lande96,
  author = {Lande, T.S. and Ranjbar, H. and Ismail, M. and Berg, Y.},
  title = {An analog floating-gate memory in a standard digital technology},
  booktitle = {Microelectronics for Neural Networks, 1996., Proceedings of Fifth
	International Conference on},
  year = {1996},
  pages = {271 -276},
  month = {12-14},
  abstract = {In this paper we present a simple CMOS analog memory structure using
	the floating gate of a MOS transistor. The structure is based on
	a special but simple layout which allows significant tunneling at
	relatively low voltage levels. The programming of the memory is achieved
	using the standard Fowler-Nordheim tunneling and is implemented in
	a standard digital CMOS process with only one polysilicon layer.
	A simple on-chip memory driver circuit is also presented. Experimental
	results from test chips fabricated in a standard 2-micron CMOS process
	show six orders of magnitude dynamic range in current for subthreshold
	operation },
  doi = {10.1109/MNNFS.1996.493802},
  keywords = {2 micron;CMOS analog memory structure;Fowler-Nordheim tunneling;MOS
	transistor;Si;analog floating-gate memory;onchip memory driver circuit;polysilicon
	layer;programming;standard digital technology;subthreshold operation;CMOS
	analogue integrated circuits;CMOS memory circuits;analogue processing
	circuits;analogue storage;driver circuits;tunnelling;}
}

@ARTICLE{lane2010,
  author = {Lane, Nick and Martin, William F.},
  title = {The Energetics of Genome Complexity},
  journal = {Nature},
  year = {2010},
  volume = {467},
  pages = {929-934}
}

@ARTICLE{lane2012,
  author = {Lane, Nick and Martin, William F.},
  title = {The Origin of Membrane Bioenergetics},
  journal = {Cell},
  year = {2012},
  volume = {151},
  pages = {1406-1416},
  number = {7}
}

@INPROCEEDINGS{lang_cmss88twospirals,
  author = {Lang, K.J. and Witbrock, M.J.},
  title = {Learning to Tell Two Spirals Apart},
  booktitle = {Proceedings of 1988 Connectionist Models Summer School},
  year = {1988},
  editor = {D.S. Touretzky and G. Hinton and T. Sejnowski},
  pages = {52--59},
  publisher = {Morgan Kaufmann Publishers Inc.},
  groupsearch = {0}
}

@ARTICLE{landstrom2007,
  author = {Landstr\"{o}m, Sara and Larzon, Lars-{\AA}ke},
  title = {Reducing the TCP acknowledgment frequency},
  journal = {SIGCOMM Comput. Commun. Rev.},
  year = {2007},
  volume = {37},
  pages = {5--16},
  number = {3},
  address = {New York, NY, USA},
  issn = {0146-4833},
  publisher = {ACM}
}

@BOOK{langdon98genetic,
  title = {Genetic Programming And Data Structures},
  publisher = {Kluwer Academic Publishers Group},
  year = {1998},
  author = {Langdon, William B.},
  address = {the Netherlands}
}

@INPROCEEDINGS{langeheine_ices01,
  author = {Langeheine, J{\"o}rg and Becker, Joachim and F{\"o}lling, Simon and
	Meier, Karlheinz and Schemmel, Johannes},
  title = {Initial Studies of a New {VLSI} Field Programmable Transistor Array},
  booktitle = {Proc.\ 4th Int.\ Conf.\ on Evolvable Systems: From Biology to Hardware
	(ICES2001)},
  year = {2001},
  editor = {Liu, Yong and Kiyoshi, Tanaka and Masaya, Iwata and Higuchi, Tetsuya
	and Yasunaga, Moritoshi},
  pages = {62--73},
  address = {Tokio, Japan},
  month = Oct,
  publisher = {Springer Verlag},
  keywords = {vision eh}
}

@INPROCEEDINGS{langeheine_eh01,
  author = {Langeheine, J{\"o}rg and Becker, Joachim and F{\"o}lling, Simon and
	Meier, Karlheinz and Schemmel, Johannes},
  title = {A {CMOS} {FPTA} chip for intrinsic hardware evolution of analog electronic
	circuits},
  booktitle = {Proc.\ of the Third NASA/DOD Workshop on Evolvable Hardware},
  year = {2001},
  pages = {172--175},
  address = {Long Beach, CA, USA},
  month = Jul,
  publisher = {IEEE Computer Society Press},
  keywords = {vision eh}
}

@INPROCEEDINGS{langeheine_ices00,
  author = {Langeheine, J{\"o}rg and F{\"o}lling, Simon and Meier, Karlheinz
	and Schemmel, Johannes},
  title = {Towards a silicon primordial soup: A fast approach to hardware evolution
	with a {VLSI} transistor array},
  booktitle = {Proceedings of the 3rd International Conference on Evolvable Systems:
	From Biology to Hardware (ICES2000)},
  year = {2001},
  editor = {Miller, Julian and Thompson, Adrian and Thomson, Peter and Fogarty,
	Terence C.\ },
  pages = {123--132},
  address = {Edinburgh, Scotland, UK},
  month = Apr,
  publisher = {Springer Verlag},
  keywords = {vision eh}
}

@INPROCEEDINGS{langeheine_eh02,
  author = {J{\"o}rg Langeheine and Karlheinz Meier and Johannes Schemmel},
  title = {{I}ntrinsic {E}volution of {Q}uasi {DC} {S}olutions for {T}ransistor
	{L}evel {A}nalog {E}lectronic {C}ircuits {U}sing a {CMOS} {FPTA}
	Chip},
  booktitle = {Proceedings of the 2002 {NASA}/{DoD} Conference an Evolvable Hardware},
  year = {2002},
  keywords = {vision eh}
}

@INPROCEEDINGS{langeheine_eh04,
  author = {J{\"o}rg Langeheine and Karlheinz Meier and Johannes Schemmel and
	Martin Trefzer},
  title = {Intrinsic evolution of digital-to-analog converters using a {CMOS}
	{FPTA} chip},
  booktitle = {Proceedings of the 2004 {NASA}/{DoD} Conference an Evolvable Hardware
	({EH}2004)},
  year = {2004},
  keywords = {vision eh}
}

@INPROCEEDINGS{langeheine_gecco04,
  author = {J{\"o}rg Langeheine and Martin Trefzer and Daniel Br\"uderle and
	Karlheinz Meier and Johannes Schemmel},
  title = {On the Evolution of Analog Electronic Circuits Using Building Blocks
	on a {CMOS} {FPTA}},
  booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference({GECCO}2004)},
  year = {2004}
}

@MASTERSTHESIS{langner03masterthesis,
  author = {Langner, J.},
  title = {{Development of a Parallel Computing Optimized Head Movement Correction
	Method in Positron Emission Tomography}},
  school = {{University of Applied Sciences Dresden and Research Center Dresden-Rossendorf}},
  year = {2003},
  type = {Master of Computer Science thesis},
  url = {http://www.jens-langner.de/ftp/MScThesis.pdf}
}

@BOOK{langtangen2012python,
  title = {Python Scripting for Computational Science},
  publisher = {Springer},
  year = {2012},
  author = {Langtangen, Hans Petter},
  series = {Texts in Computational Science and Engineering},
  edition = {4th},
  isbn = {9783642219627},
  url = {http://books.google.de/books?id=0GIBfAEACAAJ}
}

@BOOK{langtangen2009primer,
  title = {A Primer on Scientific Programming With Python},
  publisher = {Springer},
  year = {2009},
  author = {Langtangen, Hans Petter},
  series = {Texts in Computational Science and Engineering},
  edition = {2nd},
  isbn = {9783642024740},
  lccn = {2009931367},
  url = {http://books.google.de/books?id=cVof07z\_rA4C}
}

@BOOK{langtangen_python,
  title = {Python Scripting for Computational Science},
  publisher = {Springer},
  year = {2008},
  author = {Langtangen, Hans Petter},
  edition = {3rd},
  month = {February},
  abstract = {The goal of this book is to teach computational scientists how to
	develop tailored, flexible, and human-efficient working environments
	built from small programs (scripts) written in the easy-to-learn,
	high-level language Python. The focus is on examples and applications
	of relevance to computational scientists: gluing existing applications
	and tools, e.g. for automating simulation, data analysis, and visualization;
	steering simulations and computational experiments; equipping old
	programs with graphical user interfaces; making computational Web
	applications; and creating interactive interfaces with a Maple/Matlab-like
	syntax to numerical applications in C/C++ or Fortran. In short, scripting
	with Python makes you much more productive, increases the reliability
	of your scientific work and lets you have more fun - on Unix, Windows
	and Macintosh. All the tools and examples in this book are open source
	codes. The third edition is compatible with the new NumPy implementation
	and features updated information, correction of errors, and improved
	associated software tools.},
  howpublished = {Hardcover},
  isbn = {3540739157},
  keywords = {0-epfl, 2008, books, programming, python}
}

@ARTICLE{langton90,
  author = {Langton, C. G.},
  title = {Computation at the Edge of Chaos},
  journal = {Physica D},
  year = {1990},
  volume = {42},
  keywords = {lq liquid}
}

@Article{lansky1997sources,
  Author         = {L{\'a}nsk{\`y}, Petr},
  Title          = {Sources of periodical force in noisy
                   integrate-and-fire models of neuronal dynamics},
  Journal        = {Physical Review E},
  Volume         = {55},
  Pages          = {2040--2043},
  year           = 1997
}

@Article{lapicque1907recherches,
  Author         = {Lapicque, Louis},
  Title          = {Recherches quantitatives sur l'excitation electrique des nerfs traitee comme une polarization},
  Journal        = {Journal de Physiologie et Pathologie General},
  Volume         = {9},
  Pages          = {620-635},
  year           = 1907
}

@article{larkum2009, crossref = {larkum2009synaptic} }
@article{larkum2009synaptic,
  title={Synaptic integration in tuft dendrites of layer 5 pyramidal neurons: a new unifying principle},
  author={Larkum, Matthew E and Nevian, Thomas and Sandler, Maya and Polsky, Alon and Schiller, Jackie},
  journal={Science},
  volume={325},
  number={5941},
  pages={756--760},
  year={2009},
  publisher={American Association for the Advancement of Science},
  doi={10.1126/science.1171958}
}

@article{larkum2013, crossref = {larkum2013cellular} }
@article{larkum2013cellular,
  title={A cellular mechanism for cortical associations: an organizing principle for the cerebral cortex},
  author={Larkum, Matthew},
  journal={Trends in neurosciences},
  volume={36},
  number={3},
  pages={141--151},
  year={2013},
  publisher={Elsevier Current Trends}
}

@INCOLLECTION{lawrence98prior,
  author = {Lawrence, S. and Burns, I. and Back, A.D. and Tsoi, A.C. and Giles,
	C. L.},
  title = {Neural Network Classification and Prior Class Probabilities},
  booktitle = {Tricks of the Trade},
  publisher = {Springer Verlag},
  year = {1998},
  editor = {Orr, G. and {M\"uller}, K.-R. and Caruana, R.},
  series = {Lecture Notes in Computer Science State-of-the-Art Surveys},
  pages = {299--314},
  file = {lawrence98prior.pdf:lawrence98prior.pdf:PDF},
  url = {citeseer.ist.psu.edu/lawrence98neural.html}
}

@ARTICLE{lawrence1997recognition,
  author = {S. Lawrence and C.L. Giles and A.C. Tsoi and A.D. Back},
  title = {Face recognition: a convolutional neural network approach},
  journal = {Transactions on Neural Networks},
  year = {1997},
  volume = {8(1)},
  pages = {98-113},
  file = {lawrence1997recognition.pdf:lawrence1997recognition.pdf:PDF},
  keywords = {convolutional NN},
  owner = {fieres}
}

@TECHREPORT{lawrence96generalization,
  author = {Lawrence, S. and Giles, C. L. and Tsoi, A.C.},
  title = {What Size Neural Network Gives Optimal Generalization? {C}onvergence
	Properties of Backpropagation},
  institution = {UMIACS},
  year = {1996},
  number = {UMIACS-TR-96-22 and CS-TR-3617},
  month = {April},
  file = {lawrence96generalization.pdf:lawrence96generalization.pdf:PDF},
  url = {citeseer.ist.psu.edu/lawrence96what.html}
}

@ARTICLE{lazar07fading,
  author = {Lazar, Andreea and Pipa, Gordon and Triesch, Jochen},
  title = {Fading memory and time series prediction in recurrent networks with
	different forms of plasticity},
  journal = {Neural Networks},
  year = {2007},
  volume = {20},
  pages = {312--322},
  number = {3},
  month = apr,
  abstract = {We investigate how different forms of plasticity shape the dynamics
	and computational properties of simple recurrent spiking neural networks.
	In particular, we study the effect of combining two forms of neuronal
	plasticity: spike timing dependent plasticity (STDP), which changes
	the synaptic strength, and intrinsic plasticity (IP), which changes
	the excitability of individual neurons to maintain homeostasis of
	their activity. We find that the interaction of these forms of plasticity
	gives rise to interesting network dynamics characterized by a comparatively
	large number of stable limit cycles. We study the response of such
	networks to external input and find that they exhibit a fading memory
	of recent inputs. We then demonstrate that the combination of STDP
	and IP shapes the network structure and dynamics in ways that allow
	the discovery of patterns in input time series and lead to good performance
	in time series prediction. Our results underscore the importance
	of studying the interaction of different forms of plasticity on network
	behavior.},
  booktitle = {Echo State Networks and Liquid State Machines},
  keywords = {Recurrent neural networks, Time series, Intrinsic plasticity, Spike
	timing dependent plasticity, liquid},
  owner = {mreuss},
  timestamp = {2007.06.14},
  url = {http://www.sciencedirect.com/science/article/B6T08-4NMWR88-2/2/6e7233822008549f97db7d29db2de1c1}
}

@misc{lgplv21,
  title        = {{GNU} Lesser General Public License},
  version      = {2.1},
  organization = {Free Software Foundation},
  url          = {http://www.gnu.org/licenses/gpl.html},
  pagination   = {section},
  language     = {english},
}

@article{le2008representational,
  title={Representational power of restricted Boltzmann machines and deep belief networks},
  author={Le Roux, Nicolas and Bengio, Yoshua},
  journal={Neural Computation},
  volume={20},
  number={6},
  pages={1631--1649},
  year={2008},
  publisher={MIT Press}
}

@ARTICLE{legenstein07edge,
  author = {Le\-gen\-stein, Robert and Ma\-ass, Wolfgang},
  title = {Edge of chaos and prediction of computational performance for neural
	circuit models},
  journal = {Neural Networks},
  year = {2007},
  volume = {20},
  pages = {323--334},
  number = {3},
  month = apr,
  abstract = {We analyze in this article the significance of the edge of chaos for
	real-time computations in neural microcircuit models consisting of
	spiking neurons and dynamic synapses. We find that the edge of chaos
	predicts quite well those values of circuit parameters that yield
	maximal computational performance. But obviously it makes no prediction
	of their computational performance for other parameter values. Therefore,
	we propose a new method for predicting the computational performance
	of neural microcircuit models. The new measure estimates directly
	the kernel property and the generalization capability of a neural
	microcircuit. We validate the proposed measure by comparing its prediction
	with direct evaluations of the computational performance of various
	neural microcircuit models. The proposed method also allows us to
	quantify differences in the computational performance and generalization
	capability of neural circuits in different dynamic regimes (UP- and
	DOWN-states) that have been demonstrated through intracellular recordings
	in vivo.},
  booktitle = {Echo State Networks and Liquid State Machines},
  keywords = {Neural networks, Spiking networks, Edge of chaos, Microcircuits, Computational
	performance, Network dynamics, liquid},
  owner = {mreuss},
  timestamp = {2007.06.14}
}

@article{legallo2018mixedprecision,
    Author = {Le Gallo, Manuel and Sebastian, Abu and Mathis, Roland and Manica, Matteo and Giefers, Heiner and Tuma, Tomas and Bekas, Costas and Curioni, Alessandro and Eleftheriou, Evangelos},
    Doi = {10.1038/s41928-018-0054-8},
    Isbn = {2520-1131},
    Journal = {Nature Electronics},
    Number = 4,
    Pages = {246--253},
    Title = {Mixed-precision in-memory computing},
    Url = {https://doi.org/10.1038/s41928-018-0054-8},
    Volume = 1,
    Year = 2018
}

@article{legallo2018mixedprecision_nourl,
    Author = {Le Gallo, Manuel and Sebastian, Abu and Mathis, Roland and Manica, Matteo and Giefers, Heiner and Tuma, Tomas and Bekas, Costas and Curioni, Alessandro and Eleftheriou, Evangelos},
    Doi = {10.1038/s41928-018-0054-8},
    Isbn = {2520-1131},
    Journal = {Nature Electronics},
    Number = 4,
    Pages = {246--253},
    Title = {Mixed-precision in-memory computing},
    Volume = 1,
    Year = 2018
}

@MISC{lecunmnist,
  title={The MNIST database of handwritten digits},
  author={LeCun, Yann and Cortes, Corinna},
  year={1998}
}

@ARTICLE{lecun1989recognition,
  author = {Y. LeCun and B. Boser and J.S. Denker and D. Henderson and R.E. Howard
	and W. Hubbard and L.D. Jackel},
  title = {Backpropagation applied to handwritten zip code recognition},
  journal = {Neural Computation},
  year = {1989},
  volume = {1(4)},
  pages = {541-551},
  file = {lecun1989recognition.ps.gz:lecun1989recognition.ps.gz:PDF},
  keywords = {convolutional networks, learning, convolutional NN},
  owner = {fieres}
}

@ARTICLE{lecun1998recognition,
  author = {Y. LeCun and L. Bottou and Y. Bengioa and P. Haffner},
  title = {Gradient-based learning applied to document recognition},
  journal = {Proceedings of the IEEE},
  year = {1998},
  volume = {86},
  pages = {2278--2324},
  number = {11},
  file = {lecun1998recognition.ps.gz:lecun1998recognition.ps.gz:PDF},
  keywords = {learning, convolutional NN},
  owner = {fieres}
}

@inproceedings{lecun2004learning,
  title={Learning methods for generic object recognition with invariance to pose and lighting},
  author={LeCun, Yann and Huang, Fu Jie and Bottou, Leon},
  booktitle={Computer Vision and Pattern Recognition, 2004. CVPR 2004. Proceedings of the 2004 IEEE Computer Society Conference on},
  volume={2},
  pages={II--97},
  organization={IEEE}
}

@article{lecun2015deep,
	abstract = "Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.",
	author = "LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey",
	doi = "http://dx.doi.org/10.1038/nature14539 10.1038/nature14539",
	issn = "0028-0836",
	journal = "Nature",
	month = "may",
	number = "7553",
	pages = "436--444",
	publisher = "Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.",
	title = "{Deep learning}",
	volume = "521",
	year = "2015"
}

@ARTICLE{lecun89nnchips,
  author = {Y. LeCun and L. D. Jackel and B. Boser and J.S. Denker and H. P.
	Graf and I. Guyon and D. Henderson and R.E. Howard and W. Hubbard},
  title = {Handwritten digit recognition: Applications of neural net chips and
	automatic learning},
  journal = {IEEE Communications Magazine},
  year = {1989},
  volume = {November 1989},
  pages = {41-46},
  keywords = {convolutional NN, vlsi},
  owner = {fieres}
}

@ARTICLE{lee99,
  author = {D. K. Lee and L. Itti and C. Koch and J. Braun},
  title = {Attention activates winner-take-all competition among visual filters},
  journal = {Nature Neuroscience},
  year = {1999},
  volume = {2},
  pages = {375-81},
  number = {4},
  month = {Apr},
  abstract = {Shifting attention away from a visual stimulus reduces, but does not
	abolish, visual discrimination performance. This residual vision
	with 'poor' attention can be compared to normal vision with 'full'
	attention to reveal how attention alters visual perception. We report
	large differences between residual and normal visual thresholds for
	discriminating the orientation or spatial frequency of simple patterns,
	and smaller differences for discriminating contrast. A computational
	model, in which attention activates a winner-take-all competition
	among overlapping visual filters, quantitatively accounts for all
	observations. Our model predicts that the effects of attention on
	visual cortical neurons include increased contrast gain as well as
	sharper tuning to orientation and spatial frequency.},
  address = {Computation and Neural Systems, California Institute of Technology,
	Pasadena 91125, USA.},
  file = {http://iLab.usc.edu/publications/doc/Lee_etal99nn.pdf},
  if = {2000 impact factor: 12.636},
  keywords = {Attention/*physiology ; Contrast Sensitivity/physiology ; Discrimination
	(Psychology)/*physiology ; Human ; *Models, Neurological ; Neurons/physiology
	; Pattern Recognition, Visual/physiology ; Perceptual Masking ; Sensory
	Thresholds ; Space Perception/physiology ; Support, U.S. Gov't, Non-P.H.S.
	; Support, U.S. Gov't, P.H.S. ; Visual Cortex/*physiology ; Visual
	Perception/*physiology ; 1999/04/16 02:03},
  type = {mod;td;psy}
}

@ARTICLE{legenstein05learn,
  author = {Legenstein, Robert and Naeger, Christian and Maass, Wolfgang},
  title = {What Can a Neuron Learn with Spike-Timing-Dependent Plasticity?},
  journal = {Neural Computation},
  year = {2005},
  volume = {17},
  pages = {2337-2382},
  number = {11},
  month = {November},
  abstract = {Spiking neurons are very flexible computational modules, which can
	implement with different values of their adjustable synaptic parameters
	an enormous variety of different transformations F from input spike
	trains to output spike trains. We examine in this letter the question
	to what extent a spiking neuron with biologically realistic models
	for dynamic synapses can be taught via spike-timing-dependent plasticity
	(STDP) to implement a given transformation F. We consider a supervised
	learning paradigm where during training, the output of the neuron
	is clamped to the target signal (teacher forcing). The well-known
	perceptron convergence theorem asserts the convergence of a simple
	supervised learning algorithm for drastically simplified neuron models
	(McCulloch-Pitts neurons). We show that in contrast to the perceptron
	convergence theorem, no theoretical guarantee can be given for the
	convergence of STDP with teacher forcing that holds for arbitrary
	input spike patterns. On the other hand, we prove that average case
	versions of the perceptron convergence theorem hold for STDP in the
	case of uncorrelated and correlated Poisson input spike trains and
	simple models for spiking neurons. For a wide class of cross-correlation
	functions of the input spike trains, the resulting necessary and
	sufficient condition can be formulated in terms of linear separability,
	analogously as the well-known condition of learnability by perceptrons.
	However, the linear separability criterion has to be applied here
	to the columns of the correlation matrix of the Poisson input. We
	demonstrate through extensive computer simulations that the theoretically
	predicted convergence of STDP with teacher forcing also holds for
	more realistic models for neurons, dynamic synapses, and more general
	input distributions. In addition, we show through computer simulations
	that these positive learning results hold not only for the common
	interpretation of STDP, where STDP changes the weights of synapses,
	but also for a more realistic interpretation suggested by experimental
	data where STDP modulates the initial release probability of dynamic
	synapses.},
  file = {legenstein05learn.pdf:legenstein05learn.pdf:PDF},
  keywords = {plasticity learning},
  owner = {mreuss}
}

@ARTICLE{legenstein2008learning,
  author = {Legenstein, R. and Pecevski, D. and Maass, W.},
  title = {A learning theory for reward-modulated spike-timing-dependent plasticity
	with application to biofeedback},
  journal = {PLoS Computational Biology},
  year = {2008},
  volume = {4},
  pages = {e1000180},
  number = {10},
  file = {:legenstein2008learning.pdf:PDF},
  owner = {simon},
  publisher = {Public Library of Science},
  timestamp = {2013.01.05}
}

@ARTICLE{lehmann08plasticity,
  author = {Lehmann, Konrad and L\"owel, Siegrid},
  title = {Age-Dependent Ocular Dominance Plasticity in Adult Mice},
  journal = {PLOS ONE},
  year = {2008},
  volume = {3},
  pages = {e3120},
  number = {9},
  month = {Sep},
  abstract = {Background: Short monocular deprivation (4 days) induces a shift in
	the ocular dominance of binocular neurons in the juvenile mouse visual
	cortex but is ineffective in adults. Recently, it has been shown
	that an ocular dominance shift can still be elicited in young adults
	(around 90 days of age) by longer periods of deprivation (7 days).
	Whether the same is true also for fully mature animals is not yet
	known. Methodology/Principal Findings: We therefore studied the effects
	of different periods of monocular deprivation (4, 7, 14 days) on
	ocular dominance in C57Bl/6 mice of different ages (25 days, 90\u2013100
	days, 109\u2013158 days, 208\u2013230 days) using optical imaging
	of intrinsic signals. In addition, we used a virtual optomotor system
	to monitor visual acuity of the open eye in the same animals during
	deprivation. We observed that ocular dominance plasticity after 7
	days of monocular deprivation was pronounced in young adult mice
	(90\u2013100 days) but significantly weaker already in the next age
	group (109\u2013158 days). In animals older than 208 days, ocular
	dominance plasticity was absent even after 14 days of monocular deprivation.
	Visual acuity of the open eye increased in all age groups, but this
	interocular plasticity also declined with age, although to a much
	lesser degree than the optically detected ocular dominance shift.
	Conclusions/Significance: These data indicate that there is an age-dependence
	of both ocular dominance plasticity and the enhancement of vision
	after monocular deprivation in mice: ocular dominance plasticity
	in binocular visual cortex is most pronounced in young animals, reduced
	but present in adolescence and absent in fully mature animals older
	than 110 days of age. Mice are thus not basically different in ocular
	dominance plasticity from cats and monkeys which is an absolutely
	essential prerequisite for their use as valid model systems of human
	visual disorders. },
  doi = {10.1371/journal.pone.0003120},
  publisher = {Public Library of Science},
  url = {http://dx.doi.org/10.1371%2Fjournal.pone.0003120}
}

@PHDTHESIS{lehmann94hardware,
  author = {T. Lehmann},
  title = {Hardware learning in analogue {VLSI} neural networks},
  school = {Techincal University of Denmark, Lyngby, Denmark},
  year = {1994},
  type = {PhD Thesis},
  owner = {fieres}
}

@inproceedings{leng2016spiking,
  author   = {Luziwei Leng and Mihai A. Petrovici and Roman Martel and Ilja Bytschok and Oliver Breitwieser and Johannes Bill and Johannes Schemmel and Karlheinz Meier},
  title    = {Spiking neural networks as superior generative and discriminative models},
  booktitle = {Cosyne Abstracts, Salt Lake City USA},
  year     = {2016},
  month    = {February}
}

@ARTICLE{Leuner2010,
  author = {Leuner, Benedetta and Gould, Elizabeth},
  title = {Structural Plasticity and Hippocampal Function},
  journal = {Annual Review of Psychology},
  year = {2010},
  volume = {61},
  pages = {111-140},
  number = {1},
  note = {PMID: 19575621},
  doi = {10.1146/annurev.psych.093008.100359},
  eprint = {http://www.annualreviews.org/doi/pdf/10.1146/annurev.psych.093008.100359},
  owner = {simon},
  timestamp = {2013.04.24},
  url = {http://www.annualreviews.org/doi/abs/10.1146/annurev.psych.093008.100359}
}

@ARTICLE{levy1983LTP,
  author = {Levy, W.B. and Steward,O.},
  title = {Temporal contiguity requirements for long-term associative potentiation/depression
	in the hippocampus},
  journal = {Neuroscience},
  year = {1983},
  volume = {8},
  pages = {791-97},
  owner = {bkaplan},
  timestamp = {2008.12.03}
}

@INPROCEEDINGS{lewis00toward,
  author = {M. Anthony Lewis and Ralph Etienne-Cummings and Avis H. Cohen and
	Mitra Hartmann},
  title = {Toward Biomorphic Control using Custom a{VLSI} Chips},
  booktitle = {Proceedings of the International conference on robotics and automation},
  year = {2000},
  publisher = {IEEE Press}
}

@INPROCEEDINGS{liaodidtowards,
  author = {S. Y. Liao},
  title = {Towards a New Standard for System Level Design},
  booktitle = {Proceedings of the Eighth International Workshop on Hardware/Software
	Codesign 2000 (CODES 2000)},
  year = {2000},
  pages = {2--7},
  publisher = {IEEE Cat. No.00TH8518},
  citeseerurl = {citeseer.nj.nec.com/528784.html},
  keywords = {SystemC}
}

@MISC{libev_homepage,
  author = {libev},
  title = {Website},
  howpublished = {\url{http://libev.schmorp.de/}},
  year = {2012}
}

@MISC{libev_homepage_benchmark,
  author = {libev},
  title = {Website -- Benchmarks},
  howpublished = {\url{http://libev.schmorp.de/bench.html}},
  year = {2012}
}

@article{long2007structure,
	title={Atomic structure of a voltage-dependent K+ channel in a lipid membrane-like environment},
	author={Long, Stephen B. and Tao, Xiao and Campbell, Ernest B. and MacKinnon, Roderick},
	journal={Nature},
	volume={450},
	pages={276--382},
	year={2007},
	publisher={Nature Publishing Group}
}

@article{lyon1988analog,
	title={An analog electronic cochlea},
	author={Lyon, R.F. and Mead, C.},
	journal={Acoustics, Speech and Signal Processing, IEEE Transactions on},
	volume={36},
	number={7},
	pages={1119--1134},
	year={1988},
	publisher={IEEE}
}

@MISC{pyqt4homepage,
  author = {Riverbank Computing Limited},
  title = {{P}y{Q}t -- {P}ython bindings for {T}rolltech's {Q}t application
	framework},
  howpublished = {http://www.riverbankcomputing.co.uk/software/pyqt/intro},
  year = {2007},
  owner = {mueller},
  timestamp = {2008.08.05}
}

@article{li2018review,
  title={Review of memristor devices in neuromorphic computing: materials sciences and device challenges},
  author={Li, Yibo and Wang, Zhongrui and Midya, Rivu and Xia, Qiangfei and Yang, J Joshua},
  journal={Journal of Physics D: Applied Physics},
  volume=51,
  number=50,
  pages=503002,
  year=2018,
  publisher={IOP Publishing}
}

@ARTICLE{lin2018programming,
  title={Programming Spiking Neural Networks on Intel's Loihi},
  author={Lin, Chit-Kwan and Wild, Andreas and Chinya, Gautham N and Cao, Yongqiang and Davies, Mike and Lavery, Daniel M and Wang, Hong},
  journal={Computer},
  volume=51,
  number=3,
  pages={52--61},
  year=2018,
  publisher={IEEE}
}

@ARTICLE{lin_csii98,
  author = {Lin, Shang-Yi and Huang, Ren-Jiun and Chiueh, Tzi-Dar},
  title = {A Tunable Gaussian/Square Function Computation Circuit for Analog
	Neural Networks},
  journal = {IEEE Transactions on Circuits and Systems II: Analog and Digital
	Signal Processing},
  year = {1998},
  volume = {45},
  pages = {441--446},
  number = {3},
  month = Mar,
  groupsearch = {0}
}

@MISC{lindenstruth_ti2,
  author = {Volker Lindenstruth},
  title = {{Informatik II (Technische Informatik)}, lecture notes, {University
	of Heidelberg}, summer term 2004},
  url = {http://www.kip.uni-heidelberg.de/ti/teaching/I2SS04/}
}

@ARTICLE{lindsey_spie99survey,
  author = {Lindsey, C. and Lindblad, T.},
  title = {Survey of Neural Network Hardware},
  journal = {SPIE},
  year = {1995},
  volume = {1995},
  pages = {1194-1205},
  number = {2492},
  month = {April},
  groupsearch = {0},
  key = {lindsey_survey},
  keywords = {vlsi, NN}
}

@ARTICLE{linsker88selforganization,
  author = {R. Linsker},
  title = {Self-organization in a perceptual network},
  journal = {Computer},
  year = {1988},
  volume = {21},
  pages = {105-- 117},
  file = {linsker88selforganization.pdf:linsker88selforganization.pdf:PDF},
  keywords = {learning},
  owner = {fieres}
}

@ARTICLE{linsker83neuralarchitecture,
  author = {R. Linsker},
  title = {From basic network principles to neural architecture},
  journal = {Proc. Natl. Sci. USA},
  year = {1983},
  volume = {83},
  pages = {7508--7512},
  note = {Series of 3 papers},
  keywords = {learning, NN},
  owner = {fieres}
}

@MISC{linuxrt_homepage,
  author = {{Real-Time Linux}},
  title = {Website},
  howpublished = {\url{https://rt.wiki.kernel.org/index.php/Main_Page}},
  year = {2012}
}

@ARTICLE{lippmann87comp,
  author = {Lippmann, R. P.},
  title = {An introduction to computing with neural nets},
  journal = {IEEE ASSP Magazine},
  year = {1987},
  volume = {4},
  pages = {4--22},
  number = {2}
}

@ARTICLE{litvak03,
  author = {Litvak, V. and Sompolinsky, H. and Segev, I. and Abeles, M.},
  title = {On the transmission of rate code in long feedforward networks with
	excitatory-inhibitory balance.},
  journal = {The Journal of neuroscience : the official journal of the Society
	for Neuroscience},
  year = {2003},
  volume = {23},
  pages = {3006--3015},
  number = {7},
  month = apr,
  abstract = {The capability of feedforward networks composed of multiple layers
	of integrate-and-fire neurons to transmit rate code was examined.
	Synaptic connections were made only from one layer to the next, and
	excitation was balanced by inhibition. When time is discrete and
	the synaptic potentials rise instantaneously, we show that, for random
	uncorrelated input to layer one, the mean rate of activity in deep
	layers is essentially independent of input firing rate. This implies
	that the input rate cannot be transmitted reliably in such feedforward
	networks because neurons in a given layer tend to synchronize partially
	with each other because of shared inputs. As a result of this synchronization,
	the average firing rate in deep layers will either decay to zero
	or reach a stable fixed point, depending on model parameters. When
	time is treated continuously and the synaptic potentials rise instantaneously,
	these effects develop slowly, and rate transmission over a limited
	number of layers is possible. However, the correlations among neurons
	at the same layer hamper reliable assessment of firing rate by averaging
	over 100 msec (or less). When the synaptic potentials develop gradually,
	as is the realistic case, transmission of rate code fails. In a network
	in which inhibition only balances the mean excitation but is not
	timed precisely with it, neurons in each layer fire together, and
	this volley successively propagates from layer to layer. We conclude
	that the transmission of rate code in feedforward networks is highly
	unlikely.},
  day = {1},
  issn = {1529-2401},
  keywords = {sequences},
  pmid = {12684488},
  posted-at = {2008-12-30 12:21:51},
  priority = {2},
  url = {http://view.ncbi.nlm.nih.gov/pubmed/12684488}
}

@BOOK{liu2002analog,
  title = {Analog VLSI: Circuits and principles},
  publisher = {The MIT press},
  year = {2002},
  author = {Shih-Chii Liu and J\"org Kramer and Giacomo Indiveri and Tobias Delbr\"uck
	and Rodney Douglas}
}

@INPROCEEDINGS{liu_ppsn98towards,
  author = {Liu, Y. and Yao, X.},
  title = {Towards Designing Neural Network Ensembles by Evolution},
  booktitle = {Proceedings of the 5th International Conference on Parallel Problem
	Solving from Nature (PPSN-V)},
  year = {1998},
  editor = {Eiben, A. E. and B\"ack, T. and Schoenauer, M. and Schwefel, H.-P.},
  volume = {1498},
  pages = {623--632},
  publisher = {Springer Verlag},
  file = {liu_ppsn98towards.pdf:liu_ppsn98towards.pdf:PDF},
  howpublished = {Lecture Notes in Computer Science}
}

@INPROCEEDINGS{livi09,
  author = {Livi, P. and Indiveri, G.},
  title = {A current-mode conductance-based silicon neuron for address-event
	neuromorphic systems},
  booktitle = {Circuits and Systems, 2009. ISCAS 2009. IEEE International Symposium
	on},
  year = {2009},
  pages = {2898 -2901},
  month = {24-27},
  abstract = {Silicon neuron circuits emulate the electrophysiological behavior
	of real neurons. Many circuits can be integrated on a single very
	large scale integration (VLSI) device, and form large networks of
	spiking neurons. Connectivity among neurons can be achieved by using
	time multiplexing and fast asynchronous digital circuits. As the
	basic characteristics of the silicon neurons are determined at design
	time, and cannot be changed after the chip is fabricated, it is crucial
	to implement a circuit which represents an accurate model of real
	neurons, but at the same time is compact, low-power and compatible
	with asynchronous logic. Here we present a current-mode conductance-based
	neuron circuit, with spike-frequency adaptation, refractory period,
	and bio-physically realistic dynamics which is compact, low-power
	and compatible with fast asynchronous digital circuits.},
  doi = {10.1109/ISCAS.2009.5118408},
  keywords = {VLSI;address-event neuromorphic systems;asynchronous logic;current-mode
	conductance-based silicon neuron;electrophysiological behavior;fast
	asynchronous digital circuits;spiking neurons;very large scale integration;VLSI;current-mode
	circuits;current-mode logic;neural chips;}
}

@BOOK{ljung1983,
  title = {Theory and Practice of Recursive Identification (Signal Processing,
	Optimization, and Control)},
  publisher = {{The MIT Press}},
  year = {1983},
  author = {Ljung, Lennart and Soderstrom, Torsten},
  month = {October},
  day = {20},
  howpublished = {Hardcover},
  isbn = {026212095X},
  posted-at = {2007-12-06 13:55:39},
  priority = {0}
}

@ARTICLE{logothetis01fmri,
  author = {Logothetis, N. K. and Pauls, J. and Augath, M. and Trinath, T. and
	Oeltermann, A. },
  title = {Neurophysiological investigation of the basis of the fMRI signal.},
  journal = {Nature},
  year = {2001},
  volume = {412},
  pages = {150--157},
  number = {6843},
  month = {July},
  abstract = {Functional magnetic resonance imaging (fMRI) is widely used to study
	the operational organization of the human brain, but the exact relationship
	between the measured fMRI signal and the underlying neural activity
	is unclear. Here we present simultaneous intracortical recordings
	of neural signals and fMRI responses. We compared local field potentials
	(LFPs), single- and multi-unit spiking activity with highly spatio-temporally
	resolved blood-oxygen-level-dependent (BOLD) fMRI responses from
	the visual cortex of monkeys. The largest magnitude changes were
	observed in LFPs, which at recording sites characterized by transient
	responses were the only signal that significantly correlated with
	the haemodynamic response. Linear systems analysis on a trial-by-trial
	basis showed that the impulse response of the neurovascular system
	is both animal- and site-specific, and that LFPs yield a better estimate
	of BOLD responses than the multi-unit responses. These findings suggest
	that the BOLD contrast mechanism reflects the input and intracortical
	processing of a given area rather than its spiking output.},
  keywords = {resting, spontaneous}
}

@INPROCEEDINGS{lohmann_ppsn91parallel,
  author = {Lohmann, R.},
  title = {Application of Evolution Strategy in Parallel Populations},
  booktitle = {Proceedings of the 1st International Conference on Parallel Problem
	Solving from Nature},
  year = {1991},
  editor = {Schwefel, H.-P. and M{\"a}nner, R.},
  volume = {496},
  pages = {198--208},
  publisher = {Springer Verlag},
  howpublished = {Lecture Notes in Computer Science}
}

@webpage{loihi2021lava,
  title = {Intel: Announcement of Loihi-2 and new software framework},
  howpublished = {\url{https://www.intel.com/content/www/us/en/newsroom/news/intel-unveils-neuromorphic-loihi-2-lava-software.html}},
  note = {\url{https://github.com/lava-nc/}},
  year = 2021
}

@ARTICLE{london2005dendritic,
  author = {London, M. and H{\"a}usser, M.},
  title = {Dendritic computation},
  journal = {Annu. Rev. Neurosci.},
  year = {2005},
  volume = {28},
  pages = {503--532},
  publisher = {Annual Reviews},
  doi = {10.1146/annurev.neuro.28.061604.135703}
}

@MISC{loock06diplomathesis,
  author = {Loock, Jan-Peter},
  title = {Evaluierung eines Floating Gate Analogspeichers für Neuronale Netze
	in Single-Poly UMC 180nm {CMOS}-Prozess},
  howpublished = {Diploma thesis (English), University of Heidelberg, HD-KIP-06-47},
  year = {2006},
  key = {loock06diplomathesis},
  keywords = {facets, floationg gate}
}

@ARTICLE{lovell97evaluation,
  author = {D. R. Lovell and T. Downs and A. C. Tsoi},
  title = {An evaluation of the Neocognitron},
  journal = {IEEE Transactions on Neural Networks},
  year = {1997},
  volume = {8},
  pages = {1098--1105},
  number = {5},
  keywords = {convolutional NN},
  owner = {fieres}
}

@ARTICLE{lundqvist2010bistable,
  author = {Lundqvist, Mikael and Compte, Albert and Lansner, Anders},
  title = {Bistable, Irregular Firing and Population Oscillations in a Modular
	Attractor Memory Network},
  journal = {PLoS Comput Biol},
  year = {2010},
  volume = {6},
  number = {6},
  month = {06},
  abstract = {The basic computational principles of the brain are still unknown,
	and one major reason for this is related to the difficulties in simultaneously
	measuring detailed data from a sufficiently large number of cells.
	In techniques where populations of cells are monitored, resolution
	is low. Computational models have no such measurement limitations
	and can be constrained by several experiments at different levels
	of granularity, enabling testing of the biological plausibility of
	different computational theories. One such theory, the attractor
	network paradigm, has gained increasing support over the past twenty
	years by, for instance, comparing the output of attractor memory
	models to population data and spike frequency modulations of neocortical
	neurons. We take this comparison further by also looking at the fine-structure
	of activity in a network model with a novel modular structure also
	seen <italic>in vivo</italic>. This allows the network to operate
	in a new dynamic regime. In particular, we reproduce the irregular
	low-rate spiking of single cells <italic>in vivo</italic>, which
	has previously been a challenge for attractor network models. Oscillations
	in field potentials at gamma and beta frequencies, again believed
	to be connected to, or even essential for, attention and consciousness,
	emerge as a feature of the underlying dynamics of the model.},
  publisher = {Public Library of Science}
}

@ARTICLE{lundqvist2006attractor,
  author = {Lundqvist, M. and Rehn, M. and Djurfeldt, M. and Lansner, A.},
  title = {Attractor dynamics in a modular network of neocortex},
  journal = {Network: Computation in Neural Systems},
  year = {2006},
  volume = {17:3},
  pages = {253-276},
  file = {:G\:\\matthias\\arbeit\\DISS\\pubsNdocs\\bench\\LUND06_AttractorDynamics.pdf:PDF},
  owner = {ehrlich NEUROBENCH KTH},
  timestamp = {2009.12.03}
}

@BOOK{lutz_python2001,
  title = {Programming Python: Object-Oriented Scripting},
  publisher = {O'Reilly \& Associates, Inc.},
  year = {2001},
  author = {Mark Lutz},
  address = {Sebastopol, CA, USA},
  note = {Foreword By-Guido Van Rossum},
  isbn = {0596000855}
}

@ARTICLE{leger05synaptic,
  author = {Jean-Fran{\c c}ois L{\'e}ger and Edward A Stern and Ad Aertsen and
	Detlef Heck},
  title = {Synaptic integration in rat frontal cortex shaped by network activity},
  journal = {J Neurophysiol},
  year = {2005},
  volume = {93},
  pages = {281--93},
  number = {1},
  month = {Jan},
  abstract = {Neocortical neurons in vivo are embedded in networks with intensive
	ongoing activity. How this network activity affects the neurons'
	integrative properties and what function this may imply at the network
	level remain largely unknown. Most of our knowledge regarding synaptic
	communication and integration is based on recordings in vitro, where
	network activity is strongly diminished or even absent. Here, we
	present results from two complementary series of experiments based
	on intracellular in vivo recordings in anesthetized rat frontal cortex.
	Specifically, we measured 1) the relationship between the excursions
	of a neuron's membrane potential and the spiking activity in the
	surrounding network and 2) how the summation of several inputs to
	a single neuron changes with the different levels of its membrane
	potential excursions and the associated states of network activity.
	The combination of these measurements enables us to assess how the
	level of network activity influences synaptic integration. We present
	direct evidence that integration of synaptic inputs in frontal cortex
	is linear, independent of the level of network activity. However,
	during periods of high network activity, the neurons' response to
	synaptic input is markedly reduced in both amplitude and duration.
	This results in a drastic shortening of its window for temporal integration,
	requiring more precise coordination of presynaptic spike discharges
	to reliably drive the neuron to spike under conditions of high network
	activity. We conclude that ongoing activity, as present in the active
	brain, emphasizes the need for neuronal cooperation at the network
	level, and cannot be ignored in the exploration of cortical function.},
  affiliation = {Department of Biology III, Albert-Ludwigs-University, Freiburg, Germany.}
}

@TECHREPORT{lecroyeres,
  author = {{L}e{C}roy},
  title = {ENHANCED RESOLUTION},
  institution = {{L}e{C}roy Corporation, 700 Chestnut Ridge Road, Chestnut Ridge,
	NY 10977-6499},
  number = {AN006A},
  url = {http://lecroygmbh.com}
}

@TECHREPORT{lecroy05remote,
  author = {{L}e{C}roy},
  title = {X-Stream Oscilloscopes - Remote Control Manual},
  institution = {{L}e{C}roy Corporation, 700 Chestnut Ridge Road, Chestnut Ridge,
	NY 10977-6499},
  year = {2005},
  number = {Revision D},
  url = {http://lecroygmbh.com}
}

@ARTICLE{maass97third,
  author = {W. Maass},
  title = {Networks of spiking neurons: the third generation of neural network
	models},
  journal = {Neural Networks},
  year = {1997},
  volume = {10},
  pages = {1659-1671}
}

@ARTICLE{maass2007,
  author = {Maass, Wolfgang and Joshi, Prashant and Sontag, Eduardo D. },
  title = {Computational Aspects of Feedback in Neural Circuits},
  journal = {PLoS Computational Biology},
  year = {2007},
  volume = {3},
  pages = {e165+},
  number = {1},
  month = {January},
  abstract = {It has previously been shown that generic cortical microcircuit models
	can perform complex real-time computations on continuous input streams,
	provided that these computations can be carried out with a rapidly
	fading memory. We investigate the computational capability of such
	circuits in the more realistic case where not only readout neurons,
	but in addition a few neurons within the circuit, have been trained
	for specific tasks. This is essentially equivalent to the case where
	the output of trained readout neurons is fed back into the circuit.
	We show that this new model overcomes the limitation of a rapidly
	fading memory. In fact, we prove that in the idealized case without
	noise it can carry out any conceivable digital or analog computation
	on time-varying inputs. But even with noise, the resulting computational
	model can perform a large class of biologically relevant real-time
	computations that require a nonfading memory. We demonstrate these
	computational implications of feedback both theoretically, and through
	computer simulations of detailed cortical microcircuit models that
	are subject to noise and have complex inherent dynamics. We show
	that the application of simple learning procedures (such as linear
	regression or perceptron learning) to a few neurons enables such
	circuits to represent time over behaviorally relevant long time spans,
	to integrate evidence from incoming spike trains over longer periods
	of time, and to process new information contained in such spike trains
	in diverse ways according to the current internal state of the circuit.
	In particular we show that such generic cortical microcircuits with
	feedback provide a new model for working memory that is consistent
	with a large set of biological constraints. Although this article
	examines primarily the computational role of feedback in circuits
	of neurons, the mathematical principles on which its analysis is
	based apply to a variety of dynamical systems. Hence they may also
	throw new light on the computational role of feedback in other complex
	biological dynamical systems, such as, for example, genetic regulatory
	networks.},
  citeulike-article-id = {1102906},
  doi = {http://dx.doi.org/10.1371/journal.pcbi.0020165},
  keywords = {bioinformatics-networks, network-control},
  posted-at = {2007-02-12 11:28:47},
  priority = {3},
  url = {http://dx.doi.org/10.1371/journal.pcbi.0020165}
}

@ARTICLE{maass04kernel,
  author = {Maass, W. and Natschlager, T. and Markram, H.},
  title = {Fading memory and kernel properties of generic cortical microcircuit
	models},
  journal = {J {P}hysiol {P}aris},
  year = {2004},
  volume = {98},
  pages = {315--30},
  number = {4-6},
  note = {Institute for Theoretical Computer Science, Technische Universitaet
	Graz, Austria. maass@igi.tugraz.at},
  abstract = {It is quite difficult to construct circuits of spiking neurons that
	can carry out complex computational tasks. On the other hand even
	randomly connected circuits of spiking neurons can in principle be
	used for complex computational tasks such as time-warp invariant
	speech recognition. This is possible because such circuits have an
	inherent tendency to integrate incoming information in such a way
	that simple linear readouts can be trained to transform the current
	circuit activity into the target output for a very large number of
	computational tasks. Consequently we propose to analyze circuits
	of spiking neurons in terms of their roles as analog fading memory
	and non-linear kernels, rather than as implementations of specific
	computational operations and algorithms. This article is a sequel
	to [W. Maass, T. Natschlager, H. Markram, Real-time computing without
	stable states: a new framework for neural computation based on perturbations,
	Neural Comput. 14 (11) (2002) 2531-2560, Online available as #130
	from: ], and contains new results about the performance of generic
	neural microcircuit models for the recognition of speech that is
	subject to linear and non-linear time-warps, as well as for computations
	on time-varying firing rates. These computations rely, apart from
	general properties of generic neural microcircuit models, just on
	capabilities of simple linear readouts trained by linear regression.
	This article also provides detailed data on the fading memory property
	of generic neural microcircuit models, and a quick review of other
	new results on the computational power of such circuits of spiking
	neurons.},
  affiliation = {EPFL},
  details = {http://infoscience.epfl.ch/record/117817},
  oai-id = {oai:infoscience.epfl.ch:117817},
  oai-set = {article},
  review = {REVIEWED},
  status = {PUBLISHED},
  unit = {LNMC}
}

@INBOOK{maass04models,
  chapter = {18},
  pages = {575-605},
  title = {Computational models for generic cortical microcircuits},
  publisher = {J. Feng},
  year = {2004},
  author = {W. Maass and T. Natschl{\"a}ger and H. Markram},
  number = {ISBN 1-58488-362-6},
  address = {Boca Raton},
  booktitle = {Computational Neuroscience: A Comprehensive Approach},
  howpublished = {Chapman \& Hall/CRC},
  key = {maass_149},
  keywords = {liquid}
}

@ARTICLE{maass04power,
  author = {W. Maass and T. Natschl{\"a}ger and H. Markram},
  title = {On the computational power of circuits of spiking neurons},
  journal = {Journal of Physiology (Paris)},
  year = {2004},
  volume = {(in press)},
  keywords = {liquid}
}

@ARTICLE{maass02realtime,
  author = {W. Maass and T. Natschl{\"a}ger and H. Markram},
  title = {Real-time Computing Without Stable States: A New Framework for Neural
	Computation Based on Perturbations},
  journal = {Neural Computation},
  year = {2002},
  volume = {14},
  pages = {2531-2560},
  number = {11},
  abstract = {A key challenge for neural modeling is to explain how a continuous
	stream of multi-modal input from a rapidly changing environment can
	be processed by stereotypical recurrent circuits of integrate-and-fire
	neurons in real-time. We propose a new framework for neural computation
	that provides an alternative to arevious approaches based on attractor
	neural networks. It is shown that the inherent transient dynamics
	of the high-dimensional dynamical system formed by a neural circuit
	may serve as a universal source of information about past stimuli,
	from which readout neurons can extract particular aspects needed
	for diverse tasks in real-time. Stable internal states are not required
	for giving a stable output, since transient internal states can be
	transformed by readout neurons into stable target outputs due to
	the high dimensionality of the dynamical system. Our approach is
	based on a rigorous computational model, the liquid state machine,
	that unlike Turing machines, does not require sequential transitions
	between discrete internal states. Like the Turing machine paradigm
	it allows for universal computational power under idealized conditions,
	but for real-time processing of time-varying input. The resulting
	new framework for neural computation has novel implications for the
	interpretation of neural coding, for the design of experiments and
	data-analysis in neurophysiology, and for neuromorphic engineering.},
  keywords = {liquid}
}

@ARTICLE{maass00neuralsystems,
  author = {Wolfgang Maass and Eduardo D. Sontag},
  title = {Neural systems as nonlinear filters},
  journal = {Neural Computation},
  year = {2000},
  volume = {12},
  pages = {2000}
}

@INPROCEEDINGS{maclin95combining,
  author = {Maclin, R. and Shavlik, J. W.},
  title = {Combining the predictions of multiple classifiers: {U}sing competitive
	learning to initialize neural networks},
  booktitle = {Proceedings of the Fourteenth International Joint Conference on Artificial
	Intelligence},
  year = {1995},
  pages = {524--530},
  address = {Montreal, Canada},
  file = {maclin95combining.pdf:maclin95combining.pdf:PDF}
}

@INPROCEEDINGS{madrenas99,
  author = {J. Madrenas and E. Alarcon and J. Cosp and J.M. Moreno},
  title = {{VLSI} design of a flexible-structure sequential mixed-signal neural
	processor},
  booktitle = {Proceedings of the 6th International Conference Mixed-Signal Design
	of Integrated Circuits and Systems (MIXDES'99), Kraków (Poland),
	June1999},
  year = {1999}
}

@INPROCEEDINGS{Madrenas2009,
  author = {Madrenas, J. and Moreno, J.M.},
  title = {Strategies in SIMD Computing for Complex Neural Bioinspired Applications},
  booktitle = {Adaptive Hardware and Systems, 2009. AHS 2009. NASA/ESA Conference
	on},
  year = {2009},
  pages = {376-381},
  doi = {10.1109/AHS.2009.31},
  keywords = {content-addressable storage;large-scale systems;neural chips;parallel
	processing;SIMD computing;complex neural bioinspired application;content-addressable
	storage;microchip processor;scalable multiprocessor architecture;Biological
	system modeling;Cells (biology);Computational modeling;Computer networks;Emulation;Hardware;Nanobioscience;Neural
	networks;Neurons;Very large scale integration},
  owner = {simon},
  timestamp = {2013.04.21}
}

@MISC{mahfoud95niching,
  author = {Mahfoud, S. W.},
  title = {Niching Methods for Genetic Algorithms},
  howpublished = {{\it PhD thesis}},
  year = {1995},
  address = {Urbana, IL, USA},
  file = {mahfoud95niching.pdf:mahfoud95niching.pdf:PDF},
  institution = {University of Illinois},
  number = {IlliGAL Report No. 95001}
}

@ARTICLE{Mahowald1991,
  author = {Mahowald, Misha and Douglas, Rodney},
  title = {A silicon neuron},
  journal = {Nature},
  year = {1991},
  volume = {354},
  pages = {515--518},
  number = {6354},
  month = {Dec},
  day = {26},
  doi = {10.1038/354515a0},
  url = {http://dx.doi.org/10.1038/354515a0}
}

@Article{mainen1995reliability,
  Author         = {Mainen, Zachary F and Sejnowski, Terrence J},
  Title          = {Reliability of spike timing in neocortical neurons},
  Journal        = {Science},
  Volume         = {268},
  Number         = {5216},
  Pages          = {1503--1506},
  year           = 1995
}

@INPROCEEDINGS{manderick_icga89parallel,
  author = {Manderick, B. and Spiessens, P.},
  title = {Fine-Grained Parallel Genetic Algorithms},
  booktitle = {Proceedings of the Third International Conference on Genetic Algorithms},
  year = {1989},
  editor = {Schaffer, J. D.},
  pages = {428--433},
  address = {San Francisco},
  publisher = {Morgan Kaufmann}
}

@ARTICLE{mangasarian90cancer,
  author = {Mangasarian, O. L. and Wolberg, W. H.},
  title = {Cancer diagnosis via linear programming},
  journal = {{SIAM} News},
  year = {1990},
  volume = {23},
  pages = {1--18},
  number = {5},
  month = Sep
}

@ARTICLE{marchand90sequential,
  author = {Marchand, M. and Golea, M. and Ruj\'an, P.},
  title = {A Convergence Theorem for Sequential Learning in Two-Layer Perceptrons},
  journal = {Europhysics Letters},
  year = {1990},
  volume = {11},
  pages = {487--492},
  file = {marchand90sequential.pdf:marchand90sequential.pdf:PDF}
}

@article{marder2006variability,
  title={Variability, compensation and homeostasis in neuron and network function},
  author={Marder, Eve and Goaillard, Jean-Marc},
  journal={Nature Reviews Neuroscience},
  volume={7},
  number={7},
  pages={563--574},
  year={2006},
  publisher={Nature Publishing Group}
}

@article{marder2011multiple,
  title={Multiple models to capture the variability in biological neurons and networks},
  author={Marder, Eve and Taylor, Adam L},
  journal={Nature Neuroscience},
  volume={14},
  number={2},
  pages={133--138},
  year={2011},
  publisher={Nature Publishing Group}
}

@article{marder2011variability,
  title={Variability, compensation, and modulation in neurons and circuits},
  author={Marder, Eve},
  journal={Proceedings of the National Academy of Sciences},
  volume={108},
  number={Supplement 3},
  pages={15542--15548},
  year={2011},
  publisher={National Acad Sciences}
}

@article{marinari1992simulated,
  title={Simulated tempering: a new Monte Carlo scheme},
  author={Marinari, Enzo and Parisi, Giorgio},
  journal={EPL (Europhysics Letters)},
  volume={19},
  number={6},
  pages={451},
  year={1992},
  publisher={IOP Publishing}
}

@inproceedings{markram1995action,
  title={Action potentials propagating back into dendrites trigger changes in efficacy of single-axon synapses between layer V pyramidal neurons},
  author={Markram, H and Sakmann, B},
  booktitle={Soc. Neurosci. Abstr},
  volume={21},
  pages={2007},
  year={1995}
}

@ARTICLE{markram2012human,
  author = {Markram, H.},
  title = {The Human Brain Project},
  journal = {Scientific American},
  year = {2012},
  volume = {306},
  pages = {50--55},
  number = {6},
  publisher = {Nature Publishing Group}
}

@article{Markram2011introducing,
title = "Introducing the Human Brain Project ",
journal = "Procedia Computer Science ",
volume = "7",
number = "",
pages = "39 - 42",
year = "2011",
note = "Proceedings of the 2nd European Future Technologies Conference and Exhibition 2011 (FET 11) ",
issn = "1877-0509",
doi = "http://dx.doi.org/10.1016/j.procs.2011.12.015",
url = "http://www.sciencedirect.com/science/article/pii/S1877050911006806",
author = "Henry Markram and Karlheinz Meier and Thomas Lippert and Sten Grillner and Richard Frackowiak and Stanislas Dehaene and Alois Knoll and Haim Sompolinsky and Kris Verstreken and Javier DeFelipe and Seth Grant and Jean-Pierre Changeux and Alois Saria",
keywords = "Human brain",
keywords = "neuroscience",
keywords = "neuroinformatics",
keywords = "modeling",
keywords = "simulation",
keywords = "supercomputing",
keywords = "HPC",
keywords = "medicine",
keywords = "neuromorphics",
keywords = "neuroprosthetics",
keywords = "neurorobotics ",
abstract = "The Human Brain Project (HBP) is a candidate project in the European Union's \{FET\} Flagship Program, funded by the \{ICT\} Program in the Seventh Framework Program. The project will develop a new integrated strategy for understanding the human brain and a novel research platform that will integrate all the data and knowledge we can acquire about the structure and function of the brain and use it to build unifying models that can be validated by simulations running on supercomputers. The project will drive the development of supercomputing for the life sciences, generate new neuroscientific data as a benchmark for modeling, develop radically new tools for informatics, modeling and simulation, and build virtual laboratories for collaborative basic and clinical studies, drug simulation and virtual prototyping of neuroprosthetic, neuromorphic, and robotic devices. "
}

@article{markram2015reconstruction,
title = "Reconstruction and Simulation of Neocortical Microcircuitry ",
journal = "Cell ",
volume = "163",
number = "2",
pages = "456 - 492",
year = "2015",
note = "",
issn = "0092-8674",
doi = "http://dx.doi.org/10.1016/j.cell.2015.09.029",
url = "http://www.sciencedirect.com/science/article/pii/S0092867415011915",
author = "Henry Markram and Eilif Muller and Srikanth Ramaswamy and Michael W. Reimann and Marwan Abdellah and Carlos Aguado Sanchez and Anastasia Ailamaki and Lidia Alonso-Nanclares and Nicolas Antille and Selim Arsever and Guy Antoine Atenekeng Kahou and Thomas K. Berger and Ahmet Bilgili and Nenad Buncic and Athanassia Chalimourda and Giuseppe Chindemi and Jean-Denis Courcol and Fabien Delalondre and Vincent Delattre and Shaul Druckmann and Raphael Dumusc and James Dynes and Stefan Eilemann and Eyal Gal and Michael Emiel Gevaert and Jean-Pierre Ghobril and Albert Gidon and Joe W. Graham and Anirudh Gupta and Valentin Haenel and Etay Hay and Thomas Heinis and Juan B. Hernando and Michael Hines and Lida Kanari and Daniel Keller and John Kenyon and Georges Khazen and Yihwa Kim and James G. King and Zoltan Kisvarday and Pramod Kumbhar and Sébastien Lasserre and Jean-Vincent Le Bé and Bruno R.C. Magalhães and Angel Merchán-Pérez and Julie Meystre and Benjamin Roy Morrice and Jeffrey Muller and Alberto Muñoz-
Céspedes and Shruti Muralidhar and Keerthan Muthurasa and Daniel Nachbaur and Taylor H. Newton and Max Nolte and Aleksandr Ovcharenko and Juan Palacios and Luis Pastor and Rodrigo Perin and Rajnish Ranjan and Imad Riachi and José-Rodrigo Rodríguez and Juan Luis Riquelme and Christian Rössert and Konstantinos Sfyrakis and Ying Shi and Julian C. Shillcock and Gilad Silberberg and Ricardo Silva and Farhan Tauheed and Martin Telefont and Maria Toledo-Rodriguez and Thomas Tränkler and Werner Van Geit and Jafet Villafranca Díaz and Richard Walker and Yun Wang and Stefano M. Zaninetta and Javier DeFelipe and Sean L. Hill and Idan Segev and Felix Schürmann",
abstract = "Summary We present a first-draft digital reconstruction of the microcircuitry of somatosensory cortex of juvenile rat. The reconstruction uses cellular and synaptic organizing principles to algorithmically reconstruct detailed anatomy and physiology from sparse experimental data. An objective anatomical method defines a neocortical volume of 0.29 ± 0.01 mm3 containing ∼31,000 neurons, and patch-clamp studies identify 55 layer-specific morphological and 207 morpho-electrical neuron subtypes. When digitally reconstructed neurons are positioned in the volume and synapse formation is restricted to biological bouton densities and numbers of synapses per connection, their overlapping arbors form ∼8 million connections with ∼37 million synapses. Simulations reproduce an array of in vitro and in vivo experiments without parameter tuning. Additionally, we find a spectrum of network states with a sharp transition from synchronous to asynchronous activity, modulated by physiological mechanisms. The spectrum
of network states, dynamically reconfigured around this transition, supports diverse information processing strategies. PaperClip Video Abstract "
}

@article{markram2004interneurons,
  title={Interneurons of the neocortical inhibitory system},
  author={Markram, Henry and Toledo-Rodriguez, Maria and Wang, Yun and Gupta, Anirudh and Silberberg, Gilad and Wu, Caizhi},
  journal={Nature Reviews Neuroscience},
  volume={5},
  number={10},
  pages={793--807},
  year={2004},
  publisher={Nature Publishing Group}
}

@ARTICLE{markram2006blue,
  author = {Markram, Henry},
  title = {The Blue Brain Project},
  journal = {Nature Reviews Neuroscience},
  year = {2006},
  volume = {7},
  pages = {153--160},
  number = {2},
  owner = {simon},
  publisher = {Nature Publishing Group},
  timestamp = {2013.01.23}
}

@ARTICLE{Markram2011,
  author = {Markram, Henry and Gerstner, Wulfram and Sj{\"o}str{\"o}m, Per Jesper},
  title = {A history of spike-timing-dependent plasticity},
  journal = {Frontiers in synaptic neuroscience},
  year = {2011},
  volume = {3},
  owner = {simon},
  publisher = {Frontiers Media SA},
  timestamp = {2013.04.24}
}

@ARTICLE{markram98information,
  author = {Markram, H. and Gupta, A. and Uziel, A. and Wang, Y. and Tsodyks,
	M.},
  title = {Information processing with frequency-dependent synaptic connections.},
  journal = {Neurobiol Learn Mem},
  year = {1998},
  volume = {70},
  pages = {101--112},
  number = {1-2},
  abstract = {The efficacy of synaptic transmission between two neurons changes
	as a function of the history of previous activations of the synaptic
	connection. This history dependence can be characterized by examining
	the dependence of transmission on the frequency of stimulation. In
	this framework synaptic plasticity can also be examined in terms
	of changes in the frequency dependence of transmission and not merely
	in terms of synaptic strength which constitutes only a linear scaling
	mechanism. Recent work shows that the frequency dependence of transmission
	determines the content of information transmitted between neurons
	and that synaptic modifications can change the content of information
	transmitted. Multipatch-clamp recordings revealed that the frequency
	dependence of transmission is potentially unique for each synaptic
	connection made by a single axon and that the class of pre-postsynaptic
	neuron determines the class of frequency dependence (activity independent),
	while the unique activity relationship between any two neurons could
	determine the precise values of the parameters within a specific
	class (activity dependent). The content of information transmitted
	between neurons is also formalized to provide synaptic transfer functions
	which can be used to determine the role of the synaptic connection
	within a network of neurons. It is proposed that deriving synaptic
	transfer functions is crucial in order to understand the link between
	synaptic transmission and information processing within networks
	of neurons and to understand the link between synaptic plasticity
	and learning and memory.},
  address = {Department of Neurobiology, The Weizmann Institute for Science, Rehovot,
	76100, Israel. bnmark@weizmann.weizmann.ac.il},
  keywords = {connectivity, learning, memory, synaptic\_dynamics, synaptic\_plasticity}
}

@ARTICLE{markram97regulation,
  author = {H. Markram and J. L{\"u}bke and M. Frotscher and B. Sakmann},
  title = {Regulation of Synaptic Efficacy By Coincidence of Postsynaptic Aps.},
  journal = {Science},
  year = {1997},
  volume = {275},
  pages = {213-215}
}

@ARTICLE{markram04,
  author = {Markram, Henry and Toledo-Rodriguez, Maria and Wang, Yun and Gupta,
	Anirudh and Silberberg, Gilad and Wu, Caizhi},
  title = {Interneurons of the neocortical inhibitory system},
  journal = {Nat Rev Neurosci},
  year = {2004},
  volume = {5},
  pages = {793--807},
  number = {10},
  month = {Oct},
  doi = {10.1038/nrn1519},
  issn = {1471-003X},
  url = {http://dx.doi.org/10.1038/nrn1519}
}

@ARTICLE{markram98differential,
  author = {Markram, H. and Wang, Y. and Tsodyks, M.},
  title = {Differential signaling via the same axon of neocortical pyramidal
	neurons.},
  journal = {Proceedings of the National Academy of Sciences of the United States
	of America},
  year = {1998},
  volume = {95},
  pages = {5323--5328},
  number = {9},
  month = apr,
  abstract = {The nature of information stemming from a single neuron and conveyed
	simultaneously to several hundred target neurons is not known. Triple
	and quadruple neuron recordings revealed that each synaptic connection
	established by neocortical pyramidal neurons is potentially unique.
	Specifically, synaptic connections onto the same morphological class
	differed in the numbers and dendritic locations of synaptic contacts,
	their absolute synaptic strengths, as well as their rates of synaptic
	depression and recovery from depression. The same axon of a pyramidal
	neuron innervating another pyramidal neuron and an interneuron mediated
	frequency-dependent depression and facilitation, respectively, during
	high frequency discharges of presynaptic action potentials, suggesting
	that the different natures of the target neurons underlie qualitative
	differences in synaptic properties. Facilitating-type synaptic connections
	established by three pyramidal neurons of the same class onto a single
	interneuron, were all qualitatively similar with a combination of
	facilitation and depression mechanisms. The time courses of facilitation
	and depression, however, differed for these convergent connections,
	suggesting that different pre-postsynaptic interactions underlie
	quantitative differences in synaptic properties. Mathematical analysis
	of the transfer functions of frequency-dependent synapses revealed
	supra-linear, linear, and sub-linear signaling regimes in which mixtures
	of presynaptic rates, integrals of rates, and derivatives of rates
	are transferred to targets depending on the precise values of the
	synaptic parameters and the history of presynaptic action potential
	activity. Heterogeneity of synaptic transfer functions therefore
	allows multiple synaptic representations of the same presynaptic
	action potential train and suggests that these synaptic representations
	are regulated in a complex manner. It is therefore proposed that
	differential signaling is a key mechanism in neocortical information
	processing, which can be regulated by selective synaptic modifications.},
  address = {Department of Neurobiology, The Weizmann Institute for Science, Rehovot,
	76100, Israel. bnmarkATweizmann.weizmann.ac.il},
  citeulike-article-id = {2933097},
  issn = {0027-8424},
  keywords = {efficacity, model, synapse, synaptic},
  posted-at = {2008-07-06 00:50:47},
  priority = {0},
  url = {http://view.ncbi.nlm.nih.gov/pubmed/9560274}
}

@ARTICLE{markram04interneurons,
  author = {Markram, Henry and Toledo-Rodriguez, Maria and Wang, Yun and Gupta,
	Anirudh and Silberberg, Gilad and Wu, Caizhi},
  title = {Interneurons of the neocortical inhibitory system},
  journal = {Nat Rev Neurosci},
  year = {2004},
  volume = {5},
  pages = {793--807},
  number = {10},
  month = {Oct},
  doi = {10.1038/nrn1519},
  issn = {1471-003X},
  url = {http://dx.doi.org/10.1038/nrn1519}
}

@ARTICLE{marshall95adaptive,
  author = {J. A. Marshall},
  title = {Adaptive Perceptual Pattern Recognition by Self-Organizing Neural
	Networks: Context, Uncertainty, Multiplicity, and Scale},
  journal = {Neural Networks},
  year = {1995},
  volume = {8},
  pages = {335--362},
  number = {3},
  abstract = {A new context-sensitive neural network, called an "EXIN" (excitatory+inhibitory)
	network, is described. EXIN networks self-organize in complex perceptual
	environments, in the presence of multiple superimposed patterns,
	multiple scales, and uncertainty. The networks use a new inhibitory
	learning rule, in addition to an excitatory learning rule, to allow
	superposition of multiple simultaneous neural activations (multiple
	winners), under strictly regulated circumstances, instead of forcing
	winner-take-all pattern classifications. The multiple activations
	represent uncertainty or multiplicity in perception and pattern recognition.
	Perceptual scission (breaking of linkages) between independent category
	groupings thus arises and allows effective global contextsensitive
	segmentation constraint satisfaction, and exclusive credit attribution.
	A Weber Law neuron-growth rule lets the network learn and classify
	input patterns despite variations in their spatial scale. Applications
	of the new techn...},
  annote = {Inhibitory Learning},
  citeseerurl = {citeseer.nj.nec.com/marshall95adaptive.html},
  content = {a inhibitory learning rule: whenever a neuron is active, its output
	inhibitory connections to other active neurons become gradually
	stronger (i.e. more inhibitory), while its output inhibitory connections
	to inactive neurons become gradually weaker.},
  keywords = {learning}
}

@MISC{marshall98generalization,
  author = {J. Marshall and V. Gupta},
  title = {Generalization and exclusive allocation of credit in unsupervised
	category learning},
  year = {1998},
  abstract = {A new way of measuring generalization in unsupervised learning is
	presented. The measure is based on an exclusive allocation, or credit
	assignment, criterion. In a classifier that satisfies the criterion,
	input patterns are parsed so that the credit for each input feature
	is assigned exclusively to one of multiple, possibly overlapping,
	output categories. Such a classifier achieves context-sensitive,
	global representations of pattern data. Two additional constraints,
	sequence masking and uncertainty multiplexing, are described; these
	can be used to refine the measure of generalization. The generalization
	performance of EXIN networks, winner-take-all competitive learning
	networks, linear decorrelator networks, and Nigrin?s SONNET-2 network
	are compared.},
  citeseerurl = {citeseer.nj.nec.com/marshall98generalization.html},
  keywords = {learning},
  text = {Marshall, J.A. & Gupta,V.S. (1998) Generalization and exclusive allocation
	of credit in unsupervised category learning. Network: Computation
	in Neural Systems, 9, 279--302.}
}

@article{marsili2020implementation,
  title = {Implementation and validation of real-time algorithms for atrial fibrillation detection on a wearable {ECG} device},
  journal = {Computers in Biology and Medicine},
  volume = 116,
  pages = 103540,
  year = 2020,
  issn = {0010-4825},
  doi = {https://doi.org/10.1016/j.compbiomed.2019.103540},
  author = {Italo Agustin Marsili and Luca Biasiolli and Michela Mas\`{e} and Alberto Adami and Alberto Oliver Andrighetti and Flavia Ravelli and Giandomenico Nollo}
}

@ARTICLE{Martin2000,
  author = {Stephen Martin and P. D. Grimwood and R. G. M. Morris},
  title = {Synaptic Plasticity and Memory: An Evaluation of the Hypothesis},
  journal = {Annual Review of Neuroscience},
  year = {2000},
  volume = {23},
  pages = {649-711},
  month = {March},
  doi = {10.1146/annurev.neuro.23.1.649},
  owner = {simon},
  timestamp = {2012.12.14}
}

@INPROCEEDINGS{masa1999moneypen,
  author = {P. Masa and P. Heim and E. Franzi and others},
  title = {10 mW CMOS retina and classifier for handheld, 1000 images/s optical
	character recognition system},
  booktitle = {Proceedings of the IEEE International Solid-State Circuit Conference},
  year = {1999},
  pages = {202--},
  note = {see also\\ {\tt http://www.csem.ch/detailed/pdf/m\_111\_Handheld~OCR~for~eBanking.pdf}},
  file = {masa1999moneypen.pdf:masa1999moneypen.pdf:PDF},
  owner = {fieres},
  url = {http://www.csem.ch/detailed/pdf/m\_111\_Handheld~OCR~for~eBanking.pdf}
}

@ARTICLE{masa1994analogNN,
  author = {Peter Masa and Klaas Hoen and Hans Wallinga},
  title = {A High-Speed Analog Neural Processor},
  journal = {IEEE Micro},
  year = {1994},
  volume = {14},
  pages = {40-50},
  number = {3},
  file = {masa1994analogNN.pdf:masa1994analogNN.pdf:PDF},
  owner = {fieres}
}

@article{masquelier2013network,
  title={Network Bursting Dynamics in Excitatory Cortical Neuron Cultures Results from the Combination of Different Adaptive Mechanism},
  author={Masquelier, Timoth{\'e}e and Deco, Gustavo},
  journal={PLOS ONE},
  volume={8},
  number={10},
  pages={e75824},
  year={2013},
  publisher={Public Library of Science}
}

@BOOK{massie2012monitoring,
  author = {Massie, M. and Li, B. and Nicholes, B. and Vuksan, V.},
  isbn = {9781449329709},
  publisher = {O'Reilly Media, Incorporated},
  series = {Oreilly and Associate Series},
  title = {Monitoring with Ganglia},
  url = {http://books.google.de/books?id=w4LLpXeVCbkC},
  year = 2012
}

@MISC{matplotlib_homepage,
  author = {Matplotlib},
  title = {Website},
  howpublished = {\url{http://matplotlib.sourceforge.net}},
  year = {2008}
}

@mastersthesis{mauch2016masterthesis,
  author   = {Christian Mauch},
  title    = {Commissioning of a Neuromorphic Computing Platform},
  school   = {Universit\"at Heidelberg},
  year     = 2016,
  type     = {Masterthesis}
}

@ARTICLE{mauch2020resource,
	author = {Mauch, Christian and Breitwieser, Oliver Julien and M{\"u}ller, Eric and Schmitt, Sebastian and Schemmel, Johannes},
	title = {Resource Management for the Neuromorphic {BrainScaleS} Platform},
	year = 2020,
	journal={arXiv preprint},
	url={TODO}
}

@ARTICLE{matsumura_1996,
  author = {Matsumura, Michikazu and Chen, Dao-{F}en and Sawaguchi, Toshiyuki
	and Kubota, Kisou and Fetz, Eberhard E.},
  title = {{Synaptic Interactions between Primate Precentral Cortex Neurons
	Revealed by Spike-Triggered Averaging of Intracellular Membrane Potentials
	In Vivo}},
  journal = {J. Neurosci.},
  year = {1996},
  volume = {16},
  pages = {7757-7767},
  number = {23},
  eprint = {http://www.jneurosci.org/cgi/reprint/16/23/7757.pdf}
}

@article{mayr2019spinnaker,
  title={Spinnaker 2: A 10 million core processor system for brain simulation and machine learning},
  author={Mayr, Christian and Hoeppner, Sebastian and Furber, Steve},
  journal={arXiv preprint arXiv:1911.02385},
  year=2019
}

@MISC{mccanne1993,
  author = {Mccanne, Steven and Jacobson, Van},
  title = {The BSD Packet Filter: A new architecture for user-level packet capture},
  year = {1993},
  keywords = {bpf, deos, extension, os},
  pages = {259--269},
  posted-at = {2008-12-31 13:44:25},
  priority = {2}
}

@ARTICLE{mccormick93,
  author = {D.A. McCormick and Z. Wang and J.R. Huguenard},
  title = {Neurotransmitter control of neocortical neuronal activity and excitability},
  journal = {Cerebral Cortex},
  year = {1993},
  volume = {3},
  pages = {387-398.},
  keywords = {spiking}
}

@ARTICLE{mcculloch43neuron,
  author = {McCulloch, Warren S. and Pitts, Walter},
  title = {A logical calculus of the ideas immanent in nervous activity},
  journal = {Bulletin of Mathematical Biophysics},
  year = {1943},
  pages = {127--147},
  annote = {first paper about the McCulloch-Pitts neuron from the masters themselves}
}

@ARTICLE{mcquoid93ensembles,
  author = {McQuoid, M. R. J.},
  title = {Neural Ensembles: {S}imultaneous Recognition of Multiple {2-D} Visual
	Objects},
  journal = {Neural Networks},
  year = {1993},
  volume = {6},
  pages = {907-917},
  keywords = {convolutional NN}
}

@ARTICLE{mead90neuromorphic,
  author = {Mead, C. A.},
  title = {Neuromorphic Electronic Systems},
  journal = {Proceedings of the IEEE},
  year = {1990},
  volume = {78},
  pages = {1629-1636}
}

@BOOK{mead89analog,
  title = {Analog {VLSI} and Neural Systems},
  publisher = {Addison Wesley},
  year = {1989},
  author = {Mead, C. A.},
  address = {Reading, MA},
  keywords = {vlsi}
}

@ARTICLE{mead88silicon,
  author = {Mead, Carver A. and Mahowald, M. A.},
  title = {A silicon model of early visual processing},
  journal = {Neural Networks},
  year = {1988},
  volume = {1},
  pages = {91--97},
  number = {1},
  keywords = {vlsi}
}

@ARTICLE{mehring03inference,
  author = {Carsten Mehring and J{\"o}rn Rickert and Eilon Vaadia and Simone
	Cardoso de Oliveira and Ad Aertsen and S. Rotter},
  title = {Inference of hand movements from local field potentials in monkey
	motor cortex},
  journal = {Nat. Neurosci.},
  year = {2003},
  volume = {6},
  pages = {1253--1254},
  number = {12}
}

@ARTICLE{mel1997SEEMORE,
  author = {Bartlett W. Mel},
  title = {SEEMORE: Combining Color, Shape, and Texture Histogramming in a Neurally
	Inspired Approach to Visual Object Recognition},
  journal = {Neural Computation},
  year = {1997},
  volume = {9},
  pages = {777--804},
  file = {mel1997SEEMORE.pdf:mel1997SEEMORE.pdf:PDF},
  owner = {fieres}
}

@INPROCEEDINGS{Mensi2011,
  author = {Skander Mensi and Richard Naud and Wulfram Gerstner},
  title = {From Stochastic Nonlinear Integrate-and-Fire to Generalized Linear
	Models},
  booktitle = {Advances in Neural Information Processing Systems},
  year = {2011},
  owner = {simon},
  timestamp = {2012.12.05}
}

@INPROCEEDINGS{merolla2006dynamic,
  author = {Paul A. Merolla and Kwabena Boahen},
  title = {Dynamic computation in a recurrent network of heterogeneous silicon
	neurons},
  booktitle = {Proceedings of the 2006 IEEE International Symposium on Circuits
	and Systems (ISCAS 2006)},
  year = {2006},
  keywords = {neuromorphic},
  owner = {bruederl},
  timestamp = {2008.09.02}
}

@article{merolla2014million,
  title={A million spiking-neuron integrated circuit with a scalable communication network and interface},
  author={Merolla, Paul A. and Arthur, John V. and Alvarez-Icaza, Rodrigo and Cassidy, Andrew S. and Sawada, Jun and Akopyan, Filipp and Jackson, Bryan L and Imam, Nabil and Guo, Chen and Nakamura, Yutaka and others},
  journal={Science},
  volume=345,
  number=6197,
  pages={668--673},
  year=2014,
  publisher={American Association for the Advancement of Science},
  doi = {10.1126/science.1254642},
}

@ARTICLE{metropolis53equation,
  author = {Metropolis, N. and Rosenbluth, A. W. and Rosenbluth, M. N. and Theller,
	A. H.},
  title = {Equation of state calculations by fast computing machines},
  journal = {Journal of Chemical Physics},
  year = {1953},
  volume = {21},
  pages = {1087--1092},
  number = {6}
}

@BOOK{michalewicz99genetic,
  title = {Genetic Algorithms {+} Data Structures {=} Evolution Programms},
  publisher = {Springer Verlag},
  year = {1999},
  author = {Michalewicz, Z.},
  address = {Berlin, Heidelberg, New York},
  isbn = {3-540-60676-9}
}

@ARTICLE{micikevicius2017mixed,
  title={Mixed precision training},
  author={Micikevicius, Paulius and Narang, Sharan and Alben, Jonah and Diamos, Gregory and Elsen, Erich and Garcia, David and Ginsburg, Boris and Houston, Michael and Kuchaiev, Oleksii and Venkatesh, Ganesh and others},
  journal={arXiv preprint arXiv:1710.03740},
  year={2017},
  url={https://arxiv.org/abs/1710.03740}
}

@ARTICLE{mihalas2009generalized,
  author = {Mihalas, S. and Niebur, E.},
  title = {A generalized linear integrate-and-fire neural model produces diverse
	spiking behaviors},
  journal = {Neural computation},
  year = {2009},
  volume = {21},
  pages = {704--718},
  number = {3},
  publisher = {MIT Press}
}

@INPROCEEDINGS{miller89designing,
  author = {Miller, G. F. and Todd, P. M.},
  title = {Designing Neural Networks using Genetic Algorithms},
  booktitle = {Proceedings of the Third International Conference on Genetic Algorithms},
  year = {1989},
  editor = {Schaffer, J. D.},
  pages = {379--384},
  publisher = {Morgan Kaufmann}
}

@MISC{millner13oral,
  author = {Sebastian Millner},
  title = {Personal Communication},
  year = {2013},
  owner = {simon},
  timestamp = {2013.04.17}
}

@PHDTHESIS{Millner2012,
  author = {Sebastian Millner},
  title = {Development of a Multi-Compartment Neuron Model Emulation},
  year = 2012,
  month = {November},
  owner = {simon},
  timestamp = {2013.04.03},
  url = {http://www.ub.uni-heidelberg.de/archiv/13979},
  school = {Ruprecht-Karls University Heidelberg}
}

@MISC{millner08diploma,
  author = {Sebastian Millner},
  title = {An Integrated Operational Amplifier for a Large Scale Neuromorphic
	System},
  howpublished = {Diploma thesis, University of Heidelberg, HD-KIP-08-19},
  year = {2008},
  keywords = {vision}
}

@INPROCEEDINGS{millner10, crossref = {millner2010vlsi}}
@INPROCEEDINGS{millner2010vlsi,
  author = {Sebastian Millner and Andreas Gr\"{u}bl and Karlheinz Meier and Johannes
	Schemmel and Marc-Olivier Schwartz},
  title = {A {VLSI} Implementation of the Adaptive Exponential Integrate-and-Fire
	Neuron Model},
  booktitle = {Advances in Neural Information Processing Systems 23},
  year = 2010,
  editor = {Lafferty, J. and C. K. I. Williams and J. Shawe-Taylor and R.S. Zemel
	and A. Culotta},
  pages = {1642--1650}
}

@INPROCEEDINGS{millner12esann,
  author = {Sebastian Millner and Andreas Hartel and Johannes Schemmel and Karlheinz
	Meier},
  title = {Towards biologically realistic multi-compartment neuron model emulation
	in analog {VLSI}},
  booktitle = {Proceedings ESANN 2012},
  year = {2012}
}

@MISC{minsky54neural,
  author = {Minsky, M.},
  title = {Neural Nets and the Brain Model Problem},
  howpublished = {{\it PhD thesis}},
  year = {1954},
  institution = {University of Princeton}
}

@BOOK{minsky69perceptrons,
  title = {Perceptrons},
  publisher = {MIT Press},
  year = {1969},
  author = {Minsky, Marvin and Papert, Seymour},
  address = {Cambridge, MA}
}

@PHDTHESIS{Misra1992parallelNN,
  author = {M. Misra},
  title = {Implementation of neural networks on parallel architectures},
  school = {Univ. of Southern California},
  year = {1992},
  file = {Misra1992parallelNN.ps.gz:Misra1992parallelNN.ps.gz:PDF},
  owner = {fieres}
}

@BOOK{mitchell97machinelearning,
  title = {Machine Learning},
  publisher = {The McGraw-Hill companies, Inc.},
  year = {1997},
  author = {Mitchell, T. M.},
  address = {USA},
  isbn = {0-07-115467-1},
  keywords = {learning}
}

@ARTICLE{mitra09realtime,
  author = {Mitra, S. and Fusi, S. and Indiveri, G.},
  title = {Real-Time Classification of Complex Patterns Using Spike-Based Learning
	in Neuromorphic {VLSI}},
  journal = {{IEEE} Transactions on Biomedical Circuits and Systems},
  year = {2009},
  volume = {3:(1)},
  pages = {32-42},
  publisher = {IEEE Computer Society}
}

@ARTICLE{Mitra2009,
  author = {Mitra, S. and Fusi, S. and Indiveri, G.},
  title = {Real-Time Classification of Complex Patterns Using Spike-Based Learning
	in Neuromorphic VLSI},
  journal = {Biomedical Circuits and Systems, IEEE Transactions on},
  year = {2009},
  volume = {3},
  pages = {32-42},
  number = {1},
  doi = {10.1109/TBCAS.2008.2005781},
  issn = {1932-4545},
  keywords = {VLSI;biocybernetics;bioelectric phenomena;learning (artificial intelligence);neural
	nets;neurophysiology;pattern classification;VLSI neural network;dynamic
	synapse;input spike-train;mean firing rate;neural activity patterns;neuromorphic
	VLSI;output neuron;real time complex pattern classification;spike
	based learning;spike driven plasticity mechanism;spike train pattern
	classification;spiking neuron VLSI network;spiking neuron networks;supervised
	learning rule;teacher signal;Classification;learning;neuromorphic
	VLSI;silicon neuron;silicon synapse;spike-based plasticity;synaptic
	dynamics},
  owner = {simon},
  timestamp = {2013.05.03}
}

@INPROCEEDINGS{moerland97hardware,
  author = {Moerland, P. and Fiesler, E.},
  title = {Neural Network Adaptions to Hardware Implementations},
  booktitle = {The Handbook of Neural Computation},
  year = {1997},
  editor = {Fiesler, E. and Beale, R.},
  address = {New York},
  month = {January},
  publisher = {Institute of Physics Publishing and Oxford University Publishing},
  chapter = {E1.2},
  keywords = {NN, vlsi}
}

@article{mohr2003mormyromast1,
  title={The mormyromast region of the mormyrid electrosensory lobe. I. Responses to corollary discharge and electrosensory stimuli},
  author={Mohr, Claudia and Roberts, Patrick D and Bell, Curtis C},
  journal={Journal of neurophysiology},
  volume={90},
  number={2},
  pages={1193--1210},
  year={2003},
  publisher={Am Physiological Soc}
}

@article{mohr2003mormyromast2,
  title={The mormyromast region of the mormyrid electrosensory lobe. II. Responses to input from central sources},
  author={Mohr, Claudia and Roberts, Patrick D and Bell, Curtis C},
  journal={Journal of neurophysiology},
  volume={90},
  number={2},
  pages={1211--1223},
  year={2003},
  publisher={Am Physiological Soc}
}

@INPROCEEDINGS{moloney2014myriad2,
  author = {Moloney, David and Barry, Brendan and Richmond, Richard and Connor, Fergal and Brick, Cormac and Donohoe, David},
  title = {Myriad 2: Eye of the computational vision storm},
  booktitle = {2014 IEEE Hot Chips 26 Symposium (HCS)},
  year = 2014,
  pages = {1--18},
  doi = {10.1109/HOTCHIPS.2014.7478823}
}

@ARTICLE{montalvo97,
  author = {Montalvo, J. and Gyurcsik R. and Paulos J.},
  title = {An Analog {VLSI} Neural Network with On-Chip Perturbation Learning},
  journal = {IEEE Journal of Solid-State Circuits},
  year = {1997},
  volume = {32},
  pages = {535--543},
  number = {4},
  month = {April},
  keywords = {vlsi}
}

@INPROCEEDINGS{montana_ijcai89training,
  author = {Montana, D. and Davis, L.},
  title = {Training feedforward neural networks using genetic algorithms},
  booktitle = {Proceedings of the 11th International Joint Conference on Artificial
	Intelligence},
  year = {1989},
  pages = {762--767},
  address = {San Mateo, CA},
  publisher = {Morgan Kaufmann}
}

@ARTICLE{moody89radial,
  author = {Moody, J. and Darken, C. J.},
  title = {Fast Learning in Networks of Locally-Tuned Processing Units},
  journal = {Neural Computation},
  year = {1989},
  volume = {1},
  pages = {281--294}
}

@ARTICLE{moore65cramming,
  author = {Moore, G. E.},
  title = {Cramming more components onto integrated circuits},
  journal = {Electronics},
  year = {1965},
  volume = {38},
  number = {8},
  month = {April}
}

@article{moreno2004role,
  title={Role of synaptic filtering on the firing response of simple model neurons},
  author={Moreno-Bote, Rub{\'e}n and Parga, N{\'e}stor},
  journal={Physical Review Letters},
  volume={92},
  number={2},
  pages={028102},
  year={2004},
  publisher={APS}
}

@ARTICLE{moriarty95othello,
  author = {Moriarty, D. and Miikkulainen, R.},
  title = {Discovering complex othello strategies through evolutionary neural
	networks},
  journal = {Connection Science},
  year = {1995},
  volume = {7},
  pages = {195--210},
  number = {3/4},
  file = {moriarty95othello.pdf:moriarty95othello.pdf:PDF}
}

@article{morris1981voltage,
  title={Voltage oscillations in the barnacle giant muscle fiber},
  author={Morris, Catherine and Lecar, Harold},
  journal={Biophysical journal},
  volume={35},
  number={1},
  pages={193--213},
  year={1981},
  publisher={Elsevier}
}

@ARTICLE{morrison07stdp,
  author = {Morrison, Abigail and Aertsen, Ad and Diesmann, Markus},
  title = {{Spike-Timing-Dependent Plasticity in Balanced Random Networks}},
  journal = {Neural Comp.},
  year = {2007},
  volume = {19},
  pages = {1437-1467},
  number = {6},
  abstract = {The balanced random network model attracts considerable interest because
	it explains the irregular spiking activity at low rates and large
	membrane potential fluctuations exhibited by cortical neurons in
	vivo. In this article, we investigate to what extent this model is
	also compatible with the experimentally observed phenomenon of spike-timing-dependent
	plasticity (STDP). Confronted with the plethora of theoretical models
	for STDP available, we reexamine the experimental data. On this basis,
	we propose a novel STDP update rule, with a multiplicative dependence
	on the synaptic weight for depression, and a power law dependence
	for potentiation. We show that this rule, when implemented in large,
	balanced networks of realistic connectivity and sparseness, is compatible
	with the asynchronous irregular activity regime. The resultant equilibrium
	weight distribution is unimodal with fluctuating individual weight
	trajectories and does not exhibit development of structure. We investigate
	the robustness of our results with respect to the relative strength
	of depression. We introduce synchronous stimulation to a group of
	neurons and demonstrate that the decoupling of this group from the
	rest of the network is so severe that it cannot effectively control
	the spiking of other neurons, even those with the highest convergence
	from this group.},
  eprint = {http://neco.mitpress.org/cgi/reprint/19/6/1437.pdf},
  keywords = {plasticity spiking}
}

@ARTICLE{morrison08_stdp,
  author = {Morrison, Abigail and Diesmann, Markus and Gerstner, Wulfram},
  title = {Phenomenological models of synaptic plasticity based on spike timing},
  journal = {Biological Cybernetics},
  year = {2008},
  volume = {98},
  pages = {459--478},
  number = {6},
  month = {June},
  citeulike-article-id = {2835349},
  doi = {10.1007/s00422-008-0233-1},
  issn = {0340-1200},
  keywords = {ltd, ltp, pasticity, review, spiking-neuron, stdp},
  posted-at = {2008-07-08 20:00:22},
  priority = {4},
  publisher = {Springer}
}

@ARTICLE{morrison05distributed,
  author = {Morrison, Abigail and Mehring, Carsten and Geisel, Theo and Aertsen,
	Ad and Diesmann, Markus},
  title = {Advancing the boundaries of high connectivity network simulation
	with distributed computing},
  journal = {Neural Comput.},
  year = {2005},
  volume = {17},
  pages = {1776--1801},
  number = {8}
}

@ARTICLE{mostafa89vcdim,
  author = {Mostafa, Y. S.},
  title = {The Vapnik-Chervonenkis Dimension: Information versus Complexity
	in Learning},
  journal = {Neural Computation},
  year = {1989},
  volume = {1},
  pages = {312--317}
}

@ARTICLE{mostafa85capacity,
  author = {Mostafa, Y. S. and St. Jaques, J.},
  title = {Information Capacity of the Hopfield Model},
  journal = {IEEE Transactions on Information Theory},
  year = {1985},
  volume = {IT-31},
  pages = {461--464},
  number = {4}
}

@article{mostafa2017supervised,
  title={Supervised learning based on temporal coding in spiking neural networks},
  author={Mostafa, Hesham},
  journal={IEEE transactions on neural networks and learning systems},
  volume=29,
  number=7,
  pages={3227--3235},
  year=2017,
  publisher={IEEE}
}

@INCOLLECTION{mountcastle79organization,
  author = {Vernon B. Mountcastle},
  title = {An Organizing Principle for Cerebral Function: {T}he Unit Module
	and the Distributed system},
  booktitle = {Neuroscience, Fourth Study Program},
  publisher = {MIT Press},
  year = {1979},
  editor = {F. O. Schmitt},
  pages = {21--42},
  address = {Cambridge, MA},
  biburl = {http://www.bibsonomy.org/bibtex/246befeeb86726e3c127d0fdfd83867c0/idsia},
  keywords = {nn }
}

@ARTICLE{mountcastle1997columnar,
  author = {Mountcastle, V B},
  title = {The columnar organization of the neocortex},
  journal = {Brain},
  year = {1997},
  volume = {120},
  pages = {701-722},
  number = {4},
  abstract = {The modular organization of nervous systems is a widely documented
	principle of design for both vertebrate and invertebrate brains of
	which the columnar organization of the neocortex is an example. The
	classical cytoarchitectural areas of the neocortex are composed of
	smaller units, local neural circuits repeated iteratively within
	each area. Modules may vary in cell type and number, in internal
	and external connectivity, and in mode of neuronal processing between
	different large entities; within any single large entity they have
	a basic similarity of internal design and operation. Modules are
	most commonly grouped into entities by sets of dominating external
	connections. This unifying factor is most obvious for the heterotypical
	sensory and motor areas of the neocortex. Columnar defining factors
	in homotypical areas are generated, in part, within the cortex itself.
	The set of all modules composing such an entity may be fractionated
	into different modular subsets by different extrinsic connections.
	Linkages between them and subsets in other large entities form distributed
	systems. The neighborhood relations between connected subsets of
	modules in different entities result in nested distributed systems
	that serve distributed functions. A cortical area defined in classical
	cytoarchitectural terms may belong to more than one and sometimes
	to several distributed systems. Columns in cytoarchitectural areas
	located at some distance from one another, but with some common properties,
	may be linked by long-range, intracortical connections.}
}

@INPROCEEDINGS{Mueller04_highconductance,
  author = {Eilif Muller and Karlheinz Meier and Johannes Schemmel},
  title = {Methods for Simulating High-Conductance States in Neural Microcircuits},
  booktitle = {Brain Inspired Cognitive Systems BICS2004},
  year = {2004}
}

@PHDTHESIS{mueller06phd,
  author = {Eilif Benjamin Muller},
  title = {Markov Process Models for Neural Ensembles with Spike-Frequency Adaptation},
  school = {Ruprecht-Karls University Heidelberg},
  year = {2006},
  howpublished = {Ph.D. thesis, University of Heidelberg, HD-KIP-06-17}
}

@PHDTHESIS{mueller2014phd,
  author = {Eric Christian M{\"u}ller},
  title = {Novel Operation Modes of Accelerated Neuromorphic Hardware},
  year = 2014,
  school = {Ruprecht-Karls-Universit{\"a}t Heidelberg},
  note = {HD-KIP 14-98},
  url = {http://www.kip.uni-heidelberg.de/Veroeffentlichungen/details.php?id=3112}
}

@ARTICLE{mueller2020bss1,
	author = {M{\"u}ller, Eric and Schmitt, Sebastian and Mauch, Christian and Schmidt, Hartmut and Montes, Jos{\'e} and Ilmberger, Joscha and Kl{\"a}hn, Johann and Passenberg, Felix and Koke, Christoph and Kleider, Mitja and Jeltsch, Sebastian and G{\"u}ttler, Maurice and Husmann, Dan and Billaudelle, Sebastian and M{\"u}ller, Paul and Gr{\"u}bl, Andreas and Kaiser, Jakob and Weidner, Jonas and Vogginger, Bernhard and Partzsch, Johannes and Mayr, Christian and Schemmel, Johannes},
	title = {The Operating System of the Neuromorphic {BrainScaleS-1} System},
	year = 2020,
	month = mar,
	note = {submitted to Neurocomputing OSP},
	journal={arXiv preprint},
	eprint={2003.13749},
	archivePrefix={arXiv},
	primaryClass={cs.NE},
	url={http://arxiv.org/abs/2003.13749}
}

@ARTICLE{mueller2022operating, crossref = {mueller2020bss1_nourl} }
@ARTICLE{mueller2020bss1_nourl,
	author = {M{\"u}ller, Eric and Schmitt, Sebastian and Mauch, Christian and Billaudelle, Sebastian and Gr{\"u}bl, Andreas and G{\"u}ttler, Maurice and Husmann, Dan and Ilmberger, Joscha and Jeltsch, Sebastian and Kaiser, Jakob and Klähn, Johann and Kleider, Mitja and Koke, Christoph and Montes, Jos{\'e} and M{\"u}ller, Paul and Partzsch, Johannes and Passenberg, Felix and Schmidt, Hartmut and Vogginger, Bernhard and Weidner, Jonas and Mayr, Christian and Schemmel, Johannes},
	title = {The Operating System of the Neuromorphic {BrainScaleS-1} System},
	journal = {Neurocomputing},
	volume = 501,
	pages = {790--810},
	year = 2022,
	issn = {0925-2312},
	eprint={2003.13749},
	archivePrefix={arXiv},
	primaryClass={cs.NE},
	doi={10.1016/j.neucom.2022.05.081}
}

@TECHREPORT{mueller2020bss2ll,
	author = {M{\"u}ller, Eric and Mauch, Christian and Spilger, Philipp and Breitwieser, Oliver Julien and Kl{\"a}hn, Johann and St{\"o}ckel, David and Wunderlich, Timo and Schemmel, Johannes},
	title = {Extending {BrainScaleS} {OS} for {BrainScaleS-2}},
	year = 2020,
	month = mar,
	institution = {Electronic Vision(s), Kirchhoff Institute for Physics, Heidelberg University, Germany},
	address = {Heidelberg, Germany},
	doi={2003.13750}
}

@TECHREPORT{mueller2020bss2ll_nourl,
	author = {M{\"u}ller, Eric and Mauch, Christian and Spilger, Philipp and Breitwieser, Oliver Julien and Kl{\"a}hn, Johann and St{\"o}ckel, David and Wunderlich, Timo and Schemmel, Johannes},
	title = {Extending {BrainScaleS} {OS} for {BrainScaleS-2}},
	year = 2020,
	month = mar,
	institution = {Electronic Vision(s), Kirchhoff Institute for Physics, Heidelberg University, Germany},
	address = {Heidelberg, Germany},
	doi={2003.13750}
}

@ARTICLE{mueller2021bss2, crossref = {mueller2022scalable} }
@ARTICLE{mueller2021scalable, crossref = {mueller2022scalable} }
@ARTICLE{mueller2022scalable,
  author = {M{\"u}ller, Eric and Arnold, Elias and Breitwieser, Oliver and Czierlinski, Milena and Emmel, Arne and Kaiser, Jakob and Mauch, Christian and Schmitt, Sebastian and Spilger, Philipp and Stock, Raphael and Stradmann, Yannik and Weis, Johannes and Baumbach, Andreas and Billaudelle, Sebastian and Cramer, Benjamin and Ebert, Falk and Göltz, Julian and Ilmberger, Joscha and Karasenko, Vitali and Kleider, Mitja and Leibfried, Aron and Pehle, Christian and Johannes Schemmel},
  title = {A Scalable Approach to Modeling on Accelerated Neuromorphic Hardware},
  volume = 16,
  year = 2022,
  journal = {Front. Neurosci.},
  doi = {10.3389/fnins.2022.884128},
  issn = {1662-453X},
  abstract = {Neuromorphic systems open up opportunities to enlarge the explorative space for computational research. However, it is often challenging to unite efficiency and usability. This work presents the software aspects of this endeavor for the BrainScaleS-2 system, a hybrid accelerated neuromorphic hardware architecture based on physical modeling. We introduce key aspects of the BrainScaleS-2 Operating System: experiment workflow, API layering, software design, and platform operation. We present use cases to discuss and derive requirements for the software and showcase the implementation. The focus lies on novel system and software features such as multi-compartmental neurons, fast re-configuration for hardware-in-the-loop training, applications for the embedded processors, the non-spiking operation mode, interactive platform access, and sustainable hardware/software co-development. Finally, we discuss further developments in terms of hardware scale-up, system usability, and efficiency.}
}


@ARTICLE{muller2011,
  author = {Muller, Lyle and Destexhe, Alain},
  title = {A model of propagating waves in cerebral cortex across network states},
  journal = {BMC Neuroscience},
  year = {2011},
  volume = {12},
  month = {July},
  doi = {10.1186/1471-2202-12-S1-P67},
  url = {http://www.biomedcentral.com/1471-2202/12/S1/P67}
}

@ARTICLE{muller2010,
  author = {Muller, Lyle and Polack, Pierre-Olivier and Contreras, Diego and
	Destexhe, Alain},
  title = {Analysis of voltage-sensitive dye imaging data during propagating
	cortical waves in mouse visual cortex reveals fine structure and
	state dependence},
  journal = {Soc Neurosci Abstr},
  year = {2010},
  url = {http://www.abstractsonline.com/Plan/ViewAbstract.aspx?sKey=e699bd44-fb06-4ed8-bc95-6a577d4d6d85&cKey=72e3d85a-b88b-432f-bf76-934bfe7b97b9&mKey=%7bE5D5C83F-CE2D-4D71-9DD6-FC7231E090FB%7d}
}

@article{muller2012,
  author = "Lyle Muller and Alain Destexhe",
  title = "Propagating waves in thalamus, cortex and the thalamocortical system: Experiments and models",
  journal = "Journal of Physiology-Paris",
  volume = "106",
  number = "5–6",
  pages = "222 - 238",
  year = "2012",
  note = "<ce:title>New trends in neurogeometrical approaches to the brain and mind problem</ce:title>",
  issn = "0928-4257",
  doi = "10.1016/j.jphysparis.2012.06.005",
  url = "http://www.sciencedirect.com/science/article/pii/S0928425712000393",
  keywords = "Voltage-sensitive dye",
  keywords = "Multi-electrode array",
  keywords = "Population dynamics",
  keywords = "Propagating waves",
  keywords = "Oscillations",
  keywords = "Sensory cortices",
  keywords = "Spiking neural networks"
}

@INPROCEEDINGS{murakawa_ices98,
  author = {Murakawa, Masahiro and Yoshizawa, Shuji and Toshio, Adachi and Suzuki,
	Shiro and Takasuka, Kaoru and Iwata, Masaya and Higuchi, Tetsuya},
  title = {Analogue {EHW} Chip for Intermediate Frequency Filters},
  booktitle = {Proc.\ 2nd Int.\ Conf.\ on Evolvable Systems: From Biology to Hardware
	(ICES98)},
  year = {1998},
  editor = {Sipper, Moshe and Mange, Daniel and P{\'e}rez-Uribe, Andr{\'e}s},
  pages = {134--143},
  address = {Lausanne, Switzerland},
  month = sep,
  publisher = {Springer Verlag},
  annote = {GmC Filter -> Handy},
  groupsearch = {0}
}

@ARTICLE{murakoshi1993,
  author = {Takayuki Murakoshi and Jian-Zhong Guo and Tomomi Ichinose},
  title = {Electrophysiological identification of horizontal synaptic connections
	in rat visual cortex in vitro},
  journal = {Neuroscience Letters},
  year = {1993},
  volume = {163},
  pages = {211 - 214},
  number = {2},
  doi = {10.1016/0304-3940(93)90385-X},
  issn = {0304-3940},
  url = {http://www.sciencedirect.com/science/article/pii/030439409390385X}
}

@INPROCEEDINGS{muehlenbein_icga89parallel,
  author = {M{\"u}hlenbein, H.},
  title = {Parallel Genetic Algorithms, Population Genetics and Combinatorial
	Optimization},
  booktitle = {Proceedings of the Third International Conference on Genetic Algorithms},
  year = {1989},
  editor = {Schaffer, J. D.},
  pages = {416--421},
  address = {San Francisco},
  publisher = {Morgan Kaufmann}
}

@ARTICLE{mezard89tiling,
  author = {M{\'e}zard, M. and Nadal, J.-P.},
  title = {Learning inf feedforward layered networks: the tiling algorithm},
  journal = {Journal of Physics A},
  year = {1989},
  volume = {22},
  pages = {2191--2203}
}

@MISC{modeldb_homepage,
  author = {{M}odel{DB}},
  title = {Website},
  howpublished = {\url{http://senselab.med.yale.edu/modeldb}},
  year = {2008}
}

@phdthesis{nagel1975phd,
    Author = {Nagel, Laurence W.},
    Title = {SPICE2: A Computer Program to Simulate Semiconductor Circuits},
    School = {EECS Department, University of California, Berkeley},
    Year = 1975,
    URL = {http://www.eecs.berkeley.edu/Pubs/TechRpts/1975/9602.html},
    Abstract = {The size and complexity of present integrated circuits has increased
to the point where computer aids are virtually indispensable.
A circuit simulation program that characterizes the electrical
performance of a circuit is one important computer aid in the
circuit design process.  The need for accurate and efficient
circuit simulation has prompted the development of many circuit
simulation programs as well as the advancement of the associated
numerical methods.

Two purposes are served by this thesis.  First the numerical methods
that are necessary ingredients of a circuit simulation program are
detailed, and the implementations of these methods are described
and compared.  Second, the theoretical and practical aspects of the
SPICE1 and SPICE2 programs are documented.  SPICE is one of several
circuit simulation programs that presently are used by a substantial
portion of the electronics industry.  The development of SPICE is
due mainly to the author.  The numerical methods that are employed
in SPICE, of course, are the results of many researchers.

This thesis begins with a functional description of the SPICE
program. The presentation of the overall task of circuit simulation is
subdivided into the topics of equation formulation, linear equation
solution, nonlinear equation solution, and numerical integration.  The
bulk of this thesis is devoted to a comparison of the available methods
within each of these general topics.  These comparisons are based upon an
evaluation of the performance of the methods for typical electronic
circuit simulation problems.

The numerical methods that are employed in both versions of SPICE
are presented in detail.  These methods are chosen according to
guidelines that are presented in the introductory sections of
this thesis.  Different guidelines probably would result in the
implementation of different methods.  The widespread use of SPICE,
however, indicates that the algorithms that are employed in SPICE are
applicable to a wide range of practical circuit simulation problems.}
}

@MISC{RFC896,
  author = {John Nagle},
  title = {{RFC 896}: {C}ongestion control in {IP/TCP} internetworks},
  month = jan,
  year = {1984},
  url = {http://www.faqs.org/rfcs/rfc896.html}
}

@article{nakanishi1998functional,
  title={Functional synapses in synchronized bursting of neocortical neurons in culture},
  author={Nakanishi, Keiko and Kukita, Fumio},
  journal={Brain research},
  volume={795},
  number={1},
  pages={137--146},
  year={1998},
  publisher={Elsevier}
}

@ARTICLE{naud08,
  author = {Naud, Richard and Marcille, Nicolas and Clopath, Claudia and Gerstner,
	Wulfram},
  title = {Firing patterns in the adaptive exponential integrate-and-fire model},
  journal = {Biological Cybernetics},
  year = {2008},
  volume = {99},
  pages = {335--347},
  number = {4},
  month = {Nov},
  abstract = {Abstract\&nbsp;\&nbsp;For simulations of large spiking neuron networks,
	an accurate, simple and versatile single-neuron modeling framework
	is required. Here we explore the versatility of a simple two-equation
	model: the adaptive exponential integrate-and-fire neuron. We show
	that this model generates multiple firing patterns depending on the
	choice of parameter values, and present a phase diagram describing
	the transition from one firing type to another. We give an analytical
	criterion to distinguish between continuous adaption, initial bursting,
	regular bursting and two types of tonic spiking. Also, we report
	that the deterministic model is capable of producing irregular spiking
	when stimulated with constant current, indicating low-dimensional
	chaos. Lastly, the simple model is fitted to real experiments of
	cortical neurons under step current stimulation. The results provide
	support for the suitability of simple models such as the adaptive
	exponential integrate-and-fire neuron for large network simulations.},
  day = {01},
  doi = {10.1007/s00422-008-0264-7},
  url = {http://dx.doi.org/10.1007/s00422-008-0264-7}
}

@article{neckar2018braindrop,
  title={Braindrop: A mixed-signal neuromorphic architecture with a dynamical systems-based programming model},
  author={Neckar, Alexander and Fok, Sam and Benjamin, Ben V and Stewart, Terrence C and Oza, Nick N and Voelker, Aaron R and Eliasmith, Chris and Manohar, Rajit and Boahen, Kwabena},
  journal={Proceedings of the IEEE},
  volume=107,
  number=1,
  pages={144--164},
  year=2018,
  publisher={IEEE}
}

@article{neftci2013event-driven_fulldate,
  title={Event-Driven Contrastive Divergence for Spiking Neuromorphic Systems},
  author={Neftci, Emre and Das, Srinjoy and Pedroni, Bruno and Kreutz-Delgado, Kenneth and Cauwenberghs, Gert},
  journal={arXiv preprint arXiv:1311.0966},
  year={November 6th, 2013}
}

@article{neftci2014event-driven_fulldate,
  title={Event-Driven Contrastive Divergence for Spiking Neuromorphic Systems},
  author={Neftci, Emre and Das, Srinjoy and Pedroni, Bruno and Kreutz-Delgado, Kenneth and Cauwenberghs, Gert},
  journal={Frontiers in Neuromorphic Engineering},
  year={January 30th, 2014}
}

@article{neftci2015event,
  title={Event-driven contrastive divergence: neural sampling foundations},
  author={Neftci, Emre and Das, Srinjoy and Pedroni, Bruno and Kreutz-Delgado, Kenneth and Cauwenberghs, Gert},
  journal={Frontiers in neuroscience},
  volume={9},
  year={2015},
  publisher={Frontiers Media SA}
}

@article{neftci2019surrogate,
  title={Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based optimization to spiking neural networks},
  author={Neftci, Emre O. and Mostafa, Hesham and Zenke, Friedemann},
  journal={IEEE Signal Processing Magazine},
  volume=36,
  number=6,
  pages={51--63},
  year=2019,
  doi={10.1109/MSP.2019.2931595},
  publisher={IEEE}
}

@MANUAL{neo_homepage,
  key = {Neo},
  organization = {The NeuralEnsemble Initiative},
  title = {A base library for handling electrophysiology data in {Python}},
  url = {http://www.neuralensemble.org/neo},
  year = 2014,
  note = {[Online; accessed: 2014-04-29]}
}

@ARTICLE{Nere2012,
  author = {Nere, Andrew AND Olcese, Umberto AND Balduzzi, David AND Tononi, Giulio},
  title = {A Neuromorphic Architecture for Object Recognition and Motion Anticipation
	Using Burst-STDP},
  journal = {PLOS ONE},
  year = {2012},
  volume = {7},
  pages = {e36958},
  number = {5},
  month = {05},
  abstract = {<p>In this work we investigate the possibilities offered by a minimal
	framework of artificial spiking neurons to be deployed <italic>in
	silico</italic>. Here we introduce a hierarchical network architecture
	of spiking neurons which learns to recognize moving objects in a
	visual environment and determine the correct motor output for each
	object. These tasks are learned through both supervised and unsupervised
	spike timing dependent plasticity (STDP). STDP is responsible for
	the strengthening (or weakening) of synapses in relation to pre-
	and post-synaptic spike times and has been described as a Hebbian
	paradigm taking place both <italic>in vitro</italic> and <italic>in
	vivo</italic>. We utilize a variation of STDP learning, called burst-STDP,
	which is based on the notion that, since spikes are expensive in
	terms of energy consumption, then strong bursting activity carries
	more information than single (sparse) spikes. Furthermore, this learning
	algorithm takes advantage of homeostatic renormalization, which has
	been hypothesized to promote memory consolidation during NREM sleep.
	Using this learning rule, we design a spiking neural network architecture
	capable of object recognition, motion detection, attention towards
	important objects, and motor control outputs. We demonstrate the
	abilities of our design in a simple environment with distractor objects,
	multiple objects moving concurrently, and in the presence of noise.
	Most importantly, we show how this neural network is capable of performing
	these tasks using a simple leaky-integrate-and-fire (LIF) neuron
	model with binary synapses, making it fully compatible with state-of-the-art
	digital neuromorphic hardware designs. As such, the building blocks
	and learning rules presented in this paper appear promising for scalable
	fully neuromorphic systems to be implemented in hardware chips.</p>},
  doi = {10.1371/journal.pone.0036958},
  owner = {simon},
  publisher = {Public Library of Science},
  timestamp = {2013.01.06},
  url = {http://dx.doi.org/10.1371%2Fjournal.pone.0036958}
}

@inproceedings{nessler2009stdp,
  title={STDP enables spiking neurons to detect hidden causes of their inputs.},
  author={Nessler, Bernhard and Pfeiffer, Michael and Maass, Wolfgang},
  booktitle={NIPS},
  pages={1357--1365},
  year={2009}
}

@ARTICLE{nessler2013sem,
  author = {Nessler, Bernhard and Pfeiffer, Michael and Buesing, Lars and Maass, Wolfgang},
  title = {Bayesian Computation Emerges in Generic Cortical Microcircuits through Spike-Timing-Dependent Plasticity},
  journal = {PLoS Computational Biology},
  year = {2013},
  volume = {9},
  pages = {e1003037},
  number = {4},
  editor = {Sporns, Olaf},
  publisher = {Public Library of Science},
  doi = {10.1371/journal.pcbi.1003037},
}

@INPROCEEDINGS{netter02arobotic,
  author = {Thomas Netter and Nicolas Franceschini},
  title = {A robotic aircraft that follows terrain using a neuromorphic eye},
  booktitle = {Conf. Intelligent Robots and System},
  year = {2002},
  pages = {129--134}
}

@ARTICLE{neubauer1998neocognitron,
  author = {C. Neubauer},
  title = {Evaluation of convolutional neural networks for visual recognition},
  journal = {Transactions on Neural Networks},
  year = {1998},
  volume = {9},
  pages = {685--696},
  number = {4},
  file = {neubauer1998neocognitron.pdf:neubauer1998neocognitron.pdf:PDF},
  keywords = {convolutional NN},
  owner = {fieres}
}

@TECHREPORT{neumann45draft,
  author = {J. von Neumann},
  title = {First draft of a report on the EDVAC},
  institution = {Moore School of Electrical Engeneering Library, University of Pennsylvania},
  year = {1945},
  note = {Transscript in: M. D. Godfrey: Introduction to ``The first draft
	report on the EDVAC'' by John von Neumann. IEEE Annals of the History
	of Computing 15(4), 27--75 (1993)},
  file = {neumann45draft.pdf:neumann45draft.pdf:PDF},
  keywords = {NN},
  owner = {fieres}
}

@BOOK{Neumann1958,
  title = {The computer and the brain},
  publisher = {Yale University Press},
  year = {1958},
  author = {Neumann, John von},
  address = {New Haven, CT, USA},
  isbn = {0300007930},
  owner = {simon},
  source = {Library of Congress catalog card number: 58-6542},
  timestamp = {2013.04.29}
}

@inproceedings{neuwirth2015scalable,
  title={Scalable communication architecture for network-attached accelerators},
  author={Neuwirth, Sarah and Frey, Dirk and Nuessle, Mondrian and Bruening, Ulrich},
  booktitle={2015 IEEE 21st International Symposium on High Performance Computer Architecture (HPCA)},
  pages={627--638},
  year=2015,
  organization={IEEE},
  doi={10.1109/HPCA.2015.7056068}
}

@BOOK{Forouzan2003,
	title = {TCP/IP Protocol Suite (2nd ed.)},
	publisher = {McGraw-Hill},
	year = {2003},
	author = {Forouzan, Behrouz A.},
	isbn = {0-07-246060-1}
}

@ARTICLE{neves08synaptic,
  author = {Neves, Guilherme and Cooke, Sam F. and Bliss, Tim V.},
  title = {Synaptic plasticity, memory and the hippocampus: a neural network
	approach to causality},
  journal = {Nat Rev Neurosci},
  year = {2008},
  volume = {9},
  pages = {65--75},
  number = {1},
  month = {January},
  doi = {http://dx.doi.org/10.1038/nrn2303},
  keywords = {hippocampus, memory, review, synaptic-plasticity},
  posted-at = {2008-02-13 11:02:21},
  publisher = {Nature Publishing Group},
  url = {http://dx.doi.org/10.1038/nrn2303}
}

@proceedings{niceworkshop2021,
  title = {NICE '20: Proceedings of the Neuro-Inspired Computational Elements Workshop},
  isbn = {9781450377188},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  location = {Heidelberg, Germany},
  doi = {10.1145/3381755},
  year  = 2021
}


@MISC{niedenzu_diplomathesis,
  author = {Niedenzu, Dominik},
  title = {Aufbau eines bin{\"a}ren {Neocognitrons}},
  howpublished = {Diploma thesis (German), University of Heidelberg, HD-KIP-03-11},
  year = {2003},
  key = {niedenzu_diplomathesis},
  keywords = {vision, convolutional NN, vlsi}
}

@ARTICLE{nimwegen99statistical,
  author = {van Nimwegen, E. and Crutchfield, J. P. and Mitchell, M.},
  title = {Statistical dynamics of the {R}oyal {R}oad genetic algorithm},
  journal = {Theoretical Computer Science},
  year = {1999},
  volume = {229},
  pages = {41--102},
  file = {nimwegen99statistical.pdf:nimwegen99statistical.pdf:PDF}
}

@MISC{qt_homepage,
  author = {Nokia},
  title = {{Q}t Cross-Platform Application Framework},
  howpublished = {\url{http://www.qtsoftware.com/}},
  year = {2009}
}

@MISC{Nonnenmacher2011,
  author = {Tobias Nonnenmacher},
  title = {Verification of an Embedded Processor for Synaptic Plasticity},
  month = {August},
  year = {2011},
  note = {Bachelor thesis},
  owner = {simon},
  timestamp = {2013.03.25}
}

@mastersthesis{nonnenmacher2015,
  author   = {Tobias Nonnenmacher},
  title    = {Characterization of Spike-Timing Dependent Plasticity in Neuromorphic Hardware},
  school   = {Ruprecht-Karls-Universit{\"a}t Heidelberg},
  year     = 2015,
  type     = {Master thesis}
}

@ARTICLE{Nordlie2009,
  author = {Nordlie, Eilen AND Gewaltig, Marc-Oliver AND Plesser, Hans Ekkehard},
  title = {Towards Reproducible Descriptions of Neuronal Network Models},
  journal = {PLoS Comput Biol},
  year = {2009},
  volume = {5},
  pages = {e1000456},
  number = {8},
  month = {08},
  abstract = {<title>Author Summary</title> <p>Scientists make precise, testable
	statements about their observations and models of nature. Other scientists
	can then evaluate these statements and attempt to reproduce or extend
	them. Results that cannot be reproduced will be duly criticized to
	arrive at better interpretations of experimental results or better
	models. Over time, this discourse develops our joint scientific knowledge.
	A crucial condition for this process is that scientists can describe
	their own models in a manner that is precise and comprehensible to
	others. We analyze in this paper how well models of neuronal networks
	are described in the scientific literature and conclude that the
	wide variety of manners in which network models are described makes
	it difficult to communicate models successfully. We propose a <italic>good
	model description practice</italic> to improve the communication
	of neuronal network models.</p>},
  doi = {10.1371/journal.pcbi.1000456},
  publisher = {Public Library of Science},
  url = {http://dx.doi.org/10.1371%2Fjournal.pcbi.1000456}
}

@BOOK{norris02,
  title = {Gigabit Ethernet Technology and Applications},
  publisher = {Artech House},
  year = {2003},
  author = {Mark Norris},
  address = {Boston}
}

@ARTICLE{northmore96spike,
  author = {David P. M. Northmore and John G. Elias},
  title = {Spike Train Processing by a Silicon Neuromorph: The Role of Sublinear
	Summation in Dendrites},
  journal = {Neural Computation},
  year = {1996},
  volume = {8},
  pages = {1245--1265},
  number = {6},
  abstract = {A dendritic tree, as part of a silicon neuromorph, was modelled in
	VLSI as a multibranched, passive cable structure with multiple synaptic
	sites that either depolarize or hyperpolarize local "membran patches",
	thereby raising or lowering the probability of spike generation of
	an integrate-and-fire "soma". As expected from previous theoretical
	analyses, contemporaneous synaptic activation at widely separated
	sites on the artificial tree resulted in near-linear summation, as
	did neighboring excitatory and inhibitory activations. Activation
	of synapses of the same type close in time and space produced local
	saturation of potential, resulting in spike train processing capabilities
	not possible with linear summation alone. The resulting sublinear
	synaptic summation, as well as being physiologically plausible, is
	sufficient for a variety of spike train processing functions. With
	the appropriate arrangement of synaptic inputs on its dendritic tree,
	a neuromorph was shown to discriminate input pulse intervals and
	patterns, pulse train frequencies, and detect correlation between
	input trains.},
  citeseerurl = {citeseer.nj.nec.com/3237.html},
  keywords = {spiking}
}

@inproceedings{nokland2016, crossref = {nokland2016direct} }
@inproceedings{nokland2016direct,
  author = {A. N\/okland},
  title = {Direct Feedback Alignment Provides Learning in Deep Neural Networks},
  booktitle={NIPS},
  year={2016}
}

@ARTICLE{nowlan92weightsharing,
  author = {S. J. Nowlan and G. E. Hinton},
  title = {Simplifying Neural Networks by Soft Weight-Sharing},
  journal = {Neural Computation},
  year = {1992},
  volume = {4},
  pages = {473-493},
  file = {nowlan92weightsharing.pdf:nowlan92weightsharing.pdf:PDF},
  owner = {fieres}
}

@MISC{numpy_homepage,
  author = {Numpy},
  title = {Website},
  howpublished = {\url{http://numpy.scipy.org}},
  year = {2012}
}

@MISC{NXPSemiconductors2010,
  author = {{NXP Semiconductors}},
  title = {LPC2939 ARM9 microcontroller with CAN, LIN, and USB},
  month = {April},
  year = {2010},
  note = {Rev. 3},
  owner = {simon},
  timestamp = {2013.04.15},
  url = {http://www.nxp.com/documents/data_sheet/LPC2939.pdf}
}

@MISC{neurotools_homepage,
  author = {{N}euro{T}ools},
  title = {Website},
  howpublished = {\url{http://neuralensemble.org/trac/NeuroTools}},
  year = {2008}
}

@INPROCEEDINGS{ochoa_gecco99recombination,
  author = {Ochoa, G. and Harvey, I. and Buxton, H.},
  title = {On recombination and optimal mutation rates},
  booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
  year = {1999},
  volume = {1},
  pages = {488--495},
  address = {San Francisko, CA, USA},
  publisher = {Morgan Kaufmann},
  file = {ochoa_gecco99recombination.pdf:ochoa_gecco99recombination.pdf:PDF}
}

@MISC{ocp30,
  author = {{OCP}},
  title = {Open Core Protocol Specification 3.0},
  year = {2009},
  company = {OCP IP},
  owner = {simon},
  timestamp = {2013.02.16},
  url = {http://www.ocpip.org/home}
}

@article{oh2004gpu,
  title={{GPU} implementation of neural networks},
  author={Oh, Kyoung-Su and Jung, Keechul},
  journal={Pattern Recognition},
  volume={37},
  number={6},
  pages={1311--1314},
  year={2004},
  publisher={Elsevier}
}

@ARTICLE{ohki07specificity,
  author = {Kenichi Ohki and R Clay Reid},
  title = {Specificity and randomness in the visual cortex},
  journal = {Curr Opin Neurobiol},
  year = {2007},
  volume = {17},
  pages = {401--7},
  number = {4},
  month = {Aug},
  abstract = {Research on the functional anatomy of visual cortical circuits has
	recently zoomed in from the macroscopic level to the microscopic.
	High-resolution functional imaging has revealed that the functional
	architecture of orientation maps in higher mammals is built with
	single-cell precision. By contrast, orientation selectivity in rodents
	is dispersed on visual cortex in a salt-and-pepper fashion, despite
	highly tuned visual responses. Recent studies of synaptic physiology
	indicate that there are disjoint subnetworks of interconnected cells
	in the rodent visual cortex. These intermingled subnetworks, described
	in vitro, may relate to the intermingled ensembles of cells tuned
	to different orientations, described in vivo. This hypothesis may
	soon be tested with new anatomic techniques that promise to reveal
	the detailed wiring diagram of cortical circuits.},
  affiliation = {Department of Neurobiology, Harvard Medical School, 220 Longwood Avenue,
	Boston, MA 02115, USA.}
}

@ARTICLE{ohsaki1994singlepoly,
    author={Ohsaki, K. and Asamoto, N. and Takagaki, S.},
    journal={Solid-State Circuits, IEEE Journal of},
    title={A single poly EEPROM cell structure for use in standard CMOS
        processes},
    year=1994,
    volume=29,
    number=3,
    pages={311-316},
    keywords={CMOS integrated circuits;EPROM;integrated memory
        circuits;silicon;0.8 micron;5 to 9 V;NMOS transistors;PMOS
            transistors;Si;common gate;control node;data
            retention;endurance;floating gate;high temperature;inversion
            layer;polysilicon gate;single poly EEPROM cell
            structure;standard CMOS processes;threshold voltage shift;CMOS
            logic circuits;CMOS process;Circuit testing;EPROM;Logic
            testing;MOS devices;MOSFETs;Standards
            development;Temperature;Threshold voltage},
    doi={10.1109/4.278354},
    ISSN={0018-9200},
    month={Mar},
}

@ARTICLE{oja92principle,
  author = {E. Oja},
  title = {Principle components, minor components, and linear neural networks},
  journal = {Neural Networks},
  year = {1992},
  volume = {5},
  pages = {927--936},
  keywords = {learning},
  owner = {fieres}
}

@ARTICLE{oja82,
  author = {E. Oja},
  title = {A Simplified Neuron Model as a Principal Component Analyzer},
  journal = {Journal of Mathematical Biology},
  year = {1982},
  volume = {15},
  pages = {267-273},
  biburl = {http://www.bibsonomy.org/bibtex/2b5e3b7082e89ffa329b1c7e90395ba5a/brian.mingus},
  description = {CCNLab BibTeX},
  keywords = {nnets}
}

@ARTICLE{okun08instantaneous,
  author = {Michael Okun and Ilan Lampl},
  title = {Instantaneous correlation of excitation and inhibition during ongoing
	and sensory-evoked activities},
  journal = {Nat Neurosci},
  year = {2008},
  volume = {11},
  pages = {535--7},
  number = {5},
  month = {May},
  abstract = {Temporal and quantitative relations between excitatory and inhibitory
	inputs in the cortex are central to its activity, yet they remain
	poorly understood. In particular, a controversy exists regarding
	the extent of correlation between cortical excitation and inhibition.
	Using simultaneous intracellular recordings in pairs of nearby neurons
	in vivo, we found that excitatory and inhibitory inputs are continuously
	synchronized and correlated in strength during spontaneous and sensory-evoked
	activities in the rat somatosensory cortex.},
  affiliation = {Department of Neurobiology, Weizmann Institute of Science, Rehovot
	76100, Israel.}
}

@ARTICLE{oliker92distributed,
  author = {Oliker, S. and Furst, M. and Maimon, O.},
  title = {A distributed genetic algorithm for neural network design and training},
  journal = {Complex Systems},
  year = {1992},
  volume = {6},
  pages = {459--477},
  number = {5}
}

@ARTICLE{Oliphant2007,
  author = {Travis E. Oliphant},
  title = {Python for Scientific Computing},
  journal = {IEEE Computing in Science and Engineering},
  year = {2007},
  volume = {9},
  pages = {10--20},
  number = {3}
}

@MISC{facets_d7-6,
  author = {Dan Husmann de Oliveira},
  title = {Build a first prototype of the final system by mounting the prototype-wafer
	on its dedicated PCB. Evaluate the yield of the inter-wafer as well
	as the wafer-PCB connections},
  howpublished = {{FACETS} Deliverable D7-6},
  year = {2008},
  keywords = {facets, deliverable},
  owner = {smillner},
  timestamp = {2010.08.30}
}

@ARTICLE{omondi00neurocomputers,
  author = {Omondi, A. R.},
  title = {Neurocomputers: {A} dead end ?},
  journal = {International Journal of Neural Systems},
  year = {2000},
  volume = {10},
  pages = {475--481},
  number = {6}
}

@MISC{or1200_coremark_2013,
  author = {OpenCores},
  title = {OR1200 OpenRISC Processor},
  howpublished = {Website},
  month = {April},
  year = {2013},
  owner = {simon},
  timestamp = {2013.04.15},
  url = {http://opencores.org/or1k/OR1200_OpenRISC_Processor}
}

@MISC{openfabrics_2011,
  author = {{OpenFabrics}},
  title = {Website},
  howpublished = {\url{http://www.openfabrics.org/index.php?option=com_content&view=article&id=3}},
  year = {2011}
}

@MISC{OpenRISCProject2013,
  author = {{OpenRISC Project}},
  howpublished = {Website},
  month = {April},
  year = {2013},
  owner = {simon},
  timestamp = {2013.04.15},
  url = {openrisc.net}
}

@ARTICLE{opitz99popular,
  author = {Opitz, D. W. and Maclin, R.},
  title = {Popular ensemble methods: {A}n empirical study},
  journal = {Journal of Artificial Intelligence Research},
  year = {1999},
  volume = {11},
  pages = {337--353},
  file = {opitz99popular.pdf:opitz99popular.pdf:PDF}
}

@ARTICLE{opitz96actively,
  author = {Opitz, D. W. and Shavlik, J. W.},
  title = {Actively searching for an effective neural network ensemble},
  journal = {Connection Science},
  year = {1996},
  volume = {8},
  pages = {337--353},
  number = {3/4},
  file = {opitz96actively.pdf:opitz96actively.pdf:PDF}
}

@ARTICLE{oram94modeling,
  author = {M.W. Oram and D.I. Perret},
  title = {Modeling visual recognition from neurobiological constraints},
  journal = {Neural Networks},
  year = {1994},
  volume = {7},
  pages = {945--972},
  keywords = {convolutional NN},
  owner = {fieres}
}

@BOOK{orebaugh2006,
  title = {Wireshark \& Ethereal Network Protocol Analyzer Toolkit (Jay Beale's
	Open Source Security)},
  publisher = {Syngress Publishing},
  year = {2006},
  author = {Orebaugh, Angela and Ramirez, Gilbert and Burke, Josh and Pesce,
	Larry},
  isbn = {1597490733}
}

@MISC{ostendorf_diplomathesis,
  author = {Ostendorf, Boris},
  title = {{Charakterisierung eines Neuronalen Netzwerk-Chips}},
  howpublished = {Diploma thesis (German), University of Heidelberg, HD-KIP 07-12},
  year = {2007},
  keywords = {vision, vlsi},
  owner = {mueller},
  timestamp = {2008.08.01}
}

@INPROCEEDINGS{oster2005hardsoft,
  author = {Matthias Oster and Adrian M. Whatley and Shih-Chii Liu and Rodney
	J. Douglas},
  title = {A Hardware/Software Framework for Real-Time Spiking Systems},
  booktitle = {Proceedings of the 2005 International Conference on Artificial Neural
	Networks (ICANN2005)},
  year = {2005},
  owner = {bruederl},
  timestamp = {2008.09.02}
}

@ARTICLE{Otmakhov1993,
  author = {Nikolai Otmakhov and Aneil M. Shirke and Roberto Malinow},
  title = {Measuring the impact of probabilistic transmission on neuronal output
	},
  journal = {Neuron },
  year = {1993},
  volume = {10},
  pages = {1101 - 1111},
  number = {6},
  doi = {10.1016/0896-6273(93)90058-Y},
  issn = {0896-6273},
  owner = {simon},
  timestamp = {2013.05.03},
  url = {http://www.sciencedirect.com/science/article/pii/089662739390058Y}
}

@article{ovtcharov2015accelerating,
  title={Accelerating deep convolutional neural networks using specialized hardware},
  author={Ovtcharov, Kalin and Ruwase, Olatunji and Kim, Joo-Young and Fowers, Jeremy and Strauss, Karin and Chung, Eric S},
  journal={Microsoft Research Whitepaper},
  volume={2},
  year={2015}
}

@ARTICLE{ozturk07associative,
  author = {Ozturk, Mustafa C. and Principe, Jose C.},
  title = {An associative memory readout for ESNs with applications to dynamical
	pattern recognition},
  journal = {Neural Networks},
  year = {2007},
  volume = {20},
  pages = {377--390},
  number = {3},
  month = apr,
  abstract = {The use of echo state networks (ESN) to find patterns in time (dynamical
	pattern recognition) has been limited. This paper argues that ESNs
	are particularly well suited for dynamical pattern recognition and
	proposes a linear associative memory (LAM) as a novel readout for
	ESNs. From the class of LAMs, the minimum average correlation energy
	(MACE) filter is adopted because of its high rejection characteristics
	that allow its use as a detector in the automatic pattern recognition
	literature. In the ESN application, the MACE interprets the states
	of the ESN as a two-dimensional "image", one dimension being time
	and the other the processing element index (space). An optimal template
	image for each class, which associates ESN states with the class
	label, can be analytically computed using training data. During testing,
	ESN states are correlated with each template image and the class
	label of the template with the highest correlation is assigned to
	the input pattern. The ESN-MACE combination leads to a nonlinear
	template matcher with robust noise performance as needed in non-Gaussian,
	nonlinear digital communication channels. A real-world data experiment
	for chemical sensing with an electronic nose is included to demonstrate
	the validity of this approach. Moreover, the proposed readout can
	also be used with liquid state machines eliminating the need to convert
	spike trains into continuous signals by binning or low-pass filtering.},
  booktitle = {Echo State Networks and Liquid State Machines},
  keywords = {liquid},
  owner = {mreuss},
  timestamp = {2007.06.14},
  url = {http://www.sciencedirect.com/science/article/B6T08-4NMWR88-6/2/3f9acbb285b9ea8dfcf70d1c7ba9b212}
}

@MISC{opengl_website,
  author = {{O}pen{GL}},
  title = {Website},
  howpublished = {\url{http://www.opengl.org}}
}

@MANUAL{packet_man,
  title = {PACKET(7) Linux Programmer's Manual},
  year = {2020},
  month = feb,
  url = {http://man7.org/linux/man-pages/man7/packet.7.html}
}

@BOOK{pagejones00uml,
  title = {Fundamentals of Object-Oriented Design in {UML}},
  publisher = {Addison Wesley},
  year = {2000},
  author = {Page-Jones, M.},
  address = {Reading, MA},
  isbn = {0-201-69946-X}
}

@ARTICLE{pakkenberg1997,
  author = {Pakkenberg, B. and Gundersen, H.J.},
  title = {{Neocortical Neuron Number in Humans: Effect of Age and Sex}},
  journal = {{J Comp Neurol.}},
  year = {1997},
  volume = {384},
  pages = {312--320},
  number = {2},
  month = jul
}

@ARTICLE{Pakkenberg2003,
  author = {Pakkenberg, Bente and Pelvig, Dorte and Marner, Lisbeth and Bundgaard,
	Mads J and Gundersen, Hans J{\o}rgen G and Nyengaard, Jens R and
	Regeur, Lisbeth},
  title = {Aging and the human neocortex},
  journal = {Experimental gerontology},
  year = {2003},
  volume = {38},
  pages = {95--99},
  number = {1},
  owner = {simon},
  publisher = {Elsevier},
  timestamp = {2013.05.07}
}

@INPROCEEDINGS{panda01systemc,
  author = {Preeti Ranjan Panda},
  title = {SystemC},
  booktitle = {{ISSS}},
  year = {2001},
  pages = {75-80},
  citeseerurl = {citeseer.nj.nec.com/panda01systemc.html},
  keywords = {SystemC}
}

@ARTICLE{paninski2006sta,
  author = {Paninski, Liam},
  title = {The Spike-Triggered Average of the Integrate-and-Fire Cell Driven
	by Gaussian White Noise},
  journal = {Neural Comput.},
  year = {2006},
  volume = {18},
  pages = {2592--2616},
  number = {11},
  address = {Cambridge, MA, USA},
  issn = {0899-7667},
  publisher = {MIT Press}
}

@ARTICLE{pare98impact,
  author = {Pare, D. and Shink, E. and Gaudreau, H. and Destexhe, A. and Lang,
	E. J.},
  title = {Impact of spontaneous synaptic activity on the resting properties
	of cat neocortical pyramidal neurons In vivo},
  journal = {J Neurophysiol},
  year = {1998},
  volume = {79},
  pages = {1450-60},
  number = {3}
}

@INPROCEEDINGS{parekh_icnn97mtiling,
  author = {Parekh, R.G. and Yang, J. and Honavar, V.},
  title = {Pruning strategies for the MTiling constructive learning algorithm},
  booktitle = {Proceedings of the IEEE International Conference on Neural Networks
	({ICNN}'97)},
  year = {1997},
  volume = {3},
  pages = {1960 - 1965},
  address = {Houston, TX, USA}
}

@INPROCEEDINGS{parekh_icnn97mupstart,
  author = {Parekh, R.G. and Yang, J. and Honavar, V.},
  title = {MUpstart - A Constructive Neural Network Learning Algorithm for Multi-Category
	Pattern Classification},
  booktitle = {Proceedings of the IEEE International Conference on Neural Networks
	({ICNN}'97)},
  year = {1997},
  pages = {1924--1929},
  address = {Houston, TX, USA},
  file = {parekh_icnn97mupstart.pdf:parekh_icnn97mupstart.pdf:PDF}
}

@ARTICLE{partzsch2011,
  author = {Partzsch, J. and Sch\"uffny, R.},
  title = {Analyzing the Scaling of Connectivity in Neuromorphic Hardware and
	in Models of Neural Networks},
  journal = {Neural Networks, IEEE Transactions on},
  year = {2011},
  volume = {22},
  pages = {919 -935},
  number = {6},
  month = {june },
  doi = {10.1109/TNN.2011.2134109},
  issn = {1045-9227},
  keywords = {connection topologies;graph description;hardware architecture;neural
	connectivity;neural networks;neuromorphic hardware systems;graph
	theory;neural nets;Algorithms;Computer Simulation;Computer-Aided
	Design;Equipment Design;Equipment Failure Analysis;Image Interpretation,
	Computer-Assisted;Models, Theoretical;Neural Networks (Computer);Pattern
	Recognition, Automated;Signal Processing, Computer-Assisted;}
}

@mastersthesis{passenberg2019masterthesis,
  author   = {Felix Constantin Passenberg},
  title    = {Improving the BrainScaleS-1 place and route software towards real world waferscale experiments},
  school   = {Ruprecht-Karls-Universit{\"a}t Heidelberg},
  year     = {2019},
  type     = {Master thesis},
}

@ARTICLE{pei2019tianjic,
  title    = "Towards artificial general intelligence with hybrid Tianjic chip
              architecture",
  author   = "Pei, Jing and Deng, Lei and Song, Sen and Zhao, Mingguo and
              Zhang, Youhui and Wu, Shuang and Wang, Guanrui and Zou, Zhe and
              Wu, Zhenzhi and He, Wei and Chen, Feng and Deng, Ning and Wu, Si
              and Wang, Yu and Wu, Yujie and Yang, Zheyu and Ma, Cheng and Li,
              Guoqi and Han, Wentao and Li, Huanglong and Wu, Huaqiang and
              Zhao, Rong and Xie, Yuan and Shi, Luping",
  abstract = "There are two general approaches to developing artificial general
              intelligence (AGI)1: computer-science-oriented and
              neuroscience-oriented. Because of the fundamental differences in
              their formulations and coding schemes, these two approaches rely
              on distinct and incompatible platforms2-8, retarding the
              development of AGI. A general platform that could support the
              prevailing computer-science-based artificial neural networks as
              well as neuroscience-inspired models and algorithms is highly
              desirable. Here we present the Tianjic chip, which integrates the
              two approaches to provide a hybrid, synergistic platform. The
              Tianjic chip adopts a many-core architecture, reconfigurable
              building blocks and a streamlined dataflow with hybrid coding
              schemes, and can not only accommodate computer-science-based
              machine-learning algorithms, but also easily implement
              brain-inspired circuits and several coding schemes. Using just
              one chip, we demonstrate the simultaneous processing of versatile
              algorithms and models in an unmanned bicycle system, realizing
              real-time object detection, tracking, voice control, obstacle
              avoidance and balance control. Our study is expected to stimulate
              AGI development by paving the way to more generalized hardware
              platforms.",
  journal  = "Nature",
  volume   =  572,
  number   =  7767,
  pages    = "106--111",
  month    =  aug,
  year     =  2019,
  language = "en"
}

@ARTICLE{peters1997organization,
  author={Peters, A. and Sethares, C.},
  journal={Journal of Neurocytology},
  title={The organization of double bouquet cells in monkey striate cortex},
  year={1997},
  volume={26},
  number={12},
  pages={779 - 797},
  doi={10.1023/A:1018518515982}
}

@incollection{krizhevsky2012imagenet,
  title = {ImageNet Classification with Deep Convolutional Neural Networks},
  author = {Alex Krizhevsky and Sutskever, Ilya and Geoffrey E. Hinton},
  booktitle = {Advances in Neural Information Processing Systems 25},
  editor = {F. Pereira and C.J.C. Burges and L. Bottou and K.Q. Weinberger},
  pages = {1097--1105},
  year = {2012},
  publisher = {Curran Associates, Inc.},
  url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf}
}

@INPROCEEDINGS{pasero_ijcnn04codesign,
  author = {Pasero, E. and Perri, M.},
  title = {Hw-sw codesign of a flexible neural controller through a fpga-based
	neural network programmed in vhdl},
  booktitle = {Proceedings of the International Joint Conference on Neural Networks
	{IJCNN}'04},
  year = {2004},
  pages = {3161--3166},
  publisher = {IEEE Press}
}

@ARTICLE{paszke2017automatic,
  title = {Automatic differentiation in PyTorch},
  author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  year = 2017
}

@incollection{paszke2019pytorch,
  title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  booktitle = {Advances in Neural Information Processing Systems 32},
  editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d'Alch{\'e}{-}Buc and E. Fox and R. Garnett},
  pages = {8024--8035},
  year = {2019},
  publisher = {Curran Associates, Inc.},
  url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}

@incollection{paszke2019pytorch_nourl,
  title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  booktitle = {Advances in Neural Information Processing Systems 32},
  editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d'Alch{\'e}{-}Buc and E. Fox and R. Garnett},
  pages = {8024--8035},
  year = {2019},
  publisher = {Curran Associates, Inc.}
}

@incollection{paszke2019pytorch_nourl_shorter,
  title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  booktitle = {Advances in Neural Information Processing Systems 32},
  editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d'Alch{\'e}{-}Buc and E. Fox and R. Garnett},
  year = 2019,
}

@BOOK{Patterson1996,
  title = {Computer Architecture: A Quantitative Approach},
  publisher = {Morgan Kaufmann},
  year = {1996},
  editor = {Bruce M. Spatz},
  author = {David A. Patterson and John L. Hennessy},
  edition = {2},
  owner = {simon},
  timestamp = {2013.02.20}
}

@BOOK{pavan04,
  title = {Floating gate devices},
  publisher = {Kluwer Academic},
  year = {2004},
  author = {Pavan, Paolo and Larcher, Luca and Marmiroli, Andrea},
  pages = {XIII, 129 S.},
  address = {Boston [u.a.]},
  note = {Includes bibliographical references and index},
  isbn = {1-4020-7731-9},
  keywords = {(s)Speicherschaltung / (s)Floating-Gate-Struktur},
  language = {eng},
  library = {HE [Signatur:<HE> IH Pavan]},
  subtitle = {operation and compact modeling}
}

@ARTICLE{Pawlak2008,
  author = {Pawlak, Verena and Kerr, Jason ND},
  title = {Dopamine receptor activation is required for corticostriatal spike-timing-dependent
	plasticity},
  journal = {The Journal of Neuroscience},
  year = {2008},
  volume = {28},
  pages = {2435--2446},
  number = {10},
  owner = {simon},
  publisher = {Soc Neuroscience},
  timestamp = {2013.04.25}
}

@ARTICLE{Pawlak2010,
  author = {Pawlak, Verena and Wickens, Jeffery R and Kirkwood, Alfredo and Kerr,
	Jason ND},
  title = {Timing is not everything: neuromodulation opens the STDP gate},
  journal = {Frontiers in synaptic neuroscience},
  year = {2010},
  volume = {2},
  owner = {simon},
  publisher = {Frontiers Research Foundation},
  timestamp = {2013.04.25}
}

@ARTICLE{pecevski09,
  author = {Pecevski, Dejan A. and Natschl{\"a}ger, Thomas and Schuch, Klaus
	N.},
  title = {PCSIM: A Parallel Simulation Environment for Neural Circuits Fully
	Integrated with {P}ython},
  journal = {Front. Neuroinform.},
  year = {2009},
  volume = {3},
  number = {11},
  abstract = {The Parallel Circuit SIMulator (PCSIM) is a software package for simulation
	of neural circuits. It is primarily designed for distributed simulation
	of large scale networks of spiking point neurons. Although its computational
	core is written in C++, PCSIM's primary interface is in the Python
	programming language, which is a powerful programming environment
	and allows the user to easily integrate the neural circuit simulator
	with data analysis and visualization tools to manage the full neural
	modeling life cycle. The main focus of this paper is to describe
	PCSIM's full integration into Python and the benefits thereof. In
	particular we will investigate how the automatically generated bidirectional
	interface and PCSIM's object-oriented modular framework enable the
	user to adopt a hybrid modeling approach: using and extending PCSIM's
	functionality either employing pure Python or C++ and thus combining
	the advantages of both worlds. Furthermore, we describe several supplementary
	packages written in pure Python and tailored towards setting up and
	analyzing neural simulations.},
  keywords = {neural simulation, Python, C++, Boost.Python, Py++, spiking neuron}
}

@article{pecevski2011probabilistic,
  title={Probabilistic inference in general graphical models through sampling in stochastic networks of spiking neurons},
  author={Pecevski, Dejan and Buesing, Lars and Maass, Wolfgang},
  journal={PLoS computational biology},
  volume={7},
  number={12},
  pages={e1002294},
  year={2011},
  publisher={Public Library of Science}
}

@article{perin2011synaptic,
  title={A synaptic organizing principle for cortical neuronal groups},
  author={Perin, Rodrigo and Berger, Thomas K and Markram, Henry},
  journal={Proceedings of the National Academy of Sciences},
  volume={108},
  number={13},
  pages={5419--5424},
  year={2011},
  publisher={National Acad Sciences}
}

@MISC{petrovici2009graph,
  author = {Petrovici, Mihai A. and Bill, Johannes},
  title = {A New Method for Quantifying and Predicting Neural Response Correlations: Internal Report.},
  year = {2009}
}

@MISC{petrovici2011capocaccia,
  author = {Petrovici, Mihai A. and Pfeil, Thomas and Friedmann, Simon and Partzsch, Johannes and Jeltsch, Sebastian and Schwartz, Marc-Olivier and Vogginger, Bernhard and Grübl, Andreas and Husmann, Dan and Schemmel, Johannes and Meier, Karlheinz},
  title = {Exploring Network Architectures with the {FACETS}/{BrainScaleS} Hardware and {PyNN}},
  url = {https://capocaccia.ethz.ch/capo/wiki/2011/bsshw11},
  year = {2011}
}

@MISC{petrovici2012capocaccia,
  author = {Petrovici, Mihai A. and Millner, Sebastian and Schwartz, Marc-Olivier and Pfeil, Thomas and Jeltsch, Sebastian and Hartel, Andreas and Schemmel, Johannes and Meier, Karlheinz},
  title = {Experiments on {BrainScaleS} Hardware Systems},
  url = {https://capocaccia.ethz.ch/capo/wiki/2012/bssexperiments12},
  year = {2012}
}

@article{petrovici2013stochastic_fulldate,
  title={Stochastic inference with deterministic spiking neurons},
  author={Petrovici, Mihai A. and Bill, Johannes and Bytschok, Ilja and Schemmel, Johannes and Meier, Karlheinz},
  journal={arXiv preprint arXiv:1311.3211},
  year={November 11th, 2013}
}

@article{petrovici2013stochastic,
  title={Stochastic inference with deterministic spiking neurons},
  author={Petrovici, Mihai A. and Bill, Johannes and Bytschok, Ilja and Schemmel, Johannes and Meier, Karlheinz},
  journal={arXiv preprint arXiv:1311.3211},
  year={2013}
}

@article{petrovici2014characterization,
  title={Characterization and Compensation of Network-Level Anomalies in Mixed-Signal Neuromorphic Modeling Platforms},
  author={Petrovici, Mihai A and Vogginger, Bernhard and M{\"u}ller, Paul and Breitwieser, Oliver and Lundqvist, Mikael and Muller, Lyle and Ehrlich, Matthias and Destexhe, Alain and Lansner, Anders and Sch{\"u}ffny, Ren{\'e} and Schemmel, Johannes and Meier, Karlheinz },
  journal={PLOS ONE},
  volume={9},
  number={10},
  pages={e108590},
  year={2014},
  publisher={Public Library of Science}
}

@article{petrovici2015hcs,
  title={The high-conductance state enables neural sampling in networks of {LIF} neurons},
  author={Petrovici, Mihai A and Bytschok, Ilja and Bill, Johannes and Schemmel, Johannes and Meier, Karlheinz},
  journal={BMC Neuroscience},
  volume={16},
  number={Suppl 1},
  pages={O2},
  year={2015},
  publisher={BioMed Central Ltd}
}

@book{petrovici2016form,
  author   = {Mihai A. Petrovici},
  title    = {Form Versus Function: Theory and Models for Neuronal Substrates},
  publisher = {Springer},
  year     = {2016}
}

@article{petrovici2016robustness,
  author   = {Mihai A. Petrovici and Anna Schroeder and Oliver Breitwieser and Andreas Gr{\"u}bl and Johannes Schemmel and Karlheinz Meier},
  title    = {Robustness from structure: fast inference on a neuromorphic device with hierarchical {LIF} networks},
  journal  = {arXiv},
  year     = {2016}
}

@article{petrovici2016stochastic,
  author   = {Mihai A. Petrovici and Johannes Bill and Ilja Bytschok and Johannes Schemmel and Karlheinz Meier},
  title    = {Stochastic inference with spiking neurons in the high-conductance state},
  journal  = {Physical Review E},
  year     = {2016},
  volume   = {94},
  number   = {4},
  pages    = {},
  month    = {October},
  doi      = {10.1103/PhysRevE.94.042312},
  url      = {http://journals.aps.org/pre/abstract/10.1103/PhysRevE.94.042312}
}

@MISC{pcsim_homepage,
  author = {Pecevski, Dejan and Natschl{\"a}ger, Thomas},
  title = {{PCSIM} Website},
  howpublished = {\url{http://sourceforge.net/projects/pcsim}},
  year = {2008}
}

@MANUAL{scsi3_standard,
  title = {Working Draft {SCSI} Parallel Interface-2 ({SPI}-2)},
  author = {Penokie, G. O.},
  organization = {American National Standard of Accredited Standards Committee NCITS},
  address = {Washington, DC},
  edition = {revision 20b},
  month = {April},
  year = {1998}
}

@ARTICLE{perrone93ensemble,
  author = {Peronne, M. P. and Cooper, L. N.},
  title = {When networks disagree: {E}nsemble methods for hybrid neural networks},
  journal = {Artificial Neural Networks for Speech and Vision},
  year = {1993},
  pages = {126--142},
  address = {London},
  editor = {Mammone, R.J.},
  file = {perrone93ensemble.pdf:perrone93ensemble.pdf:PDF},
  publisher = {Chapman \& Hall}
}

@MISC{bazel,
    Author = {{Google LLC}},
    Title = {bazel},
    Year = {2015},
    URL = {https://bazel.build},
}

@ARTICLE{peter1979synaptic,
  author = {Peter, R. and others},
  title = {Synaptic density in human frontal cortex—developmental changes and
	effects of aging},
  journal = {Brain research},
  year = {1979},
  volume = {163},
  pages = {195--205},
  number = {2},
  owner = {simon},
  publisher = {Elsevier},
  timestamp = {2013.01.23}
}

@ARTICLE{petersen03,
  author = { Carl C. H. Petersen and Amiram Grinvald and Bert Sakmann},
  title = {Spatiotemporal Dynamics of Sensory Responses in Layer 2/3 of Rat
	Barrel Cortex Measured In Vivo by Voltage-Sensitive Dye Imaging Combined
	with Whole-Cell Voltage Recordings and Neuron Reconstructions},
  journal = {The Journal of Neuroscience},
  year = {2003},
  volume = {23},
  pages = {1298 -1309},
  number = {3},
  month = {February}
}

@BOOK{peterson2003,
  title = {Computer Networks: A Systems Approach, 3rd Edition},
  publisher = {Morgan Kaufmann Publishers Inc.},
  year = {2003},
  author = {Peterson, Larry L. and Davie, Bruce S.},
  address = {San Francisco, CA, USA},
  isbn = {155860832X}
}

@ARTICLE{Peterson1961,
  author = {Peterson, W.W. and Brown, D.T.},
  title = {Cyclic Codes for Error Detection},
  journal = {Proceedings of the IRE},
  year = {1961},
  volume = {49},
  pages = {228-235},
  number = {1},
  doi = {10.1109/JRPROC.1961.287814},
  issn = {0096-8390},
  keywords = {Binary codes;Data communication;Decoding;Encoding;Error correction
	codes;Feedback;Fires;Information theory;Mathematics;Polynomials},
  owner = {simon},
  timestamp = {2013.04.28}
}

@PHDTHESIS{petrovici2015phd,
  author = {Petrovici, Mihai A.},
  title = {Function vs. Substrate: Theory and Models for Neuromorphic Hardware},
  year = {2015},
  affiliation = {Ruprecht-Karls-University, Heidelberg},
  journal = {PhD thesis}
}

@MISC{petrovici12phdthesis,
  author = {Petrovici, Mihai A.},
  howpublished = {{\it PhD thesis}, University of Heidelberg, in preparation},
  year = {2012},
  keywords = {vision}
}

@INPROCEEDINGS{pettey_icga87parallel,
  author = {Pettey, C. B. and Leuze, M. R. and Grefenstette J. J.},
  title = {A Parallel Genetic Algorithm},
  booktitle = {Proceedings of the 2nd International Conference on Genetic Algorithms},
  year = {1987},
  editor = {J. J. Grefenstette},
  pages = {155--161},
  address = {Hillsdale, New Jersey},
  publisher = {Lawrence Erlbaum}
}

@MISC{pfeil2012_oral,
  author = {Thomas Pfeil},
  title = {Personal communication},
  year = {2012},
  owner = {simon},
  timestamp = {2013.04.07}
}

@ARTICLE{Pfeil2012,
  author = {Pfeil, Thomas and Potjans, Tobias C and Schrader, Sven and Potjans,
	Wiebke and Schemmel, Johannes and Diesmann, Markus and Meier, Karlheinz},
  title = {Is a 4-bit synaptic weight resolution enough? - Constraints on enabling
	spike-timing dependent plasticity in neuromorphic hardware},
  journal = {Frontiers in Neuroscience},
  year = {2012},
  volume = {6},
  number = {90},
  abstract = {Large-scale neuromorphic hardware systems typically bear the trade-o?
	be- tween detail level and required chip resources. Especially when
	implementing spike-timing-dependent plasticity, reduction in resources
	leads to limitations as compared to ?oating point precision. By design,
	a natural modi?cation that saves resources would be reducing synaptic
	weight resolution. In this study, we give an estimate for the impact
	of synaptic weight discretization on di?erent levels, ranging from
	random walks of individual weights to computer simulations of spiking
	neural networks. The FACETS wafer-scale hardware system o?ers a 4-bit
	resolution of synaptic weights, which is shown to be su?cient within
	the scope of our network benchmark. Our ?ndings indicate that increasing
	the resolution may not even be useful in light of further restrictions
	of customized mixed-signal synapses. In ad- dition, variations due
	to production imperfections are investigated and shown to be uncritical
	in the context of the presented study. Our results represent a general
	framework for setting up and con?guring hardware-constrained synapses.
	We sug- gest how weight discretization could be considered for other
	backends dedicated to large-scale simulations. Thus, our proposition
	of a good hardware veri?cation practice may rise synergy e?ects between
	hardware developers and neuroscientists.},
  doi = {10.3389/fnins.2012.00090},
  file = {:Pfeil12_90.pdf:PDF},
  issn = {1662-453X}
}

@phdthesis{pfeil2015phd,
  author   = {Pfeil, Thomas},
  title    = {Exploring the potential of brain-inspired computing},
  school   = {Ruprecht-Karls-Universit{\"a}t Heidelberg},
  year     = 2015
}

@ARTICLE{pfister06optimal,
  author = {Pfister, Jean-Pascal and Toyoizumi, Taro and Barber, David and Gerstner,
	Wulfram},
  title = {{Optimal Spike-Timing-Dependent Plasticity for Precise Action Potential
	Firing in Supervised Learning}},
  journal = {Neural Comput.},
  year = {2006},
  volume = {18},
  pages = {1318-1348},
  number = {6},
  abstract = {In timing-based neural codes, neurons have to emit action potentials
	at precise moments in time. We use a supervised learning paradigm
	to derive a synaptic update rule that optimizes by gradient ascent
	the likelihood of postsynaptic firing at one or several desired firing
	times. We find that the optimal strategy of up- and downregulating
	synaptic efficacies depends on the relative timing between presynaptic
	spike arrival and desired postsynaptic firing. If the presynaptic
	spike arrives before the desired postsynaptic spike timing, our optimal
	learning rule predicts that the synapse should become potentiated.
	The dependence of the potentiation on spike timing directly reflects
	the time course of an excitatory postsynaptic potential. However,
	our approach gives no unique reason for synaptic depression under
	reversed spike timing. In fact, the presence and amplitude of depression
	of synaptic efficacies for reversed spike timing depend on how constraints
	are implemented in the optimization problem. Two different constraints,
	control of postsynaptic rates and control of temporal locality, are
	studied. The relation of our results to spike-timing-dependent plasticity
	and reinforcement learning is discussed.},
  eprint = {http://neco.mitpress.org/cgi/reprint/18/6/1318.pdf},
  keywords = {plasticity spiking},
  url = {http://neco.mitpress.org/cgi/content/abstract/18/6/1318}
}

@article{pickard1976generalizations,
  title={Generalizations of the Goldman-Hodgkin-Katz equation},
  author={Pickard, William F},
  journal={Mathematical Biosciences},
  volume={30},
  number={1},
  pages={99--111},
  year={1976},
  publisher={Elsevier}
}

@webpage{PixiJS,
  title={{PixiJS 5}},
  year={2019},
  url={https://www.pixijs.com/}
}

@ARTICLE{qiao2015reconfigurable,
	author={Qiao, Ning and Mostafa, Hesham and Corradi, Federico and Osswald, Marc and Stefanini, Fabio and Sumislawska, Dora and Indiveri, Giacomo},
	title={A Re-configurable On-line Learning Spiking Neuromorphic Processor comprising 256 neurons and 128K synapses},
	journal={Frontiers in Neuroscience},
	volume={9},
	year={2015},
	number={141},
	url={http://www.frontiersin.org/neuromorphic_engineering/10.3389/fnins.2015.00141/abstract},
	doi={10.3389/fnins.2015.00141},
	issn={1662-453X} ,
	abstract={Implementing compact, low-power artificial neural processing systems with real-time on-line learning abilities is still an open challenge. In this paper we present a full-custom mixed-signal VLSI device with neuromorphic learning circuits that emulate the biophysics of real spiking neurons and dynamic synapses for exploring the properties of computational neuroscience models and for building brain-inspired computing systems. The proposed architecture allows the on-chip configuration of a wide range of network connectivities, including recurrent and deep networks, with short-term and long-term plasticity. The device comprises 128 K analog synapse and 256 neuron circuits with biologically plausible dynamics and bi-stable spike-based plasticity mechanisms that endow it with on-line learning abilities. In addition to the analog circuits, the device comprises also asynchronous digital logic circuits for setting different synapse and neuron properties as well as different network configurations. This
		prototype device, fabricated using a 180 nm 1P6M CMOS process, occupies an area of 51.4 mm2, and consumes approximately 4 mW for typical experiments, for example involving attractor networks. Here we describe the details of the overall architecture and of the individual circuits and present experimental results that showcase its potential. By supporting a wide range of cortical-like computational modules comprising plasticity mechanisms, this device will enable the realization of intelligent autonomous systems with on-line learning capabilities.}
}

@INPROCEEDINGS{reuther2019survey,
  author = {Reuther, Albert and Michaleas, Peter and Jones, Michael and Gadepally, Vijay and Samsi, Siddharth and Kepner, Jeremy},
  booktitle = {2019 {IEEE} High Performance Extreme Computing Conference ({HPEC})},
  title = {Survey and Benchmarking of Machine Learning Accelerators},
  year = 2019,
  pages = {1--9},
  doi = {10.1109/HPEC.2019.8916327}
}

@ARTICLE{ricciardi1979ouprocess,
  author = {Ricciardi, Luigi M. and Sacerdote, Laura},
  title = {The Ornstein-Uhlenbeck Process as a Model for Neuronal Activity},
  journal = {Biological Cybernetics},
  year = {1979},
  volume = {35},
  pages = {1-9},
}

@ARTICLE{ricciardi1988fptdensity,
  author = {Ricciardi, Luigi M. and Sato, Shunsuke},
  title = {First-Passage-Time Density and Moments of the Ornstein-Uhlenbeck Process},
  journal = {Journal of Applied Probability},
  year = {1988},
  volume = {25},
  pages = {43-57},
}

@article{richardson2006statistics,
  title={Statistics of subthreshold neuronal voltage fluctuations due to conductance-based synaptic shot noise},
  author={Richardson, Magnus JE and Gerstner, Wulfram},
  journal={Chaos: An Interdisciplinary Journal of Nonlinear Science},
  volume={16},
  number={2},
  pages={026106},
  year={2006},
  publisher={AIP Publishing}
}

@article{richet1910address,
  title={An Address ON ANCIENT HUMORISM AND MODERN HUMORISM: Delivered at the International Congress of Physiology held in Vienna, September 27th to 30th},
  author={Richet, Charles},
  journal={British medical journal},
  volume={2},
  number={2596},
  pages={921},
  year={1910},
  publisher={BMJ Group}
}

@INPROCEEDINGS{rutenbar_dac00,
  author = {Phelps, Rodney and Krasnicki, Michael J and Rutenbar, Rob A and Carley,
	L Richard and Hellums, James R},
  title = {A Case Study of Synthesis for Industrial-Scale Analog {IP}: Redesign
	of the Equalizer/Filter Frontend for an {ADSL} {CODEC}},
  booktitle = {Proceedings of the 2000 ACM/IEEE Design Automation Conference (DAC
	00)},
  year = {2000},
  pages = {1--6},
  address = {Los Angeles, CA, USA},
  month = jun,
  groupsearch = {0},
  howpublished = {CD-ROM: ISBN: 1-58113-188-7}
}

@ARTICLE{rueckauer2017conversion,
  AUTHOR={Rueckauer, Bodo and Lungu, Iulia-Alexandra and Hu, Yuhuang and Pfeiffer, Michael and Liu, Shih-Chii},
  TITLE={Conversion of Continuous-Valued Deep Networks to Efficient Event-Driven Networks for Image Classification},
  JOURNAL={Frontiers in Neuroscience},
  VOLUME={11},
  PAGES={682},
  YEAR={2017},
  URL={https://www.frontiersin.org/article/10.3389/fnins.2017.00682},
  DOI={10.3389/fnins.2017.00682},
  ISSN={1662-453X},
}

@INPROCEEDINGS{rueckauer2018conversion,
  author={Bodo Rueckauer and Shih-Chii Liu},
  booktitle={2018 IEEE International Symposium on Circuits and Systems (ISCAS)},
  title={Conversion of analog to spiking neural networks using sparse temporal coding},
  year=2018,
  volume={},
  number={},
  pages={1--5},
  keywords={encoding;neural nets;sparse temporal coding;analog neural network;ANN;computation cost;memory access;time-to-first-spike;temporal information;spiking neural network;temporal encoding scheme;SNN;TTFS;MNIST handwritten dataset;Encoding;Mathematical model;Biological neural networks;Hardware;Computational modeling},
  doi={10.1109/ISCAS.2018.8351295},
  ISSN={2379-447X},
  month=May,
}

@ARTICLE{rueckauer2021nxtf,
  title={{NxTF}: An API and Compiler for Deep Spiking Neural Networks on Intel Loihi},
  author={Rueckauer, Bodo and Bybee, Connor and Goettsche, Ralf and Singh, Yashwardhan and Mishra, Joyesh and Wild, Andreas},
  journal={arXiv preprint},
  aprint = {2101.04261},
  archivePrefix = {arXiv},
  primaryClass = {cs.LG},
  year = 2021,
  month = jan
}

@misc{park2018deep,
  title = {Deep Learning Inference in Facebook Data Centers: Characterization, Performance Optimizations and Hardware Implications},
  author = {Jongsoo Park and Maxim Naumov and Protonu Basu and Summer Deng and Aravind Kalaiah and Daya Khudia and James Law and Parth Malani and Andrey Malevich and Satish Nadathur and Juan Pino and Martin Schatz and Alexander Sidorov and Viswanath Sivakumar and Andrew Tulloch and Xiaodong Wang and Yiming Wu and Hector Yuen and Utku Diril and Dmytro Dzhulgakov and Kim Hazelwood and Bill Jia and Yangqing Jia and Lin Qiao and Vijay Rao and Nadav Rotem and Sungjoo Yoo and Mikhail Smelyanskiy},
  year = 2018,
  eprint = {1811.09886},
  archivePrefix = {arXiv},
  primaryClass = {cs.LG}
}

@ARTICLE{Pfeil12_90,
  author =       {Pfeil, Thomas and Potjans, Tobias C and Schrader, Sven and Potjans, Wiebke and Schemmel, Johannes and Diesmann, Markus and Meier, Karlheinz},
  title =        {Is a 4-bit synaptic weight resolution enough? - Constraints on enabling spike-timing dependent plasticity in neuromorphic hardware},
  journal =      {Frontiers in Neuroscience},
  year =         2012,
  volume =       6,
  number =       90,
  abstract =     {Large-scale neuromorphic hardware systems typically bear the trade-o? be- tween detail level and required chip resources. Especially when implementing spike-timing-dependent plasticity, reduction in resources leads to limitations as compared to ?oating point precision. By design, a natural modi?cation that saves resources would be reducing synaptic weight resolution. In this study, we give an estimate for the impact of synaptic weight discretization on di?erent levels, ranging from random walks of individual weights to computer simulations of spiking neural networks. The FACETS wafer-scale hardware system o?ers a 4-bit resolution of synaptic weights, which is shown to be su?cient within the scope of our network benchmark. Our ?ndings indicate that increasing the resolution may not even be useful in light of further restrictions of customized mixed-signal synapses. In ad- dition, variations due to production imperfections are investigated and shown to be uncritical in the context of the presented study. Our results represent a general framework for setting up and con?guring hardware-constrained synapses. We sug- gest how weight discretization could be considered for other backends dedicated to large-scale simulations. Thus, our proposition of a good hardware veri?cation practice may rise synergy e?ects between hardware developers and neuroscientists.},
  doi =          {10.3389/fnins.2012.00090},
  issn =         {1662-453X}
}

@INPROCEEDINGS{pfeil2013neuromorphic,
  author={Thomas Pfeil and Anne-Christine Scherzer and Johannes Schemmel and Karlheinz Meier},
  booktitle={Neural Networks (IJCNN), The 2013 International Joint Conference on},
  title={Neuromorphic learning towards nano second precision},
  year={2013},
  pages={1-5},
  keywords={Hebbian learning;acoustic signal processing;hearing;medical signal processing;neural nets;azimuthal angle;highly accelerated hardware substrate;interaural time differences;nanosecond precision;neuromorphic hardware system;neuromorphic learning;neuromorphic substrate;neuronal parameters;on-chip Hebbian learning mechanism;spiking neural networks;synaptic delays;synaptic parameters;temporal coding;Delays;Emulation;Hardware;Neuromorphics;Neurons;System-on-chip;Vectors},
  doi={10.1109/IJCNN.2013.6706828},
  ISSN={2161-4393},
  month={Aug},
}

@article{pfeil2013neuromorphicarxiv,
  title={Neuromorphic Learning towards Nano Second Precision},
  author={Pfeil, Thomas and Scherzer, Anne-Christine and Schemmel, Johannes and Meier, Karlheinz},
  journal={arXiv preprint arXiv:1309.4283},
  year={2013}
}

@PHDTHESIS{philipp08phd,
  author = {Stefan Philipp},
  title = {Design and Implementation of a Multi-Class Network Architecture for
	Hardware Neural Networks},
  school = {Ruprecht-Karls Universit\"at Heidelberg},
  year = {2008},
  howpublished = {Ph.D. thesis, Universit\"at Heidelberg, HD-KIP-08-20}
}

@ARTICLE{stefan_arq,
  author = {Philipp, S.},
  title = {Generic ARQ Protocol in VHDL},
  journal = {Internal FACETS documentation.},
  year = {2008}
}

@MISC{stefan_arqtalk,
	author = {Philipp, S.},
	title = {Generic ARQ Protocol in VHDL},
	year = {2008},
	howpublished = {Internal Visions presentation}
}

@ARTICLE{stefanini2014pyncs,
	AUTHOR={Stefanini, Fabio and Neftci, Emre O. and Sheik, Sadique and Indiveri, Giacomo},
	TITLE={PyNCS: A Microkernel for High-level Definition and Configuration of Neuromorphic Electronic Systems},
	JOURNAL={Frontiers in Neuroinformatics},
	VOLUME=8,
	PAGES=73,
	YEAR=2014,
	URL={https://www.frontiersin.org/article/10.3389/fninf.2014.00073},
	DOI={10.3389/fninf.2014.00073},
	ISSN={1662-5196}
}

@INPROCEEDINGS{philipp_iwann07,
  author = {S. Philipp and A. Gr\"ubl and K. Meier and J. Schemmel},
  title = {Interconnecting {VLSI} Spiking Neural Networks Using Isochronous
	Connections},
  booktitle = {Proceedings of the 9th International Work-Conference on Artificial
	Neural Networks ({IWANN}'2007)},
  year = {2007},
  volume = {LNCS 4507},
  pages = {471-478},
  month = sep,
  publisher = {Springer Verlag},
  howpublished = {Lecture Notes in Computer Science},
  keywords = {neuromorphic}
}

@BOOK{pissanetzky84sparsematrix,
  title = {Sparse Matrix Technology},
  publisher = {Academic Press},
  year = {1984},
  author = {Sergio Pissanetzky},
  address = {London},
  owner = {mueller},
  timestamp = {2008.09.04}
}

@ARTICLE{plenz96neural,
  author = {D Plenz and Ad Aertsen},
  title = {Neural dynamics in cortex-striatum co-cultures--II. Spatiotemporal
	characteristics of neuronal activity},
  journal = {Neuroscience},
  year = {1996},
  volume = {70},
  pages = {893--924},
  number = {4},
  month = {Feb},
  abstract = {Neural dynamics in organotypic cortex-striatum co-cultures grown for
	three to six weeks under conditions of dopamine deficiency are described.
	Single neuron activities were recorded intra- and extracellularly,
	and spatiotemporal spreading of population activity was mapped using
	voltage-sensitive dyes. The temporal properties of spike firing were
	characterized by interspike interval histograms, autocorrelation
	and crosscorrelation. Cortical pyramidal neurons (n = 40) showed
	irregular firing with a weak tendency to burst or to oscillate. Crosscorrelations
	revealed strong near-coincident firing and synaptic interactions.
	Disinhibition was a notable feature in a strongly firing cortical
	interneuron. Cortical activity spread in the co-culture, thus inducing
	an overall, homogeneous depolarization in the striatal part. Striatal
	cells were divided into principal cells and type I and II secondary
	cells. Principal cells (n = 40) were similar to those reported previously
	in vivo. Spiking activity ranged from irregular spiking at very low
	rates to episodic bursting, with an average burst duration of 1 s.
	Interspike intervals were single-peaked. Intracellular recordings
	revealed characteristic, long-lasting subthreshold depolarizations
	("enabled state") that were shortened by local muscarinic receptor
	blockade. During prolonged time periods in the "enabled state", locally
	applied bicuculline induced strong firing in most principal neurons.
	Striatal secondary type I neurons (n = 25) showed high spiking rates,
	single- and double-peaked interval histograms and low-threshold,
	short-lasting stereotyped bursting activity and occasional rhythmic
	bursting. The firing of these neurons was increased by bicuculline.
	Crosscorrelations showed synchronization of these cells with principal
	cell activity. Secondary type II neurons (n = 15) revealed tonic,
	irregular firing patterns similar to cortical neurons, except with
	occasional firing in doublet spikes. We conclude that under conditions
	of dopamine deficiency in corticostriatal co-cultures (i) the cortex
	induces the "enabled" state and typical bursting mode in striatal
	principal neurons; (ii) principal neurons are strongly inhibited
	during the "enabled" state; (iii) muscarinic activity, presumably
	from tonically active striatal cholinergic interneurons, stabilizes
	the "enabled" state; (iv) striatal GABAergic interneurons receives
	synaptic inhibition and take part in synchronized activity among
	striatal principal cells. Our results favor the view of the striatum
	as a lateral inhibition network.},
  affiliation = {Max-Planck-Institut f{\"u}r biologische Kybernetik, T{\"u}bingen,
	Germany.}
}

@ARTICLE{poggio99hierarchical,
  author = {M. Riesenhuber T. Poggio},
  title = {Hierarchical models of object recognition in cortex},
  journal = {Nature Neuroscience},
  year = {1999},
  volume = {2},
  pages = {1019--1025},
  keywords = {convolutional NN},
  owner = {fieres}
}

@article{pouget2013probabilistic,
  title={Probabilistic brains: knowns and unknowns},
  author={Pouget, Alexandre and Beck, Jeffrey M and Ma, Wei Ji and Latham, Peter E},
  journal={Nature Neuroscience},
  volume={16},
  number={9},
  pages={1170--1178},
  year={2013},
  publisher={Nature Publishing Group}
}

@ARTICLE{pospischil07,
  author = {Martin Pospischil and Zuzanna Piwkowska and Michelle Rudolph and
	Thierry Bal and Alain Destexhe},
  title = {Calculating event-triggered average synaptic conductances from the
	membrane potential},
  journal = {Journal of Neurophysiology},
  year = {2007},
  volume = {97},
  pages = {2544},
  url = {http://www.citebase.org/abstract?id=oai:arXiv.org:q-bio/0609030}
}

@ARTICLE{pospischil08,
  author = {Pospischil, Martin and Toledo-Rodriguez, Maria and Monier, Cyril
	and Piwkowska, Zuzanna and Bal, Thierry and Fr{\'e}gnac, Yves and
	Markram, Henry and Destexhe, Alain},
  title = {Minimal Hodgkin--Huxley type models for different classes of cortical
	and thalamic neurons},
  journal = {Biological Cybernetics},
  year = {2008},
  volume = {99},
  pages = {427--441},
  number = {4},
  month = {Nov},
  abstract = {Abstract\&nbsp;\&nbsp;We review here the development of Hodgkin--Huxley
	(HH) type models of cerebral cortex and thalamic neurons for network
	simulations. The intrinsic electrophysiological properties of cortical
	neurons were analyzed from several preparations, and we selected
	the four most prominent electrophysiological classes of neurons.
	These four classes are ``fast spiking'' ``regular spiking'' ``intrinsically
	bursting'' and ``low-threshold spike'' cells. For each class, we
	fit ``minimal'' HH type models to experimental data. The models contain
	the minimal set of voltage-dependent currents to account for the
	data. To obtain models as generic as possible, we used data from
	different preparations in vivo and in vitro, such as rat somatosensory
	cortex and thalamus, guinea-pig visual and frontal cortex, ferret
	visual cortex, cat visual cortex and cat association cortex. For
	two cell classes, we used automatic fitting procedures applied to
	several cells, which revealed substantial cell-to-cell variability
	within each class. The selection of such cellular models constitutes
	a necessary step towards building network simulations of the thalamocortical
	system with realistic cellular dynamical properties.},
  day = {01},
  doi = {10.1007/s00422-008-0263-8},
  url = {http://dx.doi.org/10.1007/s00422-008-0263-8}
}

@article{pospischil2011comparison,
  title={Comparison of different neuron models to conductance-based post-stimulus time histograms obtained in cortical pyramidal cells using dynamic-clamp in vitro},
  author={Pospischil, Martin and Piwkowska, Zuzanna and Bal, Thierry and Destexhe, Alain},
  journal={Biological cybernetics},
  volume={105},
  number={2},
  pages={167--180},
  year={2011},
  publisher={Springer}
}

@ARTICLE{Potjans2011,
  author = {Potjans, Wiebke AND Diesmann, Markus AND Morrison, Abigail},
  title = {An Imperfect Dopaminergic Error Signal Can Drive Temporal-Difference
	Learning},
  journal = {PLoS Comput Biol},
  year = {2011},
  volume = {7},
  pages = {e1001133},
  number = {5},
  month = {05},
  abstract = {<title>Author Summary</title> <p>What are the physiological changes
	that take place in the brain when we solve a problem or learn a new
	skill? It is commonly assumed that behavior adaptations are realized
	on the microscopic level by changes in synaptic efficacies. However,
	this is hard to verify experimentally due to the difficulties of
	identifying the relevant synapses and monitoring them over long periods
	during a behavioral task. To address this question computationally,
	we develop a spiking neuronal network model of actor-critic temporal-difference
	learning, a variant of reinforcement learning for which neural correlates
	have already been partially established. The network learns a complex
	task by means of an internally generated reward signal constrained
	by recent findings on the dopaminergic system. Our model combines
	top-down and bottom-up modelling approaches to bridge the gap between
	synaptic plasticity and system-level learning. It paves the way for
	further investigations of the dopaminergic system in reward learning
	in the healthy brain and in pathological conditions such as Parkinson's
	disease, and can be used as a module in functional models based on
	brain-scale circuitry.</p>},
  doi = {10.1371/journal.pcbi.1001133},
  owner = {simon},
  publisher = {Public Library of Science},
  timestamp = {2012.12.17},
  url = {http://dx.doi.org/10.1371%2Fjournal.pcbi.1001133}
}

@ARTICLE{potjans2012cell,
    author = {Tobias C. Potjans and Markus Diesmann},
    title = {The Cell-Type Specific Cortical Microcircuit: Relating Structure and Activity in a Full-Scale Spiking Network Modela},
    journal = {Cereb. Cortex},
    year = {2012},
    volume = {24},
    issue = {3},
    pages = {785--806},
    doi = {10.1093/cercor/bhs358}
}

@INPROCEEDINGS{Pouwelse2001,
  author = {Pouwelse, Johan and Langendoen, Koen and Sips, Henk},
  title = {Dynamic voltage scaling on a low-power microprocessor},
  booktitle = {Proceedings of the 7th annual international conference on Mobile
	computing and networking},
  year = {2001},
  pages = {251--259},
  organization = {ACM},
  owner = {simon},
  timestamp = {2013.04.22}
}

@TECHREPORT{powerisa_206,
  author = {{PowerISA}},
  title = {Power{ISA} Version 2.06 Revision B},
  institution = {Power.org},
  year = 2010,
  month = jul,
  url = {http://www.power.org/resources/reading/},
  type = {Specification}
}

@TECHREPORT{powerisa_203,
  author = {{PowerISA}},
  title = {Power{ISA} Version 2.03},
  institution = {Power.org},
  year = 2006,
  month = sep,
  url = {http://www.power.org/resources/reading/},
  type = {Specification}
}

@mastersthesis{poweritcalib,
  author   = {Patrick Nisble},
  title    = {Calibration and Regulation of a Power Supply in the BrainScaleS System},
  school   = {Universität Heidelberg},
  year     = {2018},
  type     = {Bachelor thesis}
}

@ARTICLE{prechelt95notes,
  author = {L. Prechelt},
  title = {Some notes on neural learning algorithm benchmarking},
  journal = {Neurocomputing},
  year = {1995},
  volume = {9},
  pages = {343 - 347},
  number = {3},
  citeseerurl = {citeseer.nj.nec.com/prechelt95some.html},
  keywords = {learning},
  text = {L. Prechelt, Some notes on neural learning algorithm benchmarking,
	Neurocomputing, vol. 9, no. 3, pp. 343--347, 1995.}
}

@TECHREPORT{prechelt94proben,
  author = {Prechelt, L.},
  title = {Proben1 --- {A} set of Neural Network Benchmark Problems and Benchmarking
	Rules},
  institution = {38 pages, Fakult\"at f\"ur Informatik, Universit\"at Karlsruhe},
  year = {1994},
  number = {21/94},
  file = {prechelt94proben.pdf:prechelt94proben.pdf:PDF},
  groupsearch = {0},
  url = {citeseer.nj.nec.com/prechelt94proben.html}
}

@ARTICLE{prescott2006,
  author = {Prescott, S. A. and Ratte, S. and De Koninck, Y. and Sejnowski, T.
	J. },
  title = {Nonlinear interaction between shunting and adaptation controls a
	switch between integration and coincidence detection in pyramidal
	neurons.},
  journal = {J Neurosci},
  year = {2006},
  volume = {26},
  pages = {9084--97},
  number = {36},
  month = {Sep June},
  abstract = {The membrane conductance of a pyramidal neuron in vivo is substantially
	increased by background synaptic input. Increased membrane conductance,
	or shunting, does not simply reduce neuronal excitability. Recordings
	from hippocampal pyramidal neurons using dynamic clamp revealed that
	adaptation caused complete cessation of spiking in the high conductance
	state, whereas repetitive spiking could persist despite adaptation
	in the low conductance state. This behavior was reproduced in a phase
	plane model and was explained by a shunting-induced increase in voltage
	threshold. The increase in threshold allows greater activation of
	the M current (I(M)) at subthreshold potentials and reduces the minimum
	adaptation required to stabilize the system; in contrast, activation
	of the afterhyperpolarization current is unaffected by the increase
	in threshold and therefore remains unable to stop repetitive spiking.
	The nonlinear interaction between shunting and I(M) has other important
	consequences. First, timing of spikes elicited by brief stimuli is
	more precise when background spikes elicited by sustained input are
	prohibited, as occurs exclusively with I(M)-mediated adaptation in
	the high conductance state. Second, activation of I(M) at subthreshold
	potentials, which is increased in the high conductance state, hyperpolarizes
	average membrane potential away from voltage threshold, allowing
	only large, rapid fluctuations to reach threshold and elicit spikes.
	These results suggest that the shift from a low to high conductance
	state in a pyramidal neuron is accompanied by a switch from encoding
	time-averaged input with firing rate to encoding transient inputs
	with precisely timed spikes, in effect, switching the operational
	mode from integration to coincidence detection.},
  citeulike-article-id = {941903},
  keywords = {action, adaptation, animals, cells, cellsphysiology, computer, cultured,
	differential, govt, membrane, models, nerve, netphysiology, neurological,
	neuronal, non-us, physiologicalphysiology, plasticityphysiology,
	potentialsphysiology, pyramidal, rats, research, simulation, sprague-dawley,
	support, synaptic, thresholdphysiology, transmissionphysiology},
  posted-at = {2006-11-13 16:33:43},
  priority = {2}
}

@BOOK{press92numerical,
  title = {Numerical recipes in C: The art of scientific computing},
  publisher = {Cambridge University Press},
  year = {1992},
  author = {W.H. Press and B.P. Flannery and S.A. Teukolsky and W.T Vetterling},
  edition = {Online ed.},
  owner = {fieres},
  url = {http://www.nr.com}
}

@article{probst2015probabilistic,
  title={Probabilistic inference in discrete spaces can be implemented into networks of {LIF} neurons},
  author={Probst, Dimitri and Petrovici, Mihai A and Bytschok, Ilja and Bill, Johannes and Pecevski, Dejan and Schemmel, Johannes and Meier, Karlheinz},
  journal={Frontiers in computational neuroscience},
  volume={9},
  year={2015},
  publisher={Frontiers Media SA}
}

@article{pronold2021routingefficient,
  title={Routing brain traffic through the von Neumann bottleneck: Efficient cache usage in spiking neural network simulation code on general purpose computers},
  author={Pronold, Jari and Jordan, Jakob and Wylie, Brian JN and Kitayama, Itaru and Diesmann, Markus and Kunkel, Susanne},
  archivePrefix = {arXiv},
  eprint={2109.12855},
  year=2021
}

@article{pronold2021routingparallel, crossref={pronold2022routingparallel}}
@article{pronold2022routingparallel,
  title={Routing Brain Traffic Through the Von Neumann Bottleneck: Parallel Sorting and Refactoring},
  author={Pronold, Jari and Jordan, Jakob and Wylie, Brian J. N. and Kitayama, Itaru and Diesmann, Markus and Kunkel, Susanne},
  journal={Front. Neuroinform.},
  volume=15,
  year=2022,
  doi={10.3389/fninf.2021.785068},
  issn={1662-5196},
  archivePrefix={arXiv},
  eprint={2109.11358}
}

@ARTICLE{prut98spatiotemporalstructure,
  author = {Yifat Prut and Eilon Vaadia and Hagai Bergman and Iris Haalman and
	Hamutal Slovin and Moshe Abeles},
  title = {Spatiotemporal structure of cortical activity: Properties and behavioral
	relevance},
  journal = {J. Neurophysiol},
  year = {1998},
  volume = {79},
  pages = {2857--2874}
}

@ARTICLE{pruegelbennet03finite,
  author = {Pr{\"u}gel-Bennet, A.},
  title = {Modelling Finite Populations},
  journal = {Proceedings of Foundations of Genetic Algorithms},
  year = {2003},
  volume = {7},
  pages = {99--114},
  editor = {De Jong, K. and Poli, R. and Rowe, J. E.},
  file = {pruegelbennet03finite.pdf:pruegelbennet03finite.pdf:PDF}
}

@ARTICLE{pruegelbennet01modelling,
  author = {Pr{\"u}gel-Bennet, A. and Rogers, A.},
  title = {Modelling {GA} Dynamics},
  journal = {Proceedings of Theoretical Aspects of Evolutionary Computation},
  year = {2001},
  pages = {59-86},
  editor = {Kallel, L. and Naudts, B. and Rogers, A.},
  file = {pruegelbennet01modelling.pdf:pruegelbennet01modelling.pdf:PDF}
}

@ARTICLE{pruegelbennet04statistical,
  author = {Pr{\"u}gel-Bennet, A. and Shapiro, J. L.},
  title = {Analysis of Genetic Algorithms Using Statistical Mechanics},
  journal = {Physical Review Letters},
  year = {1994},
  volume = {72},
  pages = {1305-1309},
  number = {9},
  month = {February}
}

@INPROCEEDINGS{pujol_icec98dual,
  author = {Pujol, J. and Poli, R.},
  title = {Evolving Neural Networks Using a Dual Representation with a Combined
	Crossover Operator},
  booktitle = {Proceedings of the IEEE International Conference on Evolutionary
	Computation (ICEC)},
  year = {1998},
  pages = {416--421},
  file = {pujol_icec98dual.pdf:pujol_icec98dual.pdf:PDF}
}

@ARTICLE{purschke05ratcap,
  author = {M.L. Purschke and A. Kandasamy and A. Kriplani and R. Lecomte and
	P. O'Connor and J.-F. Pratte and V. Radeka and D. Schlyer and S.
	Southekal and S. Stoll and P. Vaska and A. Villanueva and C.L. Woody
	and S. Junnakar and S. Krishnamoorthy and S. Shokouhi and R. Fontaine
	and V. Dzhordzhadze},
  title = {The RatCAP conscious small animal PET tomography},
  journal = {IEEE-NPSS Real Time Conference},
  year = {2005},
  abstract = { Summary:The RatCAP is a small, head mounted PET tomograph designed
	and built to image the brain of an awake rat. It allows PET imaging
	studies to be carried out on laboratory rats without the use of anesthesia,
	which severely suppresses brain functions and affects many of the
	neurological activities that one would like to study using PET. The
	tomograph consists of a 4 cm diameter ring containing 12 block detectors,
	each of which is comprised of a 4 times 8 array of 2.2 times 2.2
	times 5 mm3 LSO crystals read out with a matching APD array. The
	APDs are read out using a custom designed ASIC and VME readout system.
	We have successfully performed a system integration test with a partially
	instrumented tomograph ring. We present the recent progress towards
	a fully integrated system. },
  address = {Los Alamitos, CA, USA},
  isbn = {0-7803-9183-7},
  publisher = {IEEE Computer Society}
}

@BOOK{purves2001neuroscience,
  title = {Neuroscience},
  year = {2001},
  author = {Purves, D. and Augustine G. J. and Fitzpatrich D.},
  publisher = {Sinauer Associates}
}

@MISC{pygui,
  author = {{Python GUIs}},
  title = {Graphical User Interface Packages for {P}ython -- A collection},
  howpublished = {\url{http://wiki.python.org/moin/CategoryPyGUI}},
  year = {2008}
}

@MISC{python07homepage,
  author = {{Python Software Foundation}},
  title = {The {Python} Programming Language - Website},
  howpublished = {\url{http://www.python.org}},
  year = {2007}
}

@MANUAL{python_homepage,
  key = {Python},
  organization = {Python Software Foundation},
  title = {{The Python Programming Language}},
  url = {http://www.python.org},
  year = 2014,
  note = {[Online; accessed: 2014-04-29]}
}

@MANUAL{pynn_homepage,
  key = {PyNN},
  organization = {The NeuralEnsemble Initiative},
  title = {A {Python} package for simulator-independent specification of neuronal network models},
  url = {http://www.neuralensemble.org/PyNN},
  year = 2014,
  note = {[Online; accessed: 2014-04-29]}
}

@ARTICLE{qu2022review,
  author = {Qu, Peng and Yang, Le and Zheng, Weimin and Zhang, Youhui},
  title = {A review of basic software for brain-inspired computing},
  journal = {CCF Transactions on High Performance Computing},
  year = 2022,
  month = mar,
  day = 16,
  pages = {1--9},
  publisher = {Springer},
  abstract = {Brain-inspired computing, which is inspired by the information processing procedure and the biophysiological structure of the brain, is believed to have the potential to drive the next wave of computer engineering and provide a promising way for the next generation of artificial intelligence. The basic software for brain-inspired computing is the core link to realize the research goals of brain-inspired computing and build the ecological environment of brain-inspired computing applications. This paper reviews the status of the three major kinds of basic software for brain-inspired computing. Namely, the toolchains for neuromorphic chips, the software simulation frameworks, and the frameworks that integrate spiking neural networks (SNNs) and deep neural networks (DNNs). Afterward, we point out that a ``general-purpose'' hierarchical and HW/SW decoupled basic software framework would be beneficial to both the (computational) neuroscience and brain-inspired intelligence fields. And the notion ``general-purpose'' refers to the decoupling of software and hardware and supports the integration of computer science and neuroscience related research.},
  issn = {2524-4930},
  doi = {10.1007/s42514-022-00092-1}
}

@ARTICLE{quiroga05,
  author = {Quiroga, Quian Q. and Reddy, L. and Kreiman, G. and Koch, C. and
	Fried, I. },
  title = {Invariant visual representation by single neurons in the human brain},
  journal = {Nature},
  year = {2005},
  volume = {435},
  pages = {1102--1107},
  number = {7045},
  citeulike-article-id = {235179},
  doi = {http://dx.doi.org/10.1038/nature03687},
  issn = {0028-0836},
  keywords = {cogneuro},
  posted-at = {2008-02-01 01:16:00},
  priority = {0},
  publisher = {Nature Publishing Group},
  url = {http://dx.doi.org/10.1038/nature03687}
}

@ARTICLE{ragazzini1947analysis,
  author = {Ragazzini, J.R. and Randall, R.H. and Russell, F.A.},
  title = {Analysis of problems in dynamics by electronic circuits},
  journal = {Proceedings of the IRE},
  year = {1947},
  volume = {35},
  pages = {444--452},
  number = {5},
  publisher = {IEEE}
}

@INBOOK{rall89,
  pages = {9--92},
  title = {Cable theory for dendritic neurons},
  publisher = {MIT Press},
  year = {1989},
  author = {Rall, Wilfrid},
  address = {Cambridge, MA, USA},
  acmid = {94608},
  book = {Methods in neuronal modeling},
  isbn = {0-262-11133-0},
  numpages = {84}
}

@ARTICLE{rall1962electrophysiology,
  author = {Rall, W.},
  title = {Electrophysiology of a dendritic neuron model},
  journal = {Biophysical journal},
  year = {1962},
  volume = {2},
  pages = {145},
  number = {2 Pt 2},
  publisher = {The Biophysical Society}
}

@ARTICLE{rall1959branching,
  author = {Rall, W.},
  title = {Branching dendritic trees and motoneuron membrane resistivity},
  journal = {Experimental neurology},
  year = {1959},
  volume = {1},
  pages = {491--527},
  number = {5},
  publisher = {Elsevier}
}

@ARTICLE{ramakrishnan2011floating,
  author = {Ramakrishnan, S. and Hasler, P.E. and Gordon, C.},
  title = {Floating gate synapses with spike-time-dependent plasticity},
  journal = {Biomedical Circuits and Systems, IEEE Transactions on},
  year = {2011},
  volume = {5},
  pages = {244--252},
  number = {3},
  owner = {simon},
  publisher = {IEEE},
  timestamp = {2013.01.06}
}

@MASTERSTHESIS{raman2019towards,
	author = {Raman, Aruna},
	title = {Towards an Automated Platform to Implement Artificial Neural Network Topologies on Neuromorphic Hardware},
	school = {Universit{\"a}t Heidelberg},
	year = 2019,
	month = oct,
	type = {Master's thesis}
}

@BOOK{cajal1911histologie,
  title = {Histologie du Systeme Nerveux de l'homme et des Vertebres},
  year = {1911},
  author = {S {Ramon~y~Cajal}}
}

@INPROCEEDINGS{Rangan2010,
  author = {Rangan, V. and Ghosh, A. and Aparin, V. and Cauwenberghs, G.},
  title = {A subthreshold aVLSI implementation of the Izhikevich simple neuron
	model},
  booktitle = {Engineering in Medicine and Biology Society (EMBC), 2010 Annual International
	Conference of the IEEE},
  year = {2010},
  pages = {4164 -4167},
  month = {31 2010-sept. 4},
  doi = {10.1109/IEMBS.2010.5627392},
  issn = {1557-170X},
  keywords = {Izhikevich simple neuron model;MOS transistors;aVLSI;bursting dynamics;human-machine
	interface;implantable bioelectronics;log-domain circuit design;neural
	prostheses;neuron spiking;MOSFET;VLSI;biomedical electronics;equivalent
	circuits;man-machine systems;neural nets;physiological models;prosthetics;Models,
	Biological;Neurons;},
  owner = {simon},
  timestamp = {2012.12.14}
}

@article{ranzini2021experimental,
  title = {Experimental Investigation of Optoelectronic Receiver With Reservoir Computing in Short Reach Optical Fiber Communications},
  author = {Ranzini, Stenio M. and Dischler, Roman and Da Ros, Francesco and B{\"u}low, Henning and Zibar, Darko},
  journal = {Journal of Lightwave Technology},
  volume = 39,
  number = 8,
  pages = {2460--2467},
  year = 2021,
  publisher = {IEEE}
}

@InProceedings{rao2004hierarchical,
  Author         = {Rao, Rajesh P. N.},
  Title          = {Hierarchical Bayesian Inference in Networks of Spiking Neurons},
  booktitle      = {Advances in Neural Information Processing Systems},
  volume         = 17,
  Pages          = {1113--1120},
  url            = {http://papers.nips.cc/paper/2643-hierarchical-bayesian-inference-in-networks-of-spiking-neurons.pdf},
  year           = 2005
}

@ARTICLE{Rasche01,
  author = {Rasche, C. and Douglas, R.J.},
  title = {Forward- and backpropagation in a silicon dendrite},
  journal = {Neural Networks, IEEE Transactions on},
  year = {2001},
  volume = {12},
  pages = {386 -393},
  number = {2},
  month = {mar},
  abstract = {We have developed an analog very-large-scale integrated (aVLSI) electronic
	circuit that emulates a compartmental model of a neuronal dendrite.
	The horizontal conductances of the compartmental model are implemented
	as a switched capacitor network. The transmembrane conductances are
	implemented as transconductance amplifiers. The electrotonic properties
	of our silicon cable are qualitatively similar to those of the ideal
	passive cable that is commonly used to model mathematically the electrotonic
	behavior of neurons. In particular the propagation of excitatory
	postsynaptic potentials is realistic, and we are easily able to emulate
	such classical synaptic integration models as direction selectivity.
	We are also able to emulate the backpropagation into the dendrite
	of single somatic spikes and bursts of spikes. Thus, this silicon
	dendrite is suitable for incorporation in detailed silicon neurons
	operating in real-time; in particular for the emulation of forward-
	and backpropagating electrical activities found in real neurons },
  doi = {10.1109/72.914532},
  issn = {1045-9227},
  keywords = {aVLSI electronic circuit;analog VLSI electronic circuit;backpropagation;direction
	selectivity;electrotonic properties;excitatory postsynaptic potential
	propagation;forward propagation;horizontal conductances;neural nets;neuronal
	dendrite;real-time operation;silicon cable;silicon dendrite;silicon
	neurons;single somatic spikes;spike bursts;switched capacitor network;synaptic
	integration models;transconductance amplifiers;transmembrane conductances;VLSI;analogue
	integrated circuits;analogue processing circuits;backpropagation;bioelectric
	potentials;biomembrane transport;neural chips;real-time systems;somatosensory
	phenomena;switched capacitor networks;}
}

@BOOK{ray01xml,
  title = {Learning {XML}},
  publisher = {O'Reilly \& Associates, Inc.},
  year = {2001},
  author = {Ray, Eric T.},
  address = {101 Morris Street, Sebastopol, CA},
  isbn = {0-596-00046-4}
}

@ARTICLE{ray2008,
  author = {Ray,Subhasis and Bhalla, Upinder S.},
  title = {{PyMOOSE}: interoperable scripting in {Python} for {MOOSE}},
  journal = {Front. Neuroinform.},
  year = {2008},
  volume = {2},
  number = {6},
  abstract = {Python is emerging as a common scripting language for simulators.
	This opens up many possibilities for interoperability in the form
	of analysis, interfaces, and communications between simulators. We
	report the integration of Python scripting with the Multi-scale Object
	Oriented Simulation Environment (MOOSE). MOOSE is a general-purpose
	simulation system for compartmental neuronal models and for models
	of signaling pathways based on chemical kinetics. We show how the
	Python-scripting version of MOOSE, PyMOOSE, combines the power of
	a compiled simulator with the versatility and ease of use of Python.
	We illustrate this by using Python numerical libraries to analyze
	MOOSE output online, and by developing a GUI in Python/Qt for a MOOSE
	simulation. Finally, we build and run a composite neuronal/signaling
	model that uses both the NEURON and MOOSE numerical engines, and
	Python as a bridge between the two. Thus PyMOOSE has a high degree
	of interoperability with analysis routines, with graphical toolkits,
	and with other simulators.},
  keywords = {simulators, compartmental models, systems biology, NEURON, GENESIS,
	multiscale models, Python, MOOSE}
}

@ARTICLE{reed1993pruning,
  author = {R. Reed},
  title = {Pruning Algorithms - A Survey},
  journal = {Trans. of Neural Networks},
  year = {1993},
  volume = {4},
  pages = {740--},
  file = {reed1993pruning.ps.gz:reed1993pruning.ps.gz:PDF},
  owner = {fieres}
}

@ARTICLE{reich2000,
  author = {Daniel S. Reich and Ferenc Mechler and Keith P. Purpura and Jonathan
	D. Victor},
  title = {Interspike Intervals, Receptive Fields, and Information Encoding
	in Primary Visual Cortex},
  journal = {The Journal of Neuroscience},
  year = {2000},
  volume = {20},
  pages = {1964-1974},
  number = {5},
  month = {March},
  owner = {bruederl}
}

@INPROCEEDINGS{renaud2007neuromimetic,
  author = {Sylvie Renaud and Jean Tomas and Yannick Bornat and Adel Daouzli
	and Sylvain Saighi},
  title = {Neuromimetic {IC}s with analog cores: an alternative for simulating
	spiking neural networks},
  booktitle = {Proceedings of the 2007 IEEE Symposium on Circuits and Systems (ISCAS2007)},
  year = {2007},
  owner = {bruederl},
  timestamp = {2008.09.02}
}

@proceedings{resch2014sustained,
  title={Sustained Simulation Performance 2014: Proceedings of the Joint Workshop on Sustained Simulation Performance},
  editor={Resch, Michael M and Bez, Wolfgang and Focht, Erich and Kobayashi, Hiroaki and Patel, Nisarg},
  year=2014,
  publisher={Springer},
  organization={University of Stuttgart ({HLRS}) and Tohoku University},
  isbn = {9783319362540}
}

@ARTICLE{Rescorla2008,
  author = {Rescorla, R.},
  title = {Rescorla-Wagner model},
  journal = {Scholarpedia},
  year = {2008},
  volume = {3},
  pages = {2237},
  number = {3},
  doi = {10.4249/scholarpedia.2237},
  owner = {simon},
  timestamp = {2013.04.25},
  url = {http://www.scholarpedia.org/article/Rescorla-Wagner_model}
}

@ARTICLE{Rescorla1972,
  author = {Rescorla, Robert A and Wagner, Allan R},
  title = {A theory of Pavlovian conditioning: Variations in the effectiveness
	of reinforcement and nonreinforcement},
  journal = {Classical conditioning II: Current research and theory},
  year = {1972},
  pages = {64--99},
  owner = {simon},
  publisher = {New York},
  timestamp = {2013.04.25}
}

@ARTICLE{reyneri03implementation,
  author = {Reyneri, L.M.},
  title = {Implementation issues of neuro-fuzzy hardware: {G}oing toward {HW}/{SW}
	codesign},
  journal = {IEEE Transactions on Neural Networks},
  year = {2003},
  volume = {14},
  pages = {176--194},
  number = {1},
  month = {January}
}

@BOOK{rieke1997spikes,
  title = {Spikes - Exploring the neural code.},
  publisher = {MIT Press, Cambridge, MA.},
  year = {1997},
  author = {Rieke, F. and Warland, D. and de Ruyter van Steveninck, R. and Bialek,
	W.},
  owner = {bkaplan},
  timestamp = {2008.11.28}
}

@ARTICLE{rizwan2021review,
  author={Rizwan, Ali and Zoha, Ahmed and Mabrouk, Ismail Ben and Sabbour, Hani M. and Al-Sumaiti, Ameena Saad and Alomainy, Akram and Imran, Muhammad Ali and Abbasi, Qammer H.},
  journal={{IEEE} Reviews in Biomedical Engineering},
  title={A Review on the State of the Art in Atrial Fibrillation Detection Enabled by Machine Learning},
  year=2021,
  volume=14,
  pages={219--239},
  doi={10.1109/RBME.2020.2976507}
}

@INCOLLECTION{rocke2008fpaanetworks,
  author = {Rocke, Patrick and McGinley, Brian and Maher, John and Morgan, Fearghal
	and Harkin, Jim},
  title = {Investigating the Suitability of FPAAs for Evolved Hardware Spiking
	Neural Networks},
  booktitle = {Evolvable Systems: From Biology to Hardware},
  publisher = {Springer Berlin / Heidelberg},
  year = {2008},
  editor = {Hornby, Gregory and Sekanina, Luk{\'a}\v{s} and Haddow, Pauline},
  volume = {5216},
  series = {Lecture Notes in Computer Science},
  pages = {118-129},
  note = {10.1007/978-3-540-85857-7\_11},
  affiliation = {NUI Galway BIRC Research Group, Dept. Electronic Engineering Ireland},
  isbn = {978-3-540-85856-0},
  keyword = {Computer Science},
  url = {http://dx.doi.org/10.1007/978-3-540-85857-7\_11}
}

@ARTICLE{ronao2016human,
  title={Human activity recognition with smartphone sensors using deep learning neural networks},
  author={Ronao, Charissa Ann and Cho, Sung-Bae},
  journal={Expert systems with applications},
  volume={59},
  pages={235--244},
  year={2016},
  publisher={Elsevier}
}

@ARTICLE{ronao2016human_shorter,
  title={Human activity recognition with smartphone sensors using deep learning neural networks},
  author={Ronao, Charissa Ann and Cho, Sung-Bae},
  journal={Expert systems with applications},
  volume=59,
  year=2016,
  publisher={Elsevier}
}

@ARTICLE{rogers01solvable,
  author = {Rogers, A. and Pr\"ugel-Bennet, A.},
  title = {A Solvable Model of a Hard Optimization Problem},
  journal = {Proceedings of Theoretical Aspects of Evolutionary Computing},
  year = {2001},
  pages = {209--224}
}

@INPROCEEDINGS{Rohrer2004,
  author = {Rohrer, N.J. and Canada, M. and Cohen, E. and Ringler, M. and Mayfield,
	M. and Sandon, P. and Kartschoke, P. and Heaslip, J. and Allen, J.
	and McCormick, P. and Pfluger, T. and Zimmerman, J. and Lichtenau,
	C. and Werner, T. and Salem, G. and Ross, M. and Appenzeller, D.
	and Thygesen, D.},
  title = {PowerPC 970 in 130 nm and 90 nm technologies},
  booktitle = {Solid-State Circuits Conference, 2004. Digest of Technical Papers.
	ISSCC. 2004 IEEE International},
  year = {2004},
  pages = {68-69 Vol.1},
  doi = {10.1109/ISSCC.2004.1332597},
  issn = {0193-6530},
  keywords = {cache storage;circuit tuning;electric fuses;integrated circuit design;integrated
	memory circuits;microprocessor chips;parallel processing;silicon-on-insulator;1.0
	GHz;130 nm;2 GHz;512 kB;64 bit;90 nm;L2 cache;PowerPC 970;PowerPC
	microprocessor;PowerPC technologies;PowerTune;SIMD instruction set;SOI
	technology;Si-SiO2;electronic fuses;power scaling;rapid frequency
	scaling;Bandwidth;Copper;Delay;Design optimization;Libraries;Logic;Microprocessors;Pipelines;Timing;Voltage},
  owner = {simon},
  timestamp = {2013.03.28}
}

@BOOK{rojas96theory,
  title = {Theorie der neuronalen Netze: Eine systematische Einf\"uhrung},
  publisher = {Springer Verlag},
  year = {1996},
  author = {Rojas, Raul},
  address = {Berlin, Heidelberg, New York},
  isbn = {3-540-56353-9},
  keywords = {NN}
}

@book{rolls2010noisy,
  title={The noisy brain: stochastic dynamics as a principle of brain function},
  author={Rolls, Edmund T and Deco, Gustavo},
  volume={34},
  year={2010},
  publisher={Oxford university press Oxford}
}

@INPROCEEDINGS{rosenblatt60perceptron,
  author = {Rosenblatt, F.},
  title = {Perceptron Simulation Experiments},
  booktitle = {Proceedings of the IRE},
  year = {1960},
  pages = {301-309},
  journal = {Proceedings of the IRE}
}

@ARTICLE{rosenblatt58perceptron,
  author = {Rosenblatt, F.},
  title = {The Perceptron: a probabilistic model for information storage and
	organization in the brain},
  journal = {Psychological Review},
  year = {1958},
  volume = {65},
  pages = {386--408},
  annote = {the original by the inventor himself}
}

@ARTICLE{rossant10,
  author = {Rossant, Cyrille and Goodman, Dan F. and Platkiewicz, Jonathan and
	Brette, Romain},
  title = {Automatic fitting of spiking neuron models to electrophysiological
	recordings},
  journal = {Frontiers in Neuroinformatics},
  year = {2010},
  volume = {4},
  abstract = {Spiking models can accurately predict the spike trains produced by
	cortical neurons in response to somatically injected currents. Since
	the specific characteristics of the model depend on the neuron, a
	computational method is required to fit models to electrophysiological
	recordings. The fitting procedure can be very time consuming both
	in terms of computer simulations and in terms of code writing. We
	present algorithms to fit spiking models to electrophysiological
	data (time-varying input and spike trains) that can run in parallel
	on graphics processing units (GPUs). The model fitting library is
	interfaced with Brian, a neural network simulator in Python. If a
	GPU is present it uses just-in-time compilation to translate model
	equations into optimized code. Arbitrary models can then be defined
	at script level and run on the graphics card. This tool can be used
	to obtain empirically validated spiking models of neurons in various
	systems. We demonstrate its use on public data from the INCF Quantitative
	Single-Neuron Modeling 2009 competition by comparing the performance
	of a number of neuron spiking models.}
}

@INPROCEEDINGS{rossmann97short,
  author = {M. Rossmann and Andreas Buhlmeier and G. Manteuffel and Karl Goser},
  title = {Short- and Long-Term Dynamics in a Stochastic Pulse Stream Neuron
	Implemented in {FPGA}},
  booktitle = {{ICANN}},
  year = {1997},
  pages = {1241-1246},
  citeseerurl = {citeseer.nj.nec.com/rossmann97short.html},
  keywords = {spiking}
}

@MISC{rossmann-neural,
  author = {M. Rossmann and C. Burwick and A. Buhlmeier and G. Manteuffel and
	K. Goser},
  title = {Neural Dynamics in Real-Time for Large Scale Biomorphic Neural Networks},
  citeseerurl = {citeseer.nj.nec.com/315845.html},
  keywords = {spiking}
}

@MISC{rossmann96implementation,
  author = {M. Rossmann and B. Hesse and K. Goser and A. B{\"u}hlmeier and G.
	Manteuffel},
  title = {Implementation of a Biologically Inspired Neuron-Model in FPGA},
  year = {1996},
  citeseerurl = {citeseer.nj.nec.com/rossmann96implementation.html},
  keywords = {spiking},
  text = {Rossmann et. al., Implementation of a Biologically Inspired Neuron
	Model in FPGA, Proc.\ of MicroNeuro'96, Lausanne, pp 322-330, 1996}
}

@ARTICLE{g-q00stable,
  author = {van Rossum and G-Q. Bi and G.G. Turrigiano},
  title = {Stable Hebbian learning from spike timing-dependent plasticity},
  journal = {J Neurosci.},
  year = {2000},
  volume = {20},
  pages = {8812-21},
  citeseerurl = {citeseer.nj.nec.com/vanrossum00stable.html},
  keywords = {spiking learning},
  text = {van Rossum G-Q. Bi and G.G. Turrigiano., Stable Hebbian learning from
	spike timing-dependent plasticity., J Neurosci. 20:8812-21, 2000.}
}

@MANUAL{python_apireference,
  title = {{Python/C API} Reference Manual},
  author = {van Rossum, Guido},
  year = {2012},
  url = {http://docs.python.org/api/api.html}
}

@BOOK{python_reference,
  title = {Python Reference Manual: February 19, 1999, Release 1.5.2},
  publisher = {iUniverse, Incorporated},
  year = {2000},
  editor = {Drake, Fred L.},
  author = {Rossum, Guido Van},
  isbn = {1583483748}
}

@ARTICLE{rhodes2018spynnaker,
	AUTHOR={Rhodes, Oliver and Bogdan, Petruţ A. and Brenninkmeijer, Christian and Davidson, Simon and Fellows, Donal and Gait, Andrew and Lester, David R. and Mikaitis, Mantas and Plana, Luis A. and Rowley, Andrew G. D. and Stokes, Alan B. and Furber, Steve B.},
	TITLE={sPyNNaker: A Software Package for Running PyNN Simulations on SpiNNaker},
	JOURNAL={Frontiers in Neuroscience},
	VOLUME={12},
	PAGES={816},
	YEAR={2018},
	DOI={10.3389/fnins.2018.00816},
	ISSN={1662-453X},
	ABSTRACT={This work presents sPyNNaker 4.0.0, the latest version of the software package for simulating PyNN-defined spiking neural networks (SNNs) on the SpiNNaker neuromorphic platform. Operations underpinning realtime SNN execution are presented, including an event-based operating system facilitating efficient time-driven neuron state updates and pipelined event-driven spike processing. Preprocessing, realtime execution, and neuron/synapse model implementations are discussed, all in the context of a simple example SNN. Simulation results are demonstrated, together with performance profiling providing insights into how software interacts with the underlying hardware to achieve realtime execution. System performance is shown to be within a factor of 2 of the original design target of 10000 synaptic events per ms, however SNN topology is shown to influence performance considerably. A cost model is therefore developed characterising the effect of network connectivity and SNN partitioning, enabling predictions on performance improvements to be made, and demonstrating the continued potential of the SpiNNaker neuromorphic hardware.}
}

@ARTICLE{rossum01correlation,
  author = {M.C.W. van Rossum and G.G. Turrigiano},
  title = {Correlation based learning from spike timing dependent plasticity},
  journal = {Neurocomputing},
  year = {2001},
  volume = {38-40},
  pages = {409-415},
  visnote = {stdp modfunc, multiplicative rule}
}

@ARTICLE{rossum01novel,
  author = {M. C. W. van Rossum},
  title = {A Novel Spike Distance},
  journal = {Neural Computation},
  year = {2001},
  volume = {13},
  pages = {751-763},
  number = {4}
}

@INPROCEEDINGS{roth97line,
  author = {Ulrich Roth and Axel Jahnke and Heinrich Klar},
  title = {On-Line Hebbian Learning for Spiking Neurons: Architecture of the
	Weight-Unit of {NESPINN}},
  booktitle = {{ICANN}},
  year = {1997},
  pages = {1217-1222},
  citeseerurl = {citeseer.nj.nec.com/88548.html},
  keywords = {spiking learning}
}

@ARTICLE{rowley2019spinntools,
	AUTHOR={Rowley, Andrew G. D. and Brenninkmeijer, Christian and Davidson, Simon and Fellows, Donal and Gait, Andrew and Lester, David R. and Plana, Luis A. and Rhodes, Oliver and Stokes, Alan B. and Furber, Steve B.},
	TITLE={SpiNNTools: The Execution Engine for the SpiNNaker Platform},
	JOURNAL={Frontiers in Neuroscience},
	VOLUME={13},
	PAGES={231},
	YEAR={2019},
	DOI={10.3389/fnins.2019.00231},
	ISSN={1662-453X},
	ABSTRACT={SpiNNaker is a massively parallel distributed architecture primarily focused on real time simula-
	tion of spiking neural networks. The largest realisation of the architecture consists of one million
	general purpose processors, making it the largest neuromorphic computing platform in the world
	at the present time. Utilising these processors efficiently requires expert knowledge of the archi-
	tecture to generate executable code and to harness the potential of the unique inter-processor
	communications infra-structure that lies at the heart of the SpiNNaker architecture. This work
	introduces a software suite called SpiNNTools that can map a computational problem described
	as a graph into the required set of executables, application data and routing information necessary
	for simulation on this novel machine. The SpiNNaker architecture is highly scalable, giving rise to
	unique challenges in mapping the problem to the machines resources, loading the generated files
	to the machine and subsequently retrieving the results of simulation. In this paper we describe
	these challenges in detail and the solutions implemented.}
}

@ARTICLE{roxin2005,
  author = {Roxin, Alex and Brunel, Nicolas and Hansel, David},
  title = {Role of Delays in Shaping Spatiotemporal Dynamics of Neuronal Activity
	in Large Networks},
  journal = {Phys. Rev. Lett.},
  year = {2005},
  volume = {94},
  pages = {238103},
  month = {Jun},
  doi = {10.1103/PhysRevLett.94.238103},
  issue = {23},
  numpages = {4},
  publisher = {American Physical Society},
  url = {http://link.aps.org/doi/10.1103/PhysRevLett.94.238103}
}

@INPROCEEDINGS{Roy2003,
  author = {Roy, Kaushik and Mukhopadhyay, Saibal and Mahmoodi-Meimand, Hamid},
  title = {Leakage current mechanisms and leakage reduction techniques in deep-submicrometer
	CMOS circuits},
  booktitle = {Proceedings of the IEEE},
  year = {2003},
  volume = {91},
  number = {2},
  pages = {305 - 327},
  month = {February},
  organization = {IEEE},
  owner = {simon},
  timestamp = {2012.10.28}
}

@INPROCEEDINGS{rudolph_ppsn91distributed,
  author = {Rudolph, G.},
  title = {Global Optimization by Means of Distributed Evolution Strategies},
  booktitle = {Proceedings of the 1st International Conference on Parallel Problem
	Solving from Nature},
  year = {1991},
  editor = {Schwefel, H.-P. and M{\"a}nner, R.},
  volume = {496},
  pages = {209--213},
  publisher = {Springer Verlag},
  howpublished = {Lecture Notes in Computer Science}
}

@BOOK{rudolph97convergence,
  title = {Convergence Properties of Evolutionary Algorithms},
  publisher = {Verlag Dr. Kova{\u c}},
  year = {1997},
  author = {Rudolph, G.},
  address = {Hamburg},
  isbn = {3-86064-554-4}
}

@INPROCEEDINGS{rudolph_icec96convergence,
  author = {Rudolph, G.},
  title = {Convergence of Evolutionary Algorithms in General Search Spaces},
  booktitle = {Proceedings of the {IEEE} Conference on Evolutionary Computation},
  year = {1996},
  pages = {50--54},
  address = {Piscataway, NJ},
  publisher = {IEEE Press},
  file = {rudolph_icec96convergence.pdf:rudolph_icec96convergence.pdf:PDF}
}

@ARTICLE{rudolph2006analytical,
  author = {Michelle Rudolph and Alain Destexhe},
  title = {Analytical Integrate-and-Fire Neuron Models with Conductance-Based
	Dynamics for Event-Driven Simulation Strategies},
  journal = {Neural Comput.},
  year = {2006},
  volume = {18},
  pages = {2146--2210},
  number = {9},
  address = {Cambridge, MA, USA},
  issn = {0899-7667},
  publisher = {MIT Press}
}

@ARTICLE{rudolph2003,
  author = {Rudolph, Michael and Destexhe, Alain },
  title = {Tuning neocortical pyramidal neurons between integrators and coincidence
	detectors},
  journal = {J Comput Neurosci},
  year = {2003},
  volume = {14},
  pages = {239--251},
  abstract = {Do cortical neurons operate as integrators or as coincidence detectors?
	Despite the importance of this question, no definite answer has been
	given yet, because each of these two views can find its own experimental
	support. Here we investigated this question using models of morphologically-reconstructed
	neocortical pyramidal neurons under in-vivo--like conditions. In
	agreement with experiments we find that the cell is capable of operating
	in a continuum between coincidence detection and temporal integration,
	depending on the characteristics of the synaptic inputs. Moreover,
	the presence of synaptic background activity at a level comparable
	to intracellular measurements in vivo can modulate the operating
	mode of the cell, and act as a switch between temporal integration
	and coincidence detection. These results suggest that background
	activity can be viewed as an important determinant of the integrative
	mode of pyramidal neurons. Thus, background activity not only sharpens
	cortical responses but it can also be used to tune an entire network
	between integration and coincidence detection modes.},
  citeulike-article-id = {3184947},
  keywords = {file-import-08-09-03},
  posted-at = {2008-09-03 10:42:11},
  priority = {0}
}

@ARTICLE{ruf98selforganization,
  author = {Berthold Ruf and Michael Schmitt},
  title = {Self-organization of spiking neurons using action potential timing},
  journal = {IEEE Transactions on Neural Networks},
  year = {1998},
  volume = {9},
  pages = {575--578},
  number = {3},
  citeseerurl = {citeseer.nj.nec.com/ruf98selforganization.html},
  keywords = {spiking learning}
}

@ARTICLE{ruiz02hierarchical,
  author = {Ruiz, Miguel E. and Srinivasan, Padmini},
  title = {Hierarchical Text Categorization Using Neural Networks},
  journal = {Information Retrieval},
  year = {2002},
  volume = {5},
  pages = {87--118},
  number = {1},
  file = {ruiz02hierarchical.pdf:ruiz02hierarchical.pdf:PDF},
  publisher = {Kluwer Academic Publishers},
  url = {citeseer.nj.nec.com/ruiz02hierarchical.html}
}

@ARTICLE{rumelhart86backprop,
  author = {Rumelhart, D. E. and Hinton, G. E. and Williams R.J.},
  title = {Learning internal representations by error propagation},
  journal = {Parallel Distributed Processing: Explorations in the Microstructures
	of Cognition},
  year = {1986},
  volume = {I},
  pages = {318--362},
  address = {Cambridge, MA},
  editor = {Rumelhart, D. E. and McClelland, J. L.},
  keywords = {learning},
  publisher = {MIT Press}
}

@ARTICLE{rumelhart86representations,
  author = {Rumelhart, D. E. and Hinton, G. E. and Williams R.J.},
  title = {Learning representations by back-propagating errors},
  journal = {Nature},
  year = {1986},
  volume = {323},
  pages = {533--536},
  address = {London},
  keywords = {learning}
}

@ARTICLE{rumelhart85feature,
  author = {D. E. Rumelhart and D. Zipser},
  title = {Feature discovery by competitive learning},
  journal = {Cognitive Science},
  year = {1985},
  volume = {9},
  pages = {75--112},
  keywords = {learning},
  owner = {fieres}
}

@ARTICLE{rudolph05characterization,
  author = {{R}udolph, {M}ichael and {G}uillaume {P}elletier, {J}oe and {P}ar{\'e},
	{D}enis and {D}estexhe, {A}lain},
  title = {{C}haracterization of synaptic conductances and integrative properties
	during electrically induced {EEG}-activated states in neocortical
	neurons in vivo.},
  journal = {{J}ournal of neurophysiology},
  year = {2005},
  volume = {94},
  pages = {2805},
  abstract = {{T}he activation of the electroencephalogram ({EEG}) is paralleled
	with an increase in the firing rate of cortical neurons, but little
	is known concerning the conductance state of their membrane and its
	impact on their integrative properties. {H}ere, we combined in vivo
	intracellular recordings with computational models to investigate
	{EEG}-activated states induced by stimulation of the brain stem ascending
	arousal system. {E}lectrical stimulation of the pedonculopontine
	tegmental ({PPT}) nucleus produced long-lasting (approximately 20
	s) periods of desynchronized {EEG} activity similar to the {EEG}
	of awake animals. {I}ntracellularly, {PPT} stimulation locked the
	membrane into a depolarized state, similar to the up-states seen
	during deep anesthesia. {D}uring these {EEG}-activated states, however,
	the input resistance was higher than that during up-states. {C}onductance
	measurements were performed using different methods, which all indicate
	that {EEG}-activated states were associated with a synaptic activity
	dominated by inhibitory conductances. {T}hese results were confirmed
	by computational models of reconstructed pyramidal neurons constrained
	by the corresponding intracellular recordings. {T}hese models indicate
	that, during {EEG}-activated states, neocortical neurons are in a
	high-conductance state consistent with a stochastic integrative mode.
	{T}he amplitude and timing of somatic excitatory postsynaptic potentials
	were nearly independent of the position of the synapses in dendrites,
	suggesting that {EEG}-activated states are compatible with coding
	paradigms involving the precise timing of synaptic events.},
  affiliation = {{U}nit{\'e} de neurosciences int{\'e}gratives et computationnelles
	- {UNIC} - {CNRS} : {UPR}2191 - {I}nstitut de {N}eurobiologie {A}lfred
	{F}essard - {INAF} - {CNRS} : {FRC}2118 - {C}enter for {M}olecular
	\& {B}ehavioral {N}euroscience - {R}utgers {U}niversity-{N}ewark},
  keywords = {{A}nimals;{C}ats;{C}omparative {S}tudy;{D}ose {R}esponse {R}elationship;{R}adiation
	{E}lectric {C}onductivity;{E}lectric {S}timulation;{E}lectroencephalography;{M}embrane
	{P}otentials;{M}odels, {N}eurological;{N}eocortex cytology;{N}eural
	{I}nhibition physiology;{N}ormal {D}istribution;{P}otassium metabolism;{S}pectrum
	{A}nalysis methods;{S}ynaptic {T}ransmission;{T}ime {F}actors; alpha-{A}mino-3-hydroxy-5-methyl-4-isoxazolepropionic
	{A}cid;gamma-{A}minobutyric {A}cid},
  language = {{E}nglish}
}

@MISC{sala-selforganization,
  author = {Dorel M. Sala and Krzysztof J. Cios and John T. Wall},
  title = {Self-Organization in Networks of Spiking Neurons},
  citeseerurl = {citeseer.nj.nec.com/507653.html},
  keywords = {spiking learning}
}

@inproceedings{salakhutdinov2009deep,
  title={Deep boltzmann machines},
  author={Salakhutdinov, Ruslan and Hinton, Geoffrey E},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={448--455},
  year={2009}
}

@inproceedings{salakhutdinov2010learning,
  title={Learning deep Boltzmann machines using adaptive MCMC},
  author={Salakhutdinov, Ruslan},
  booktitle={Proceedings of the 27th International Conference on Machine Learning (ICML-10)},
  pages={943--950},
  year={2010}
}

@BOOK{sansen06analog,
  title = {Analog Design Essentials (The International Series in Engineering
	and Computer Science)},
  publisher = {Springer-Verlag New York, Inc.},
  year = {2006},
  author = {Sansen, Willy M. C.},
  address = {Secaucus, NJ, USA},
  isbn = {0387257462}
}

@INPROCEEDINGS{santini_eh01,
  author = {Santini, Christina Costa and Zebulum, Ricardo Salem and Pacheco,
	Marco Aur{\'e}lio C.\ and Vellasco, Marley Maria R.\ and Szwarcman,
	Mois{\'e}s H.\ },
  title = {Evolutionary experiments with a fine-grained reconfigurable architecture
	for analog and digital {CMOS} circuits},
  booktitle = {Proc.\ of the Third NASA/DOD Workshop on Evolvable Hardware},
  year = {2001},
  pages = {36--43},
  address = {Long Beach, CA, USA},
  month = Jul,
  publisher = {IEEE Computer Society Press},
  annote = {XOR / 2-1 Mux},
  groupsearch = {0},
  keywords = {eh}
}

@ARTICLE{sarpeshkar93,
  author = {Rahul Sarpeshkar and Tobias Delbruck and Carver A. Mead},
  title = {White Noise in {MOS} Transistors and Resistors},
  journal = {Circuits and Devices Magazine, IEEE},
  year = {1993},
  volume = {9},
  pages = {23--29},
  number = {6},
  month = nov,
  keywords = {noise,low-power}
}

@ARTICLE{satyanarayana1992nnchip,
  author = {Satyanarayana, S. and Tsividis, P. and Graf, H.P.},
  title = {A Reconfigurable {VLSI} Neural Network},
  journal = {IEEE Journal of Solid-State Circuits},
  year = {1992},
  volume = {27},
  pages = {67--81},
  number = {1},
  month = {January},
  file = {satyanarayana1992nnchip.pdf:satyanarayana1992nnchip.pdf:PDF},
  keywords = {vlsi, convolutional NN}
}

@ARTICLE{Scahill2003,
  author = {Scahill, Rachael I and Frost, Chris and Jenkins, Rhian and Whitwell,
	Jennifer L and Rossor, Martin N and Fox, Nick C},
  title = {A longitudinal study of brain volume changes in normal aging using
	serial registered magnetic resonance imaging},
  journal = {Archives of neurology},
  year = {2003},
  volume = {60},
  pages = {989},
  number = {7},
  owner = {simon},
  publisher = {Am Med Assoc},
  timestamp = {2013.05.07}
}

@ARTICLE{schaffer90genetic,
  author = {Schaffer, J. D. and Caruana, R. A. and Eshelmann, L. J.},
  title = {Using Genetic Search to Exploit the Emergent Behavior of Neural Networks},
  journal = {Physica D},
  year = {1990},
  volume = {42},
  pages = {244--248},
  address = {North Holland},
  publisher = {Elsevier Science Publishers B.V.}
}

@INPROCEEDINGS{schaffer_icga89parameters,
  author = {Schaffer, J. D. and Caruana, R.A. and Eshelman, L. J. and Das, R.},
  title = {A study of control parameters affecting online performance of genetic
	algorithms for function optimization},
  booktitle = {Proceedings of the Third International Conference on Genetic Algorithms},
  year = {1989},
  editor = {Schaffer, J. D.},
  pages = {51--60},
  publisher = {Morgan Kaufmann},
  file = {schaffer_icga89parameters.pdf:schaffer_icga89parameters.pdf:PDF}
}

@MISC{schemmeloral,
  author = {Schemmel, J.},
  title = {personal communication},
  address = {Kirchhoff Institut f{\"u}r Physik, Universit{\"a}t Heidelberg, Deutschland}
}

@MISC{schemmel12oral,
  author = {Schemmel, J.},
  title = {personal communication},
  year = {2012},
  address = {Kirchhoff Institut f{\"u}r Physik, Universit{\"a}t Heidelberg, Deutschland},
  owner = {simon},
  timestamp = {2013.04.06}
}

@MISC{sramctrl_js,
  author = {Johannes Schemmel},
  title = {SystemVerilog SRAM controller},
  howpublished = {Personal communication},
  year = {2011},
  owner = {simon},
  timestamp = {2013.03.01}
}

@MISC{schemmel08oral,
  author = {Schemmel, J.},
  title = {personal communication},
  year = {2008},
  address = {Kirchhoff Institut f{\"u}r Physik, Universit{\"a}t Heidelberg, Deutschland}
}

@UNPUBLISHED{schemmel_wp7stdp,
  author = {Johannes Schemmel},
  title = {{WP7 STDP implementation}},
  note = {University of Heidelberg},
  month = oct,
  year = {2006}
}

@MISC{schemmel05oral,
  author = {Schemmel, J.},
  title = {personal communication},
  year = {2005},
  address = {Kirchhoff Institut f{\"u}r Physik, Universit{\"a}t Heidelberg, Deutschland}
}

@INPROCEEDINGS{schemmel_iscas2010, crossref={schemmel2010iscas}}
@INPROCEEDINGS{schemmel2010iscas,
  author = {Schemmel, Johannes and Br\"uderle, Daniel and Gr\"ubl, Andreas and Hock, Matthias and
	Meier, Karlheinz and Millner, Sebastian},
  title = {A Wafer-Scale Neuromorphic Hardware System for Large-Scale Neural Modeling},
  booktitle = {Proceedings of the 2010 IEEE International Symposium on Circuits
	and Systems (ISCAS)},
  year = {2010},
  pages = {1947--1950},
  keywords = {neuromorphic},
  doi = {10.1109/ISCAS.2010.5536970},
}

@INPROCEEDINGS{schemmel_iscas07,
  author = {Schemmel, J. and Br\"uderle, D. and Meier, K. and Ostendorf, B.},
  title = {Modeling Synaptic Plasticity within Networks of Highly Accelerated
	{I}\&{F} Neurons},
  booktitle = {Proceedings of the 2007 IEEE International Symposium on Circuits
	and Systems (ISCAS)},
  year = {2007},
  pages = {3367--3370},
  publisher = {IEEE Press},
  key = {schemmel_iscas07},
  keywords = {vision, neuromorphic}
}

@INPROCEEDINGS{schemmel_ijcnn2008,
  author = {Schemmel, J. and Fieres, J. and Meier, K.},
  title = {Wafer-Scale Integration of Analog Neural Networks},
  booktitle = {Proceedings of the 2008 International Joint Conference on Neural
	Networks (IJCNN)},
  year = {2008},
  keywords = {neuromorphic}
}

@INPROCEEDINGS{schemmel_ijcnn06,
  author = {Schemmel, Johannes and Gr{\"u}bl, Andreas and Meier, Karlheinz and Muller, Eilif},
  title = {Implementing Synaptic Plasticity in a {VLSI} Spiking Neural Network
	Model},
  booktitle = {Proceedings of the 2006 International Joint Conference on Neural
	Networks (IJCNN)},
  year = 2006,
  publisher = {IEEE Press},
  key = {schemmel_ijcnn06},
  keywords = {vision, neuromorphic},
  doi = {10.1109/IJCNN.2006.246651},
}

@INPROCEEDINGS{schemmel2017nmda,
  title = {An Accelerated Analog Neuromorphic Hardware System Emulating {NMDA}- and Calcium-Based Non-Linear Dendrites},
  author = {Schemmel, Johannes and Kriener, Laura and M{\"u}ller, Paul and Meier, Karlheinz},
  booktitle = {2017 International Joint Conference on Neural Networks (IJCNN)},
  year = 2017,
  volume = {},
  number = {},
  pages = {2217--2226},
  doi = {10.1109/IJCNN.2017.7966124}
}

@ARTICLE{schemmel2020accelerated,
    title={Accelerated Analog Neuromorphic Computing},
    author={Johannes Schemmel and Sebastian Billaudelle and Philipp Dauer and Johannes Weis},
    year={2020},
    eprint={2003.11996},
    archivePrefix={arXiv},
    primaryClass={cs.NE},
    journal={arXiv preprint},
    url={https://arxiv.org/abs/2003.11996}
}

@ARTICLE{schemmel2020accelerated_nourl,
    title={Accelerated Analog Neuromorphic Computing},
    author={Johannes Schemmel and Sebastian Billaudelle and Philipp Dauer and Johannes Weis},
    year={2020},
    eprint={2003.11996},
    archivePrefix={arXiv},
    primaryClass={cs.NE},
    journal={arXiv preprint}
}

@MISC{hicann_spec,
  author = {Johannes Schemmel and Andreas Gr{\"u}bl and Sebastian Millner},
  title = {Specification of the {HICANN} Microchip},
  howpublished = {{FACETS} project internal documentation},
  year = {2010},
  keywords = {facets, hicann},
  owner = {simon},
  timestamp = {2013.03.01}
}

@MISC{host2fpga,
  author = {M. Ehrlich and A. Grübl and S. Hartmann and E. Müller and J. Partzsch and S. Scholze and V. Thanasoulis},
  title = {Specification of {FACETS}/{BrainScaleS}
  Stage2 Host <-> FPGA Communication},
  howpublished = {{FACETS}/{BrainScaleS} project internal documentation},
  year = {2013},
}

@MISC{hicann_doc_2012,
  author = {Johannes Schemmel and Andreas Gr{\"u}bl and Sebastian Millner and
	Simon Friedmann},
  title = {Specification of the {HICANN} Microchip},
  howpublished = {{FACETS} and {BrainScaleS} project internal documentation},
  year = {2012},
  keywords = {facets, hicann}
}

@INPROCEEDINGS{schemmel_iscas2012,
author={Schemmel, J. and Gr{\"ub}l, A. and Hartmann, S. and Kononov, A. and Mayr, C. and Meier, K. and Millner, S. and Partzsch, J. and Schiefer, S. and Scholze, S. and Sch{\"u}ffny, R. and Schwartz, M.},
booktitle={Proceedings of the 2012 IEEE International Symposium on Circuits and Systems (ISCAS)},
title={Live demonstration: A scaled-down version of the {BrainScaleS} wafer-scale neuromorphic system},
year={2012},
month={May},
pages={702-702},
doi={10.1109/ISCAS.2012.6272131},
ISSN={0271-4302},}

@MISC{schemmel12iscas,
  crossref = {schemmel_iscas2012}
}

@ARTICLE{schemmel_kluwer04hagen,
  author = {Schemmel, J. and Hohmann, S. and Meier, K. and Sch{\"u}rmann, F.},
  title = {A Mixed-Mode Analog Neural Network using Current-Steering Synapses},
  journal = {Analog Integrated Circuits and Signal Processing},
  year = {2004},
  volume = {38},
  pages = {233-244},
  number = {2-3},
  booktitle = {Analog Integrated Circuits and Signal Processing},
  keywords = {vision, vlsi, neuromorphic},
  publisher = {Kluwer}
}

@INPROCEEDINGS{schemmel_ijcnn04,
  author = {Schemmel, J. and Meier, K. and Muller, E.},
  title = {A New {VLSI} Model of Neural Microcircuits Including Spike Time Dependent
	Plasticity},
  booktitle = {Proceedings of the 2004 International Joint Conference on Neural
	Networks (IJCNN'04)},
  year = {2004},
  pages = {1711--1716},
  publisher = {IEEE Press},
  key = {schemmel_ijcnn04},
  keywords = {vision, neuromorphic}
}

@INPROCEEDINGS{schemmel_ices01,
  author = {Schemmel, J. and Meier, K. and Sch{\"u}rmann, F.},
  title = {A {VLSI} Implementation of an Analog Neural Network suited for Genetic
	Algorithms},
  booktitle = {Proceedings of the International Conference on Evolvable Systems
	{ICES} 2001},
  year = {2001},
  pages = {50-61},
  publisher = {Springer Verlag},
  howpublished = {ISBN 3-540-42671-X},
  key = {schemmel_ices2001},
  keywords = {vision, vlsi}
}

@INPROCEEDINGS{schemmel_ijcnn02,
  author = {Schemmel, J. and Sch{\"u}rmann, F. and Hohmann, S. and Meier, K.},
  title = {An Integrated Mixed-Mode Neural Network Architecture for Megasynapse
	{ANN}s},
  booktitle = {Proceedings of the 2002 International Joint Conference on Neural
	Networks {IJCNN}'02},
  year = {2002},
  pages = {2704-2710},
  publisher = {IEEE Press},
  howpublished = {ISBN 0-7803-7279-4},
  key = {schemmel_ijcnn2002},
  keywords = {vision, vlsi}
}

@INPROCEEDINGS{Schild2009,
  author = {Schild, Henning and Lackorzynski, Adam and Warg, Alexander},
  title = {Faithful Virtualization on a Real-Time Operating System},
  booktitle = {Eleventh Real-Time Linux Workshop},
  year = {2009},
  owner = {simon},
  timestamp = {2012.10.28}
}

@INPROCEEDINGS{schmid_iscas98hamming,
  author = {Schmid, A. and Leblebici, Y. and Mlynek, D.},
  title = {Hardware Realization of a Hamming Neural Network with On-Chip Learning},
  booktitle = {Proceedings ISCAS 1998},
  year = {1998},
  groupsearch = {0},
  key = {schmid_hamming}
}

@MISC{schmidt2012internship,
  author = {Schmidt, Dominik},
  title = {Readout Training for Liquid Factor Graphs},
  howpublished = {Internship Report, University of Heidelberg},
  year = {2012}
}

@MISC{schmidt13internreport,
  author = {Schmidt, Marten Ole},
  title = {{Aufnahme von STDP-Kurven mit Tripeln von Aktionspotentialen}},
  howpublished = {Internship Report (German), University of Heidelberg, HD-KIP 10-43},
  year = {2013},
  key = {schmidt13internreport},
  keywords = {vision, spikey, stage1, software, pynn, plasticity}
}

@MASTERSTHESIS{schmidt14masterthesis,
  author = {Schmidt, Dominik},
  title = {Automated Characterization of a Wafer-Scale Neuromorphic Hardware
	  System},
  year = 2014,
  type = {Master thesis},
  school = {Ruprecht-Karls-Universit{\"a}t Heidelberg}
}

@unpublished{schmidt2022commissioning,
  title  = {From clean room to machine room: Commissioning of the BrainScaleS-1 Wafer-Scale Neuromorphic System},
  author = {Schmidt, Hartmut and Montes, Jos{\'e} and Gr{\"u}bl, Andreas and G{\"u}ttler, Maurice and Husmann, Dan and Ilmberger, Joscha and Kaiser, Jakob and Mauch, Christian and M{\"u}ller, Eric and Sterzenbach, Lars and Schemmel, Johannes and Schmitt, Sebastian},
  note   = {In preparation}
}

@article{schmidhuber1992learning,
  title={Learning complex, extended sequences using the principle of history compression},
  author={Schmidhuber, J{\"u}rgen},
  journal={Neural Computation},
  volume={4},
  number={2},
  pages={234--242},
  year={1992},
  publisher={MIT Press}
}

@article{schiller2000nmda,
  title={{NMDA} spikes in basal dendrites of cortical pyramidal neurons},
  author={Schiller, Jackie and Major, Guy and Koester, Helmut J and Schiller, Yitzhak},
  journal={Nature},
  volume={404},
  number={6775},
  pages={285--289},
  year={2000},
  publisher={Nature Publishing Group}
}

@article{schmitt2017hwitl, crossref = {schmitt2017neuromorphic}}
@article{schmitt2016classification, crossref = {schmitt2017neuromorphic}}
@article{schmitt2017neuromorphic,
  title    = {Neuromorphic Hardware In The Loop: Training a Deep Spiking Network on the {BrainScaleS} Wafer-Scale System},
  author   = {Schmitt, Sebastian and Kl{\"a}hn, Johann and Bellec, Guillaume and Gr{\"u}bl, Andreas and G{\"u}ttler, Maurice and Hartel, Andreas and Hartmann, Stephan and Husmann, Dan and Husmann, Kai and Jeltsch, Sebastian and Kleider, Mitja and Koke, Christoph and Kononov, Alexander and Mauch, Christian and M{\"u}ller, Eric and M{\"u}ller, Paul and Partzsch, Johannes and Petrovici, Mihai A. and Vogginger, Bernhard and Schiefer, Stefan and Scholze, Stefan and Thanasoulis, Vasilis and Schemmel, Johannes and Legenstein, Robert and Maass, Wolfgang and Mayr, Christian and Meier, Karlheinz},
  journal  = {Proceedings of the 2017 IEEE International Joint Conference on Neural Networks (IJCNN)},
  year     = 2017,
  volume   = {},
  pages    = {2227--2234},
  doi      = {10.1109/IJCNN.2017.7966125},
  url      = {http://ieeexplore.ieee.org/document/7966125/}
}

@article{schmitt2017hwitl_nourl,
  title={Neuromorphic Hardware In The Loop: Training a Deep Spiking Network on the BrainScaleS Wafer-Scale System},
  author={Sebastian Schmitt and Johann Klähn and Guillaume Bellec and Andreas Grübl and Maurice Güttler and Andreas Hartel and Stephan Hartmann and Dan Husmann and Kai Husmann and Sebastian Jeltsch and Vitali Karasenko and Mitja Kleider and Christoph Koke and Alexander Kononov and Christian Mauch and Eric Müller and Paul Müller and Johannes Partzsch and Mihai A. Petrovici and Bernhard Vogginger and Stefan Schiefer and Stefan Scholze and Vasilis Thanasoulis and Johannes Schemmel and Robert Legenstein and Wolfgang Maass and Christian Mayr and Karlheinz Meier},
  journal  = {Proceedings of the 2017 IEEE International Joint Conference on Neural Networks},
  year     = {2017},
  volume   = {},
  pages    = {},
  doi      = {10.1109/IJCNN.2017.7966125}
}

@article{schmitt2017hwitl_nourl_shorter,
  title={Classification With Deep Neural Networks on an Accelerated Analog Neuromorphic System},
  author={Sebastian Schmitt and Johann Klähn and Guillaume Bellec and Andreas Grübl and Maurice Güttler and Andreas Hartel and Stephan Hartmann and Dan Husmann and Kai Husmann and Sebastian Jeltsch and Vitali Karasenko and Mitja Kleider and Christoph Koke and Alexander Kononov and Christian Mauch and Eric Müller and Paul Müller and Johannes Partzsch and Mihai A. Petrovici and Bernhard Vogginger and Stefan Schiefer and Stefan Scholze and Vasilis Thanasoulis and Johannes Schemmel and Robert Legenstein and Wolfgang Maass and Christian Mayr and Karlheinz Meier},
  journal  = {Proceedings of the 2017 IEEE International Joint Conference on Neural Networks},
  year     = {2017}
}


@MISC{schmitz05oral,
  author = {Schmitz, T.},
  title = {personal communication},
  year = {2005},
  address = {Kirchhoff Institut f{\"u}r Physik, Universit{\"a}t Heidelberg, Deutschland}
}

@misc{schuman2017survery,
Author = {Catherine D. Schuman and Thomas E. Potok and Robert M. Patton and J. Douglas Birdwell and Mark E. Dean and Garrett S. Rose and James S. Plank},
Title = {A Survey of Neuromorphic Computing and Neural Networks in Hardware},
Year = {2017},
Eprint = {arXiv:1705.06963},
}

@MISC{pfeil13oral,
  author = {Pfeil, T.},
  title = {personal communication},
  year = {2013},
  address = {Kirchhoff Institut f{\"u}r Physik, Universit{\"a}t Heidelberg, Deutschland}
}

@PHDTHESIS{schmitz05thesis,
  author = {T. Schmitz},
  title = {Evolution in Hardware -- Eine Experimentierplattform zum parallelen
	Training analoger neuronaler Netzwerke},
  school = {Ruprecht-Karls-University, Heidelberg},
  year = {2005},
  owner = {fieres},
  url = {http://www.kip.uni-heidelberg.de/vision/publications}
}

@INPROCEEDINGS{schmitz_ices03,
  author = {Schmitz, T. and Hohmann, S. and Meier, K. and Schemmel, J. and Sch\"urmann,
	F. },
  title = {{S}peeding up {H}ardware {E}volution: {A} {C}oprocessor for {E}volutionary
	{A}lgorithms},
  booktitle = {Proceedings of the 5th International Conference on Evolvable Systems
	{ICES} 2003},
  year = {2003},
  editor = {Andy M. Tyrrell and Pauline C. Haddow and Jim Torresen},
  pages = {274-285},
  publisher = {Springer Verlag},
  keywords = {vlsi, learning, vision}
}

@INPROCEEDINGS{schmuker2010neuromorphic,
  author = {Schmuker, M. and Hausler, C and Nawrot, M.},
  title = {Neuromorphic Classifier Microcircuits},
  booktitle = {Front. Comput. Neurosci. Conference Abstracts: Bernstein Conference
	on Computational Neuroscience},
  year = {2010},
  doi = {10.3389/conf.fncom.2010.51.00048}
}

@article{schneidman1998ion,
  title={Ion channel stochasticity may be critical in determining the reliability and precision of spike timing},
  author={Schneidman, Elad and Freedman, Barry and Segev, Idan},
  journal={Neural computation},
  volume={10},
  number={7},
  pages={1679--1703},
  year={1998},
  publisher={MIT Press}
}

@ARTICLE{scholze11a,
  author = {Scholze, S. and Eisenreich, H. and H\"oppner, S. and Ellguth, G.
	and Henker, S. and Ander, M. and H\"anzsche, S. and Partzsch, J.
	and Mayr, C. and Sch\"uffny, R.},
  title = {A 32 {GB}it/s Communication {SoC} for a Waferscale Neuromorphic System},
  journal = {Integration, the VLSI Journal},
  year = {2011},
  note = {in press},
  doi = {10.1016/j.vlsi.2011.05.003},
  owner = {user},
  publisher = {Elsevier},
  timestamp = {2010.12.15}
}

@INPROCEEDINGS{scholze2010heap,
  author = {Scholze, S. and Henker, S. and Partzsch, J. and Mayr, C. and Schuffny,
	R.},
  title = {Optimized queue based communication in VLSI using a weakly ordered
	binary heap},
  booktitle = {Mixed Design of Integrated Circuits and Systems (MIXDES), 2010 Proceedings
	of the 17th International Conference},
  year = {2010},
  pages = {316 -320},
  month = {june},
  keywords = {UMC technology;buffering component;data storage;event-based routing;global
	timestamps;large-scale VLSI routing system;optimized queue-based
	communication;priority queue algorithm;size 180 nm;sorting component;weakly-ordered
	binary heap;SRAM chips;VLSI;circuit optimisation;network routing;queueing
	theory;}
}

@ARTICLE{scholze11b,
  author = {Stefan Scholze and Stefan Schiefer and Johannes Partzsch and Stephan
	Hartmann and Christian Georg Mayr and Sebastian H\"oppner and Holger
	Eisenreich and Stephan Henker and Bernhard Vogginger and Rene Sch\"uffny},
  title = {{VLSI} implementation of a 2.8{GE}vent/s packet based {AER} interface
	with routing and event sorting functionality},
  journal = {Frontiers in Neuromorphic Engineering},
  year = {2011},
  volume = {5},
  pages = {1--13},
  number = {117},
  abstract = {State-of-the-art large scale neuromorphic systems require sophisticated
	spike event communication between units of the neural network. We
	present a high-speed communication infrastructure for a waferscale
	neuromorphic system, based on application-specific neuromorphic communication
	ICs in an FPGA-maintained environment. The ICs implement configurable
	axonal delays, as required for certain types of dynamic processing
	or for emulating spike based learning among distant cortical areas.
	Measurements are presented which show the efficacy of these delays
	in influencing behaviour of neuromorphic benchmarks. The specialized,
	dedicated AER communication in most current systems requires separate,
	low-bandwidth configuration channels. In contrast, the configuration
	of the waferscale neuromorphic system is also handled by the digital
	packet-based pulse channel, which transmits configuration data at
	the full bandwidth otherwise used for pulse transmission. The overall
	so-called pulse communication subgroup (ICs and FPGA) delivers a
	factor 25-50 more event transmission rate than other current neuromorphic
	communication infrastructures.},
  owner = {user},
  timestamp = {2010.12.15}
}

@ARTICLE{schrader2010compositionality,
  author = {Schrader, S. and Diesmann, M. and Morrison, A.},
  title = {A compositionality machine realized by a hierarchic architecture
	of synfire chains},
  journal = {Frontiers in Computational Neuroscience},
  year = {2010},
  volume = {4},
  publisher = {Frontiers Research Foundation}
}

@ARTICLE{schrader2008detecting,
  author = {Schrader, S. and Gr{\"u}n, S. and Diesmann, M. and Gerstein, G.L.},
  title = {Detecting synfire chain activity using massively parallel spike train
	recording},
  journal = {Journal of Neurophysiology},
  year = {2008},
  volume = {100},
  pages = {2165--2176},
  number = {4},
  publisher = {Am Physiological Soc}
}

@MISC{schrader07_synfire,
  author = {Schrader, S. and Morrison, A. and Diesmann, M.},
  title = {A composition machine for complex movements},
  howpublished = {\url{http://www.neuro.uni-goettingen.de/archiv/2007/pdf/Proceedings-Goettingen2007.pdf}},
  year = {2007},
  booktitle = {Proc 31st Goettingen Neurobiol Conf TS18-1C}
}

@ARTICLE{Schultz1997,
  author = {Schultz, Wolfram and Dayan, Peter and Montague, P Read},
  title = {A neural substrate of prediction and reward},
  journal = {Science},
  year = {1997},
  volume = {275},
  pages = {1593--1599},
  number = {5306},
  owner = {simon},
  publisher = {American Association for the Advancement of Science},
  timestamp = {2013.04.25}
}

@MISC{schwartz12oral,
  author = {Marc-Olivier Schwartz},
  title = {personal communication},
  year = {2012},
  address = {Kirchhoff Institut for Physics, University of Heidelberg, Germany}
}

@MISC{schwartz12phdthesis,
  author = {Marc-Olivier Schwartz},
  howpublished = {{\it PhD thesis}, University of Heidelberg, in preparation},
  year = {2012},
  keywords = {vision}
}

@INPROCEEDINGS{schueffny_nn99hardware,
  author = {Sch{\"u}ffny, R. and Graupner, A. and Schreiter, J.},
  title = {Hardware for neural networks},
  booktitle = {Proceedings of the 4th International Workshop on Neural Networks
	in Applications},
  year = {1999},
  pages = {1--6}
}

@BOOK{schultz1996membrane,
  title = {Molecular Biology of Membrane Transport Disorders},
  publisher = {Plenum Press},
  year = {1996},
  author = {Stanley G. Schultz and thoomas E. Andreoli and Arthur M. Brown and Douglas M. Farmbrough and Joseph F. Hoffman and Michael G. Welsh}
}

@MISC{schurmann_05phdthesis,
  author = {Sch{\"u}rmann, Felix},
  howpublished = {{\it PhD thesis}, University of Heidelberg, in preparation},
  year = {2005},
  keywords = {vision}
}

@INPROCEEDINGS{schuermann_eh02,
  author = {F. Sch{\"u}rmann and S. Hohmann and J. Schemmel and K. Meier},
  title = {{T}owards an {A}rtificial {N}eural {N}etwork {F}ramework},
  booktitle = {Proceedings of the 2002 {NASA}/{DoD} Conference on Evolvable Hardware},
  year = {2002},
  editor = {A. Stoica and J. Lohn and R. Katz and D. Keymeulen and R.S. Zebulum},
  pages = {266-273},
  publisher = {IEEE Computer Society},
  keywords = {vision}
}

@INPROCEEDINGS{schurmann_icann03,
  author = {Felix Sch{\"u}rmann and Steffen G. Hohmann and Karlheinz Meier and
	Johannes Schemmel},
  title = {Interfacing Binary Networks to Multi-valued Signals},
  booktitle = {Supplementary Proceedings of the Joint International Conference ICANN/ICONIP},
  year = {2003},
  pages = {430-433},
  publisher = {IEEE Press},
  howpublished = {ISBN 975-518-209-8},
  key = {schurmann_icann2003},
  keywords = {vision}
}

@INPROCEEDINGS{schurmann_nips04,
  author = {Sch{\"u}rmann, F. and Meier, K. and Schemmel, J.},
  title = {{E}dge of {C}haos {C}omputation in {M}ixed {M}ode {VLSI} -- ``{A}
	{H}ard {L}iquid''},
  booktitle = {Advances in Neural Information Processing Systems 17},
  year = {2004},
  editor = {Saul, L.K. and Weiss, Y. and Bottou, L.},
  address = {Cambride},
  publisher = {MIT Press},
  key = {schurmann_liquid2004},
  keywords = {vision liquid}
}

@book{searle2004mind,
  title={Mind: a brief introduction},
  author={Searle, John R},
  volume={259},
  year={2004},
  publisher={Oxford University Press Oxford}
}

@ARTICLE{sejnowski77storing,
  author = {Sejnowski, T. J.},
  title = {Storing covariance with nonlinearly interacting neurons},
  journal = {Journal of Mathematical Biology},
  year = {1977},
  volume = {69},
  pages = {385--389}
}

@TECHREPORT{sejnowski86nettalk,
  author = {Sejnowski, T. J. and Rosenberg, C. R.},
  title = {{NET}talk: a parallel network that learns to read aloud},
  institution = {John Hopkins University, Electrical Engineering and Computer Science},
  year = {1986},
  number = {JHU/EECS-86/01},
  howpublished = {technical report}
}

@MISC{lvds_owner,
  author = {National Semiconductor},
  title = {{LVDS} Owner's Manual},
  howpublished = {LVDS.national.com},
  year = {2004},
  groupsearch = {0},
  key = {lvds},
  keywords = {spec}
}

@MISC{i2cspec,
  author = {{NXP Semiconductors}},
  title = {{I2C}-bus specification and user manual},
  year = {2012},
  edition = {Rev. 4},
  key = {i2c},
  keywords = {spec}
}

@phdthesis{schwartz2013diss,
  author   = {Marc-Olivier Schwartz},
  title    = {Reproducing Biologically Realistic Regimes on a Highly-Accelerated Neuromorphic Hardware System},
  school   = {Universit\"at Heidelberg},
  year     = {2013}
}

@article{schwartz2019green,
    title={Green AI},
    author={Roy Schwartz and Jesse Dodge and Noah A. Smith and Oren Etzioni},
    year={2019},
    eprint={1907.10597},
    archivePrefix={arXiv},
    primaryClass={cs.CY}
}

@phdthesis{schreiber2021accelerated,
  author = {Korbinian Schreiber},
  title = {Accelerated neuromorphic cybernetics},
  school = {Universit{\"a}t Heidelberg},
  year = 2021,
  month = jan
}

@unpublished{schreiber2022insectoidpath,
  author = {Schreiber, Korbinian and Wunderlich, Timo and Spilger, Philipp and Billaudelle, Sebastian and Cramer, Benjamin and Stradmann, Yannik and Pehle, Christian and M{\"u}ller, Eric and Petrovici, Mihai A. and Schemmel, Johannes and Meier, Karlheinz},
  title = {Insectoid path integration on accelerated neuromorphic hardware},
  note = {In preparation},
  year = 2022
}

@article{seitanidis2022identifying,
  title = {Identifying heart arrhythmias through multi-level algorithmic processing of {ECG} on edge devices},
  journal = {Procedia Computer Science},
  volume = 203,
  pages = {699--706},
  year = 2022,
  issn = {1877-0509},
  doi = {https://doi.org/10.1016/j.procs.2022.07.104},
  author = {Panagiotis Seitanidis and John Gialelis and Georgia Papaconstantinou}
}

@ARTICLE{senn02beyond,
  author = {Walter Senn},
  title = {Beyond Spike Timing: The Role of Nonlinear Plasticity and Unreliable
	Synapses},
  journal = {Biol. Cybern.},
  year = {2002},
  volume = {87},
  pages = {344 - 355},
  number = {5-6},
  month = {December},
  abstract = {Abstract. Spike-timing-dependent plasticity (STDP) strengthens synapses
	that are activated immediately before a postsynaptic spike, and weakens
	those that are activated after a spike. To prevent an uncontrolled
	growth of the synaptic strengths, weakening must dominate strengthening
	for uncorrelated spike times. However, this weight-normalization
	property would preclude Hebbian potentiation when the pre- and postsynaptic
	neurons are strongly active without specific spike-time correlations.
	We show that nonlinear STDP as inherent in the data of Markram et
	al. [(1997) Science 275:213-215] can preserve the benefits of both
	weight normalization and Hebbian plasticity, and hence can account
	for learning based on spike-time correlations and on mean firing
	rates. As examples we consider the moving-threshold property of the
	Bienenstock-Cooper-Munro rule, the development of direction-selective
	simple cells by changing short-term synaptic depression, and the
	joint adaptation of axonal and dendritic delays. Without threshold
	nonlinearity at low frequencies, the development of direction selectivity
	does not stabilize in a natural stimulation environment. Without
	synaptic unreliability there is no causal development of axonal and
	dendritic delays.},
  file = {senn02beyond.pdf:senn02beyond.pdf:PDF},
  keywords = {plasticity},
  url = {http://springerlink.metapress.com/link.asp?id=dma6n7jnaepway4c}
}

@article{schiess2016, crossref = {schiess2016somato} }
@article{schiess2016somato,
  author={Schiess, M. and Urbanczik, R. and Senn, W.},
  title={Somato-dendritic Synaptic Plasticity and Error-Backpropagation in Active Dendrites.},
  journal={PLoS ComputBiol},
  year={2016},
  volume={12},
  doi={10.1371/journal.pcbi.1004638}
}

@INPROCEEDINGS{Seo2011,
  author = {Seo, J. and Brezzo, B. and Liu, Y. and Parker, B.D. and Esser, S.K.
	and Montoye, R.K. and Rajendran, B. and Tierno, J.A. and Chang, L.
	and Modha, D.S. and Friedman, D.J.},
  title = {A 45nm CMOS neuromorphic chip with a scalable architecture for learning
	in networks of spiking neurons},
  booktitle = {Custom Integrated Circuits Conference (CICC), 2011 IEEE},
  year = {2011},
  pages = {1 -4},
  month = {sept.},
  doi = {10.1109/CICC.2011.6055293},
  file = {:Seo2011.pdf:PDF},
  issn = {0886-5930},
  keywords = {CMOS neuromorphic chip;associative memory task;binary synapses;crossbar
	fan out;digital neuron circuits;efficient on chip interneuron communication;learning
	circuits;novel transposable SRAM arrays;on-chip learning;pattern
	recognition;real time pattern classification;size 45 nm;spike timing
	dependent plasticity;spiking neurons;ultralow power brain like cognitive
	computer;CMOS digital integrated circuits;integrated circuit design;learning
	(artificial intelligence);neural nets;pattern classification;silicon-on-insulator;},
  owner = {simon},
  timestamp = {2012.12.15}
}

@ARTICLE{Seol2007,
  author = {Seol, Geun Hee and Ziburkus, Jokubas and Huang, ShiYong and Song,
	Lihua and Kim, In Tae and Takamiya, Kogo and Huganir, Richard L and
	Lee, Hey-Kyoung and Kirkwood, Alfredo},
  title = {Neuromodulators control the polarity of spike-timing-dependent synaptic
	plasticity},
  journal = {Neuron},
  year = {2007},
  volume = {55},
  pages = {919--929},
  number = {6},
  owner = {simon},
  publisher = {Elsevier},
  timestamp = {2013.04.25}
}

@INCOLLECTION{serrano_nips2005,
  author = {Rafael Serrano-Gotarredona and Matthias Oster and Patrick Lichtsteiner
	and Alejandro Linares-Barranco and Rafael Paz-Vicente and Francisco
	G\'omez-Rodr\'iguez and Havard Kolle Riis and Tobi Delbr\"uck and
	Shih-Chii Liu and S. Zahnd and Adrian M. Whatley and Rodney J. Douglas
	and Philipp H\"afliger and Gabriel Jimenez-Moreno and Anton Civit
	and Teresa Serrano-Gotarredona and Antonio Acosta-Jim\'enez and Bernab\'e
	Linares-Barranco},
  title = {{AER} Building Blocks for Multi-Layer Multi-Chip Neuromorphic Vision
	Systems},
  booktitle = {Advances in Neural Information Processing Systems 18},
  publisher = {MIT Press},
  year = {2006},
  editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
  pages = {1217--1224},
  address = {Cambridge, MA}
}

@ARTICLE{serrano2009caviar,
  author={Serrano-Gotarredona, R. and Oster, M. and Lichtsteiner, P. and Linares-Barranco, A. and Paz-Vicente, R. and Gomez-Rodriguez, F. and Camunas-Mesa, L. and Berner, R. and Rivas-Perez, M. and Delbruck, T. and Shih-Chii Liu and Douglas, R. and Hafliger, P. and Jimenez-Moreno, G. and Ballcels, A.C. and Serrano-Gotarredona, T. and Acosta-Jimenez, A.J. and Linares-Barranco, B.},
  journal={Neural Networks, IEEE Transactions on},
  title={CAVIAR: A 45k neuron, 5M synapse, 12G connects/s AER hardware sensory--processing--learning--actuating system for high-speed visual object recognition and tracking},
  year={2009},
  volume={20},
  number={9},
  pages={1417-1438},
  abstract={This paper describes CAVIAR, a massively parallel hardware implementation of a spike-based sensing-processing-learning-actuating system inspired by the physiology of the nervous system. CAVIAR uses the asynchronous address-event representation (AER) communication framework and was developed in the context of a European Union funded project. It has four custom mixed-signal AER chips, five custom digital AER interface components, 45 k neurons (spiking cells), up to 5 M synapses, performs 12 G synaptic operations per second, and achieves millisecond object recognition and tracking latencies.},
  keywords={biocomputing;computer vision;feedforward;neurophysiology;object detection;object recognition;parallel architectures;CAVIAR;European Union funded project;asynchronous address-event representation communication framework;custom digital AER interface components;custom mixed-signal AER chips;high-speed visual object recognition;high-speed visual object tracking;nervous system;parallel hardware implementation;physiology;spike-based sensing-processing-learning-actuating system;Address–event representation (AER);neuromorphic chips;neuromorphic systems;vision;Action Potentials;Artificial Intelligence;Computers;Humans;Learning;Motion Perception;Neural Networks (Computer);Neurons;Pattern Recognition, Visual;Psychomotor Performance;Retina;Synapses;Time Factors;Vision, Ocular;Visual Perception},
  doi={10.1109/TNN.2009.2023653},
  ISSN={1045-9227},
}

@ARTICLE{sexton98toward,
  author = {Sexton, R. S. and Dorsez, R. E. and Johnson J. D.},
  title = {Toward global optimization of neural networks: {A} comparison of
	the genetic algorithm and backpropagation},
  journal = {Decision Support Systems},
  year = {1998},
  volume = {22},
  pages = {171-185},
  number = {2},
  file = {sexton98toward.pdf:sexton98toward.pdf:PDF}
}

@ARTICLE{shackleford_kluwer2001,
  author = {Shackleford, Barry et. al.},
  title = {A High-Performance, Pipelined, {FPGA}-Based Genetic Algorithm Machine},
  journal = {Genetic Programming and Evolvable Machines},
  year = {2001},
  volume = {1},
  pages = {33-60},
  number = {2},
  month = {March},
  groupsearch = {0}
}

@ARTICLE{shadlen98variable,
  author = {Michael N Shadlen and W T Newsome},
  title = {The variable discharge of cortical neurons: implications for connectivity,
	computation, and information coding},
  journal = {J Neurosci},
  year = {1998},
  volume = {18},
  pages = {3870--96},
  number = {10},
  month = {May},
  abstract = {Cortical neurons exhibit tremendous variability in the number and
	temporal distribution of spikes in their discharge patterns. Furthermore,
	this variability appears to be conserved over large regions of the
	cerebral cortex, suggesting that it is neither reduced nor expanded
	from stage to stage within a processing pathway. To investigate the
	principles underlying such statistical homogeneity, we have analyzed
	a model of synaptic integration incorporating a highly simplified
	integrate and fire mechanism with decay. We analyzed a "high-input
	regime" in which neurons receive hundreds of excitatory synaptic
	inputs during each interspike interval. To produce a graded response
	in this regime, the neuron must balance excitation with inhibition.
	We find that a simple integrate and fire mechanism with balanced
	excitation and inhibition produces a highly variable interspike interval,
	consistent with experimental data. Detailed information about the
	temporal pattern of synaptic inputs cannot be recovered from the
	pattern of output spikes, and we infer that cortical neurons are
	unlikely to transmit information in the temporal pattern of spike
	discharge. Rather, we suggest that quantities are represented as
	rate codes in ensembles of 50-100 neurons. These column-like ensembles
	tolerate large fractions of common synaptic input and yet covary
	only weakly in their spike discharge. We find that an ensemble of
	100 neurons provides a reliable estimate of rate in just one interspike
	interval (10-50 msec). Finally, we derived an expression for the
	variance of the neural spike count that leads to a stable propagation
	of signal and noise in networks of neurons-that is, conditions that
	do not impose an accumulation or diminution of noise. The solution
	implies that single neurons perform simple algebra resembling averaging,
	and that more sophisticated computations arise by virtue of the anatomical
	convergence of novel combinations of inputs to the cortical column
	from external sources.},
  affiliation = {Department of Physiology and Biophysics and Regional Primate Research
	Center, University of Washington, Seattle, Washington 98195-7290,
	USA.},
  keywords = {Reaction Time, Action Potentials, Higher Nervous Activity, Electrophysiology,
	Information Theory, Neural Pathways, Interneurons, Data Interpretation:
	Statistical, Macaca mulatta, Models: Neurological, Animals, Mental
	Processes, Cerebral Cortex}
}

@ARTICLE{Shafi2007,
  author = {M. Shafi and Y. Zhou and J. Quintana and C. Chow and J. Fuster and
	M. Bodner},
  title = {Variability in neuronal activity in primate cortex during working
	memory tasks },
  journal = {Neuroscience },
  year = {2007},
  volume = {146},
  pages = {1082 - 1108},
  number = {3},
  doi = {10.1016/j.neuroscience.2006.12.072},
  issn = {0306-4522},
  keywords = {spike trains},
  owner = {simon},
  timestamp = {2013.05.05},
  url = {http://www.sciencedirect.com/science/article/pii/S0306452206017593}
}

@MISC{shah_date02,
  author = {Shah, Akshat H and Dugalleix, Stephane and Lemery, Francois},
  title = {Technology Migration of a High Performance {CMOS} Amplifier Using
	an Automated Fron-to-Back Analog Design Flow},
  howpublished = {Conference and Exhibition on Design Automation \& Test in Europe
	(DATE 2002)},
  month = mar,
  year = {2002},
  address = {Le Palais de Congres, Paris, France},
  groupsearch = {0}
}

@INPROCEEDINGS{Shapiro2011,
  author = {Shapiro, D. and Parri, J. and Desmarais, J. -M and Groza, V. and
	Bolic, M.},
  title = {ASIPs for artificial neural networks},
  booktitle = {Applied Computational Intelligence and Informatics (SACI), 2011 6th
	IEEE International Symposium on},
  year = {2011},
  pages = {529-533},
  doi = {10.1109/SACI.2011.5873060},
  keywords = {Hopfield neural nets;application specific integrated circuits;content-addressable
	storage;embedded systems;hardware-software codesign;instruction sets;microprocessor
	chips;neural chips;artificial neural networks;automated instruction-set
	identification algorithm;automatic hardware/software partitioning;bidirectional
	associative memory;contemporary embedded system designs;custom instruction
	candidates;customized application-specific processors;homogeneous
	multiprocessor;hopfield auto-associative memory networks;legacy neural
	network applications;network initialization;power consumption;uniprocessor
	ASIP;Acceleration;Artificial neural networks;Biological neural networks;Hardware;Neurons;Program
	processors},
  owner = {simon},
  timestamp = {2013.04.21}
}

@ARTICLE{sharkey96combining,
  author = {Sharkey, A. J. C.},
  title = {On combining artificial neural nets},
  journal = {Connection Science},
  year = {1996},
  volume = {8},
  pages = {299--314},
  number = {3/4},
  file = {sharkey96combining.pdf:sharkey96combining.pdf:PDF}
}

@ARTICLE{sharkey97combining,
  author = {Sharkey, A. J. C. and Sharkey, N. E.},
  title = {Combining diverse neural nets},
  journal = {Knowlegde Engineering Review},
  year = {1997},
  volume = {12},
  pages = {1--17},
  number = {3},
  file = {sharkey97combining.pdf:sharkey97combining.pdf:PDF}
}

@ARTICLE{shastri2021photonics,
  title = {Photonics for artificial intelligence and neuromorphic computing},
  author = {Shastri, Bhavin J. and Tait, Alexander N. and Ferreira de Lima, T. and Pernice, Wolfram H. P. and Bhaskaran, Harish and Wright, C. D. and Prucnal, Paul R.},
  journal = {Nature Photonics},
  volume = 15,
  number = 2,
  year = 2021,
  pages = {102--114},
  publisher = {Nature Publishing Group}
}

@inproceedings{sheik2011systematic,
  title={Systematic configuration and automatic tuning of neuromorphic systems},
  author={Sheik, Sadique and Stefanini, Fabio and Neftci, Emre and Chicca, Elisabetta and Indiveri, Giacomo},
  booktitle={2011 IEEE International Symposium of Circuits and Systems (ISCAS)},
  pages={873--876},
  year={2011},
  organization={IEEE}
}


@ARTICLE{Sheik2012,
  author = {Sheik, Sadique and Coath, Martin and Indiveri, Giacomo and Denham,
	Susan L and Wennekers, Thomas and Chicca, Elisabetta},
  title = {Emergent auditory feature tuning in a real-time neuromorphic VLSI
	system},
  journal = {Frontiers in Neuroscience},
  year = {2012},
  volume = {6},
  number = {17},
  abstract = {Many sounds of ecological importance, such as communication calls,
	are characterised by time-varying spectra. However, most neuromorphic
	auditory models to date have focused on distinguishing mainly static
	patterns, under the assumption that dynamic patterns can be learned
	as sequences of static ones. In contrast, the emergence of dynamic
	feature sensitivity through exposure to formative stimuli has been
	recently modeled in a network of spiking neurons based on the thalamocortical
	architecture. The proposed network models the effect of lateral and
	recurrent connections between cortical layers, distance-dependent
	axonal transmission delays, and learning in the form of Spike Timing
	Dependent Plasticity (STDP), which effects stimulus-driven changes
	in the pattern of network connectivity. In this paper we demonstrate
	how these principles can be efficiently implemented in neuromorphic
	hardware. In doing so we address two principle problems in the design
	of neuromorphic systems: real-time event-based asynchronous communication
	in multi-chip systems, and the realization in hybrid analog/digital
	VLSI technology of neural computational principles that we propose
	underlie plasticity in neural processing of dynamic stimuli. The
	result is a hardware neural network that learns in real-time and
	shows preferential responses, after exposure, to stimuli exhibiting
	particular spectrotemporal patterns. The availability of hardware
	on which the model can be implemented, makes this a significant step
	towards the development of adaptive, neurobiologically plausible,
	spike-based, artificial sensory systems.},
  doi = {10.3389/fnins.2012.00017},
  issn = {1662-453X},
  owner = {simon},
  timestamp = {2013.05.03},
  url = {http://www.frontiersin.org/neuromorphic_engineering/10.3389/fnins.2012.00017/abstract}
}

@ARTICLE{shelley02highconductance,
  author = {Michael Shelley and David McLaughlin and Robert Shapley and Jacob
	Wielaard},
  title = {States of High Conductance in a Large-Scale Model of the Visual Cortex},
  journal = {J. Comp. Neurosci.},
  year = {2002},
  volume = {13},
  pages = {93--109},
  abstract = {This paper reports on the consequences of large, activity dependent,
	synaptic conductances for neurons in a large-scale neuronal network
	model of the input layer 4Calpha of the Macaque primary visual cortex
	(Area V1). This high conductance state accounts for experimental
	observations about orientation selectivity, dynamics, and response
	magnitude (McLaughlin et al., 2000), and the linear dependence of
	simple cells on visual stimuli (Wielaard et al., 2001). The source
	of the large conductances in the model can be traced to inhibitory
	cortico-cortical synapses, and the model's predictions of large conductance
	changes are consistent with recent intracellular measurements (Borg-Graham,
	Monier, and Fregnac, 1998; Hirsch et al., 1998; Anderson, Carandini,
	and Ferster, 2000). During visual stimulation, these conductances
	are large enough that their associated time- scales become the smallest
	in the model cortex. One consequence of this activity driven separation
	of time-scales is that a neuron responds very quickly to temporal
	changes in its synaptic drive with its intracellular potential tracking
	closely at an effective reversal potential composed of instantaneous
	synaptic inputs. From the effective potential and large synaptic
	conductance the spiking activity of a cell can be expressed in an
	interesting and simplified manner, with the result suggesting how
	accurate and smoothly grated responses are achieved in the model
	network. Further, since neurons in this high-conductance state respond
	quickly, they are also good candidates as coincidence conductors
	and burst transmitters.},
  contents = {In a high conductance state the membrane time constant = C / total
	conductance is the shortest time-scale. The neuron acts not as a
	leaky integrator of its inputs but as a near-instantaneous function
	of its inputs. *************When subthreshold, the membrane potential
	is well approximated by the effective reversal potential Vs(t), which
	is expressed explicitly in terms of the instantaneous input conductances
	to the cell. Because of the predominance of the inhibition, the modulation
	of Vs (and hence v) is approximately the ratio of excitatory to inhibitory
	conductances.*****************When forced away from Vs (as when v(t)
	spikes), the membrane potential returns quickly to being very close
	to Vs(t). If Vs happens to be above threshold, this rapid return
	will likely lead to another spike. **************the mean firing
	rate increases monotously with Vs.},
  groupsearch = {0}
}

@INPROCEEDINGS{shepard1997noise,
  author = {Shepard, K.L. and Narayanan, V.},
  title = {Noise in deep submicron digital design},
  booktitle = {Proceedings of the 1996 IEEE/ACM international conference on Computer-aided
	design},
  year = {1997},
  pages = {524--531},
  organization = {IEEE Computer Society},
  owner = {simon},
  timestamp = {2013.02.08}
}

@ARTICLE{shinomoto05,
  author = {Shigeru Shinomoto and Ichiro Fujita and Youichi Miyazaki and Youichi
	Miyazaki and Hiroshi Tamura and Hiroshi Tamura},
  title = {Regional and laminar differences in in vivo firing patterns of primate
	cortical neurons},
  journal = {J Neurophysiol},
  year = {2005},
  volume = {94},
  pages = {567--575}
}

@INPROCEEDINGS{shon-learning,
  author = {Aaron P. Shon and David Hsu and Chris Diorio},
  title = {Learning spike-based correlations and conditional probabilities in
	silicon},
  booktitle = {Advances in Neural Information Processing Systems (NIPS)},
  year = {2002},
  citeseerurl = {citeseer.nj.nec.com/543728.html},
  keywords = {spiking learning}
}

@MISC{shon-temporal,
  author = {Aaron P. Shon and Rajesh P. N. Rao},
  title = {Temporal Sequence Learning With Dynamic Synapses},
  citeseerurl = {citeseer.nj.nec.com/557325.html},
  keywords = {spiking learning}
}

@ARTICLE{shouval02evidence,
  author = {Harel Z. Shouval and Gastone C. Castellani and Brian S. Blais and
	Luk C. Yeung and Leon N Cooper},
  title = {Converging evidence for a simpli?ed biophysical model of synaptic
	plasticity},
  journal = {Biol. Cybern.},
  year = {2002},
  volume = {87},
  pages = {383-391},
  number = {5-6},
  month = {December},
  abstract = {Different mechanisms that could form the molecular basis for bi-directional
	synaptic plasticity have been identified experimentally and corresponding
	biophysical models can be constructed. However, such models are complex
	and therefore it is hard to deduce their consequences to compare
	them to existing abstract models of synaptic plasticity. In this
	paper we examine two such models: a phenomenological one inspired
	by the phenomena of AMPA receptor insertion, and a more complex biophysical
	model based on the phenomena of AMPA receptor phosphorylation. We
	show that under certain approximations both these models can be mapped
	on to an equivalent, calcium-dependent, differential equation. Intracellular
	calcium concentration varies locally in each postsynaptic compartment,
	thus the plasticity rule we extract is a single-synapse rule. We
	convert this single synapse plasticity equation to a multi-synapse
	rule by incorporating a model of the NMDA receptor. Finally we suggest
	a mathematical embodiment of metaplasticity, which is consistent
	with observations on NMDA receptor properties and dependence on cellular
	activity. These results, in combination with some of our previous
	results, produce converging evidence for the calcium control hypothesis
	including a dependence of synaptic plasticity on the level of intercellular
	calcium as well as on the temporal pattern of calcium transients.},
  file = {shouval02evidence.pdf:shouval02evidence.pdf:PDF},
  keywords = {plasticity},
  owner = {mreuss},
  url = {http://www.springerlink.com/link.asp?id=fy8dxcp7wvuu8473}
}

@article{shouval2010spike,
  title={Spike timing dependent plasticity: a consequence of more fundamental learning rules},
  author={Shouval, Harel Z and Wang, Samuel S-H and Wittenberg, Gayle M},
  journal={Frontiers in computational neuroscience},
  volume={4},
  year={2010},
  publisher={Frontiers Research Foundation}
}

@ARTICLE{shu03barrages,
  author = {Yousheng Shu and Andrea Hasenstaub and Mathilde Badoual and Thierry
	Bal and David A. McCormick},
  title = {Barrages of Synaptic Activity Control the Gain and Sensitivity of
	Cortical Neurons},
  journal = {Journal of Neuroscience},
  year = {2003},
  volume = {23},
  pages = {10388--10401},
  number = {32},
  month = {November}
}

@ARTICLE{Shulz2010,
  author = {Shulz, Daniel E and Jacob, Vincent},
  title = {Spike timing dependent plasticity in the intact brain: counteracting
	spurious spike coincidences.},
  journal = {Frontiers in Synaptic Neuroscience},
  year = {2010},
  volume = {2},
  number = {137},
  abstract = {A computationally rich algorithm of synaptic plasticity has been proposed
	based on the experimental observation that the sign and amplitude
	of the change in synaptic weight is dictated by the temporal order
	and temporal contiguity between pre- and post-synaptic activities.
	For more than a decade, this spike timing-dependent plasticity (STDP)
	has been studied mainly in brain slices of different brain structures
	and cultured neurons. Although not yet compelling, evidences for
	the STDP rule in the intact brain, including primary sensory cortices,
	have been provided lastly. From insects to mammals, the presentation
	of precisely timed sensory inputs drives synaptic and functional
	plasticity in the intact central nervous system, with similar timing
	requirements than the in vitro defined STDP rule. The convergent
	evolution of this plasticity rule in species belonging to so distant
	phylogenic groups points to the efficiency of STDP, as a mechanism
	for modifying synaptic weights, as the basis of activity-dependent
	development, learning and memory. In spite of the ubiquity of STDP
	phenomena, a number of significant variations of the rule are observed
	in different structures, neuronal types and even synapses on the
	same neuron, as well as between in vitro and in vivo conditions.
	In addition, the state of the neuronal network, its ongoing activity
	and the activation of ascending neuromodulatory systems in different
	behavioral conditions have dramatic consequences on the expression
	of spike timing-dependent synaptic plasticity, and should be further
	explored.},
  doi = {10.3389/fnsyn.2010.00137},
  issn = {1663-3563},
  url = {http://www.frontiersin.org/synaptic_neuroscience/10.3389/fnsyn.2010.00137/abstract}
}

@INPROCEEDINGS{siddiqi_icec98comparison,
  author = {Siddiqi, A. A. and Lucas, S. M.},
  title = {A comparison of matrix rewriting versus direct encoding for evolving
	neural networks},
  booktitle = {Proceedings of the 1998 IEEE International Conference on Evolutionary
	Computation ({ICEC}'98)},
  year = {1998},
  pages = {392--397},
  address = {Piscataway, NJ},
  publisher = {IEEE Press},
  file = {siddiqi_icec98comparison.pdf:siddiqi_icec98comparison.pdf:PDF}
}

@BOOK{siegert98betriebssysteme,
  title = {Betriebssysteme: Eine Einf{\"u}hrung},
  publisher = {R. Oldenbourg Verlag},
  year = {1998},
  author = {Siegert, H.-J. and Baumgarten, U.},
  address = {M{\"u}nchen},
  isbn = {3-486-24019-6}
}

@ARTICLE{silver2010neuronal,
  author = {Silver, R.A.},
  title = {Neuronal arithmetic},
  journal = {Nature Reviews Neuroscience},
  year = {2010},
  volume = {11},
  pages = {474--489},
  number = {7},
  publisher = {Nature Publishing Group}
}

@article{silver2016mastering,
  title={Mastering the game of Go with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={Nature},
  volume={529},
  number={7587},
  pages={484--489},
  year={2016},
  publisher={Nature Publishing Group}
}

@CONFERENCE{simard2003,
  author = {P.Y. Simard and D. Steinkraus and J.C. Platt},
  title = {Best practices for convolutional neural networks applied to visual
	document analysis},
  booktitle = {Intl. Conf. Document Analysis and Recognition},
  year = {2003},
  pages = {958--962},
  file = {simard2003.pdf:simard2003.pdf:PDF},
  keywords = {convolutional NN},
  owner = {fieres}
}

@INPROCEEDINGS{Simunic2001,
  author = {Simunic, Tajana and Benini, Luca and Acquaviva, Andrea and Glynn,
	Peter and De Micheli, Giovanni},
  title = {Dynamic voltage scaling and power management for portable systems},
  booktitle = {Proceedings of the 38th annual Design Automation Conference},
  year = {2001},
  pages = {524--529},
  organization = {ACM},
  owner = {simon},
  timestamp = {2013.04.22}
}

@ARTICLE{sincich2007,
  author = {Lawrence C. Sincich and Daniel L. Adams and John R. Economides and
	Jonathan C. Horton},
  title = {Transmission of Spike Trains at the Retinogeniculate Synapse},
  journal = {Journal of Neuroscience},
  year = {2007},
  volume = {27},
  pages = {2683--2692},
  number = {10}
}

@MISC{sinsel_diplomathesis,
  author = {Sinsel, Alexander},
  title = {Linuxportierung auf einen eingebetteten PowerPC 405 zur Steuerung
	eines neuronalen Netzwerkes},
  howpublished = {Diploma thesis (German), University of Heidelberg, HD-KIP-03-14},
  year = {2001},
  keywords = {vision}
}

@ARTICLE{Sjoestroem2010, crossref={sjoestroem2010spike}}
@ARTICLE{sjoestroem2010spike,
  author = {Sj\"ostr\"om, Jesper and Gerstner, Wulfram},
  title = {Spike-timing dependent plasticity},
  journal = {Scholarpedia},
  year = {2010},
  volume = {5},
  pages = {1362},
  number = {2},
  doi = {10.4249/scholarpedia.1362},
  owner = {simon},
  timestamp = {2013.04.25},
  url = {http://www.scholarpedia.org/article/Spike-timing_dependent_plasticity}
}

@ARTICLE{Sjoestroem2004,
  author = {Sj\"ostr\"om, Per Jesper and Turrigiano, Gina G. and Nelson, Sacha
	B.},
  title = {Endocannabinoid-Dependent Neocortical Layer-5 LTD in the Absence
	of Postsynaptic Spiking},
  journal = {Journal of Neurophysiology},
  year = {2004},
  volume = {92},
  pages = {3338-3343},
  number = {6},
  abstract = {Long-term depression (LTD) was induced in neocortical layer 5 pyramidal
	connections by pairing presynaptic firing with subthreshold postsynaptic
	depolarization (dLTD) or via a spike-timing protocol (tLTD). Like
	tLTD, dLTD reduced short-term depression and increased the coefficient
	of variation consistent with a presynaptic site of expression. Also
	like tLTD, dLTD was blocked by CB1 cannibinoid receptor blockade
	and required activation of presumably presynaptic NR2B-containing
	N-methyl-d-aspartate receptors. The two forms of LTD had identical
	magnitudes and time courses and occluded one another, and neither
	depended on frequency. Finally, dLTD shares with tLTD the asymmetric
	temporal window of induction. In conclusion, the types of LTD induced
	by these two protocols are indistinguishable, suggesting that the
	mechanism that underlies tLTD paradoxically does not require postsynaptic
	spiking: The subthreshold postsynaptic depolarizations of dLTD appear
	to fully substitute for postsynaptic spiking},
  doi = {10.1152/jn.00376.2004},
  eprint = {http://jn.physiology.org/content/92/6/3338.full.pdf+html},
  owner = {simon},
  timestamp = {2013.04.24},
  url = {http://jn.physiology.org/content/92/6/3338.abstract}
}

@ARTICLE{Sjoestroem2001,
  author = {Per Jesper Sj\"{o}str\"{o}m and Gina G Turrigiano and Sacha B Nelson},
  title = {Rate, Timing, and Cooperativity Jointly Determine Cortical Synaptic
	Plasticity},
  journal = {Neuron},
  year = {2001},
  volume = {32},
  pages = {1149 - 1164},
  number = {6},
  doi = {10.1016/S0896-6273(01)00542-6},
  issn = {0896-6273},
  url = {http://www.sciencedirect.com/science/article/pii/S0896627301005426}
}

@ARTICLE{sjostrom02spike,
  author = {Sjostrom, Per Jesper and Nelson, Sacha B.},
  title = {Spike timing, calcium signals and synaptic plasticity},
  journal = {Current Opinion in Neurobiology},
  year = {2002},
  volume = {12},
  pages = {305--314},
  number = {3},
  month = jun,
  abstract = {Plasticity at central synapses depends critically on the timing of
	presynaptic and postsynaptic action potentials. Key initial steps
	in synaptic plasticity involve the back-propagation of action potentials
	into the dendritic tree and calcium influx that depends nonlinearly
	on the action potential and synaptic input. These initial steps are
	now better understood. In addition, recent studies of processes as
	diverse as gene expression and channel inactivation suggest that
	responses to calcium transients depend not only their amplitude,
	but on their time course and on the location of their origin.},
  keywords = {STDP, calcium, LTP, LTD, plasticity, memory},
  owner = {mreuss},
  timestamp = {2007.06.14},
  url = {http://www.sciencedirect.com/science/article/B6VS3-45Y4MVK-F/2/33505e3b4e35be58778ba208ecea481c}
}

@ARTICLE{skowronski07automatic,
  author = {Skowronski, Mark D. and Harris, John G.},
  title = {Automatic speech recognition using a predictive echo state network
	classifier},
  journal = {Neural Networks},
  year = {2007},
  volume = {20},
  pages = {414--423},
  number = {3},
  month = apr,
  abstract = {We have combined an echo state network (ESN) with a competitive state
	machine framework to create a classification engine called the predictive
	ESN classifier. We derive the expressions for training the predictive
	ESN classifier and show that the model was significantly more noise
	robust compared to a hidden Markov model in noisy speech classification
	experiments by 8+/-1 dB signal-to-noise ratio. The simple training
	algorithm and noise robustness of the predictive ESN classifier make
	it an attractive classification engine for automatic speech recognition.},
  booktitle = {Echo State Networks and Liquid State Machines},
  keywords = {Echo state network, Automatic speech recognition, Mixture of experts,
	Noise robustness,, liquid},
  owner = {mreuss},
  timestamp = {2007.06.14},
  url = {http://www.sciencedirect.com/science/article/B6T08-4NKYNTD-1/2/7fbbf24ed0165e8f55998d7a0b4c852a}
}

@MANUAL{slurm_homepage,
  key = {SLURM},
  author = {{LLNL} and {SchedMD} and others},
  title = {Simple Linux Utility for Resource Management},
  url = {http://slurm.schedmd.com},
  year = 2014,
  note = {[Online; accessed: 2014-04-29]}
}

@software{slurm2022,
  author = {{LLNL} and {SchedMD} and others},
  title = {Simple Linux Utility for Resource Management},
  url = {http://slurm.schedmd.com},
  year = 2022
}

@BOOK{smith1985implementation,
  title = {Implementation of precise interrupts in pipelined processors},
  publisher = {IEEE Computer Society Press},
  year = {1985},
  author = {Smith, J.E. and Pleszkun, A.R.},
  volume = {13},
  number = {3},
  owner = {simon},
  timestamp = {2013.01.09}
}

@INPROCEEDINGS{Smith1998,
  author = {Smith, James E.},
  title = {A study of branch prediction strategies},
  booktitle = {25 years of the international symposia on Computer architecture (selected
	papers)},
  year = {1998},
  series = {ISCA '98},
  pages = {202--215},
  address = {New York, NY, USA},
  publisher = {ACM},
  acmid = {285980},
  doi = {10.1145/285930.285980},
  isbn = {1-58113-058-9},
  location = {Barcelona, Spain},
  numpages = {14},
  url = {http://doi.acm.org/10.1145/285930.285980}
}

@ARTICLE{smith08spatial,
  author = {M Smith and A Kohn},
  title = {Spatial and Temporal Scales of Neuronal Correlation in Primary Visual
	Cortex},
  journal = {J Neurosci},
  year = {2008},
  volume = {28},
  pages = {12591--12603},
  number = {48},
  month = {Nov},
  abstract = {The spiking activity of cortical neurons is correlated. For instance,
	trial-to-trial fluctuations in response strength are shared between
	neurons, and spikes often occur synchronously. Understanding the
	properties and mechanisms that generate these forms of correlation
	is critical for determining their role in cortical processing. We
	therefore investigated the spatial extent and functional specificity
	of correlated spontaneous and evoked activity. Because feedforward,
	recurrent, and feedback pathways have distinct extents and specificity,
	we reasoned that these measurements could elucidate the contribution
	of each type of input. We recorded single unit activity with microelectrode
	arrays which allowed us to measure correlation in many hundreds of
	pairings, across a large range of spatial scales. Our data show that
	correlated evoked activity is generated by two mechanisms that link
	neurons with similar orientation preferences on different spatial
	scales: one with high temporal precision and a limited spatial extent
	( approximately 3 mm), and a second that gives rise to correlation
	on a slow time scale and extends as far as we were able to measure
	(10 mm). The former is consistent with common input provided by horizontal
	connections; the latter likely involves feedback from extrastriate
	cortex. Spontaneous activity was correlated over a similar spatial
	extent, but approximately twice as strongly as evoked activity. Visual
	stimuli thus caused a substantial decrease in correlation, particularly
	at response onset. These properties and the circuit mechanism they
	imply provide new constraints on the functional role that correlation
	may play in visual processing.},
  affiliation = {Center for the Neural Basis of Cognition, Carnegie Mellon University,
	Pittsburgh, Pennsylvania 15213, Department of Neuroscience, Albert
	Einstein College of Medicine, Bronx, New York 10461, and Center for
	Neural Science, New York University, New York, New York 10003.}
}

@ARTICLE{smith01embodiment,
  author = {Smith, R. E. and Bonacina, C. and Kearney, P. and Merlat, W.},
  title = {Embodiment of Evolutionary Computation in General Agents},
  journal = {Evolutionary Computation},
  year = {2001},
  volume = {8},
  pages = {475--493},
  number = {4}
}

@INPROCEEDINGS{smith01tunable,
  author = {Smith, R. E. and Smith, J. E.},
  title = {New methods for tunable, random landscapes},
  booktitle = {Foundations of Genetic Algorithms},
  year = {2001},
  editor = {Martin, W. N. and Spears, W.M.},
  volume = {6},
  pages = {47--67}
}

@INPROCEEDINGS{Snider2008,
  author = {Snider, G.S.},
  title = {Spike-timing-dependent learning in memristive nanodevices},
  booktitle = {Nanoscale Architectures, 2008. NANOARCH 2008. IEEE International
	Symposium on},
  year = {2008},
  pages = {85-92},
  doi = {10.1109/NANOARCH.2008.4585796},
  keywords = {CMOS integrated circuits;nanoelectronics;nanowires;neural chips;CMOS
	neurons;conventional CMOS;learning dynamics;massive parallelism;memristive
	materials;memristive nanodevices;nanoscale computational communication;nanoscale
	synapses;nanowires;neuromorphic hardware;neuromorphic paradigm;post
	synaptic neurons;presynaptic neurons;pulse-width-modulated signals;spike-timing-dependent
	learning;spike-timing-dependent plasticity;synaptic state variables;time-division
	multiplexing;timing-based learning laws;Circuit faults;Communication
	system control;Concurrent computing;Hardware;Nanostructured materials;Neuromorphics;Neurons;Parallel
	processing;Power dissipation;Scalability;adaptive systems;analog
	memories;learning systems;neural network hardware;nonlinear circuits},
  owner = {simon},
  timestamp = {2013.04.19}
}

@ARTICLE{softky93highly,
  author = {W R Softky and C Koch},
  title = {The highly irregular firing of cortical cells is inconsistent with
	temporal integration of random {EPSP}s},
  journal = {J Neurosci},
  year = {1993},
  volume = {13},
  pages = {334--50},
  number = {1},
  month = {Jan},
  abstract = {How random is the discharge pattern of cortical neurons? We examined
	recordings from primary visual cortex (V1; Knierim and Van Essen,
	1992) and extrastriate cortex (MT; Newsome et al., 1989a) of awake,
	behaving macaque monkey and compared them to analytical predictions.
	For nonbursting cells firing at sustained rates up to 300 Hz, we
	evaluated two indices of firing variability: the ratio of the variance
	to the mean for the number of action potentials evoked by a constant
	stimulus, and the rate-normalized coefficient of variation (Cv) of
	the interspike interval distribution. Firing in virtually all V1
	and MT neurons was nearly consistent with a completely random process
	(e.g., Cv approximately 1). We tried to model this high variability
	by small, independent, and random EPSPs converging onto a leaky integrate-and-fire
	neuron (Knight, 1972). Both this and related models predicted very
	low firing variability (Cv << 1) for realistic EPSP depolarizations
	and membrane time constants. We also simulated a biophysically very
	detailed compartmental model of an anatomically reconstructed and
	physiologically characterized layer V cat pyramidal cell (Douglas
	et al., 1991) with passive dendrites and active soma. If independent,
	excitatory synaptic input fired the model cell at the high rates
	observed in monkey, the Cv and the variability in the number of spikes
	were both very low, in agreement with the integrate-and-fire models
	but in strong disagreement with the majority of our monkey data.
	The simulated cell only produced highly variable firing when Hodgkin-Huxley-like
	currents (INa and very strong IDR) were placed on distal dendrites.
	Now the simulated neuron acted more as a millisecond-resolution detector
	of dendritic spike coincidences than as a temporal integrator. We
	argue that neurons that act as temporal integrators over many synaptic
	inputs must fire very regularly. Only in the presence of either fast
	and strong dendritic nonlinearities or strong synchronization among
	individual synaptic events will the degree of predicted variability
	approach that of real cortical neurons.},
  affiliation = {Division of Physics, Mathematics, and Astronomy, California Institute
	of Technology, Pasadena 91125.}
}

@ARTICLE{Soltani_Wang_2010,
  author = {Soltani, Alireza and Wang, Xiao-Jing},
  title = {Synaptic computation underlying probabilistic inference.},
  journal = {Nature Neuroscience},
  year = {2010},
  volume = {13},
  pages = {112–9},
  number = {1},
  abstract = {We propose that synapses may be the workhorse of the neuronal computations
	that underlie probabilistic reasoning. We built a neural circuit
	model for probabilistic inference in which information provided by
	different sensory cues must be integrated and the predictive powers
	of individual cues about an outcome are deduced through experience.
	We found that bounded synapses naturally compute, through reward-dependent
	plasticity, the posterior probability that a choice alternative is
	correct given that a cue is presented. Furthermore, a decision circuit
	endowed with such synapses makes choices on the basis of the summed
	log posterior odds and performs near-optimal cue combination. The
	model was validated by reproducing salient observations of, and provides
	insights into, a monkey experiment using a categorization task. Our
	model thus suggests a biophysical instantiation of the Bayesian decision
	rule, while predicting important deviations from it similar to the
	“base-rate neglect” observed in human studies when alternatives have
	unequal prior probabilities.},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/20010823}
}

@ARTICLE{song01cortical,
  author = {Song, Sen and Abbott, L. F.},
  title = {Cortical Development and Remapping through Spike Timing-Dependent
	Plasticity},
  journal = {Neuron},
  year = {2001},
  volume = {32},
  pages = {339--350},
  number = {2},
  month = oct,
  abstract = {Long-term modification of synaptic efficacy can depend on the timing
	of pre- and postsynaptic action potentials. In model studies, such
	spike timing-dependent plasticity (STDP) introduces the desirable
	features of competition among synapses and regulation of postsynaptic
	firing characteristics. STDP strengthens synapses that receive correlated
	input, which can lead to the formation of stimulus-selective columns
	and the development, refinement, and maintenance of selectivity maps
	in network models. The temporal asymmetry of STDP suppresses strong
	destabilizing self-excitatory loops and allows a group of neurons
	that become selective early in development to direct other neurons
	to become similarly selective. STDP, acting alone without further
	hypothetical global constraints or additional forms of plasticity,
	can also reproduce the remapping seen in adult cortex following afferent
	lesions.},
  owner = {mreuss},
  timestamp = {2007.06.14},
  url = {http://www.sciencedirect.com/science/article/B6WSS-4C5RF9F-K/2/4ea9cc32d87453c29c4d5247b9b38995}
}

@ARTICLE{song00competitive,
  author = {S. Song and K. Miller and L. Abbott},
  title = {Competitive Hebbian Learning Through Spiketiming-Dependent Synaptic
	Plasticity},
  journal = {Nat. Neurosci.},
  year = {2000},
  volume = {3},
  pages = {919--926},
  abstract = {Hebbian models of development and learning require both activity-dependent
	synaptic plasticity and a mechanism that induces competition between
	different synapses. Recent experiments have characterized a form
	of long-term synaptic plasticity that depends on the relative timing
	of pre- and postsynaptic action potentials. In modeling studies,
	this form of synaptic modification, which we call spike-timing-dependent
	plasticity (STDP), automatically adjusts synaptic strengths so that
	the postsynaptic neuron becomes more sensitive to presynaptic spike
	timing. It has been argued that neurons in vivo operate in such a
	balanced or irregular ring mode, and STDP may thus explain how the
	required level of excitation arises and is maintained. Despite being
	synapse specific, STDP generates competition between different synapses
	because they compete for control of the timing of postsynaptic action
	potentials. By combining synaptic modification and competition, STPD
	can serve as a mechanism for competitive Hebbian learning that does
	not require further assumptions or constraints on synaptic efficacies.
	Acting as a competitive Hebbian mechanism of synaptic plasticity,
	STDP is not sensitive to overall firing rates or levels of input
	variability, but strengthens groups of synapses that are correlated
	over short time periods.},
  citeseerurl = {citeseer.nj.nec.com/song00competitive.html},
  content = {The integral of the synaptic modification function must be negative.
	Thus random spikes weaken the synapse. -----STDP tends to push the
	strength of a given synapse either to zero or to a maximum value.
	Intermediate values are unstable. -----STDP automatically establishes
	and maintains a state of balance of excitatory and inhibitory synaptic
	currents, where individual presynaptic action potentials are able
	to significantly affect the timing of postsynaptic spikes. -----STDP
	cannot strengthen synapses in the absence of postsynaptic firing.
	-----It would seem highly advantageous for the learning window sizes
	to be variable between different brain regions, to be modified during
	different stages of development, and perhaps to ve dynamically adjustable
	over shorter time scales as well, to stay compatible with the relevant
	input correlation and postsynaptic integration times.},
  file = { Verzeichi: Verzeichi:PDF},
  groupsearch = {0}
}

@ARTICLE{Song2000,
  author = {Song, Sen and Miller, Kenneth D and Abbott, Larry F},
  title = {Competitive Hebbian learning through spike-timing-dependent synaptic
	plasticity},
  journal = {Nature Neuroscience},
  year = {2000},
  volume = {3},
  pages = {919--926},
  number = {9},
  owner = {simon},
  publisher = {Nature Publishing Group},
  timestamp = {2013.04.25}
}

@ARTICLE{song05nonrandom,
  author = {S. Song and P. J. Sjöström and M. Reigl and S. Nelson and D. B. Chklovskii},
  title = {Highly nonrandom features of synaptic connectivity in cortical circuits},
  journal = {PLOS Biology},
  year = {2005},
  volume = {3},
  pages = {517--519},
  number = {3}
}

@INPROCEEDINGS{spears_ep94subpopulations,
  author = {Spears, W. M.},
  title = {Simple Subpopulation Schemes},
  booktitle = {Proceedings of the Third Annual Conference on Evolutionary Programming},
  year = {1994},
  editor = {Sebald, A. V. and L. J. Fogel},
  pages = {296--307},
  publisher = {World Scientific}
}

@INPROCEEDINGS{spears_icga91uniform,
  author = {Spears, W. M. and De Jong, K. A.},
  title = {On the Virtues of Parametrized Uniform Crossover},
  booktitle = {Proceedings of the 4th International Conference on Genetic Algorithms},
  year = {1991},
  editor = {Belew, R. K. and booker, L. K.},
  pages = {230--236},
  address = {San Diego, CA},
  publisher = {Morgan Kaufmann},
  file = {spears_icga91uniform.pdf:spears_icga91uniform.pdf:PDF}
}

@INPROCEEDINGS{spears99dining,
  author = {Spears, W. M. and DeJong, K.},
  title = {Dining with{GA}s: {O}perator lunch theorems},
  booktitle = {Foundations of Genetic Algorithms},
  year = {1999},
  editor = {Banzhaf, W. and Reeves, C.},
  volume = {5},
  pages = {85--101},
  file = {spears99dining.pdf:spears99dining.pdf:PDF}
}

@INPROCEEDINGS{spilger2020hxtorch,
	author = {Spilger, Philipp and M{\"u}ller, Eric and Emmel, Arne and Leibfried, Aron and Mauch, Christian and Pehle, Christian and Weis, Johannes and Breitwieser, Oliver and Billaudelle, Sebastian and Schmitt, Sebastian and Wunderlich, Timo C. and Stradmann, Yannik and Schemmel, Johannes},
	title = {hxtorch: {PyTorch} for {BrainScaleS-2} --- Perceptrons on Analog Neuromorphic Hardware},
	year = 2020,
	booktitle = {IoT Streams for Data-Driven Predictive Maintenance and IoT, Edge, and Mobile for Embedded Machine Learning},
	publisher={Springer International Publishing},
	address={Cham},
	pages={189--200},
	abstract={We present software facilitating the usage of the BrainScaleS-2 analog neuromorphic hardware system as an inference accelerator for artificial neural networks. The hardware is transparently integrated into the PyTorch machine learning framework. In particular, we support vector-matrix multiplications and convolutions; corresponding software-based autograd functionality is provided for hardware-in-the-loop training. The software provides support for automatic partitioning and scheduling of neural networks onto one or multiple chips. We discuss the implementation including optimizations, analyze runtime overhead, measure performance and evaluate the results in terms of the hardware design limitations. As an application of the introduced framework, we present a model that classifies activities of daily living with smartphone sensor data.},
	isbn={978-3-030-66770-2},
	doi={10.1007/978-3-030-66770-2_14}
}

@MASTERSTHESIS{spilger2021from,
	author   = {Philipp Spilger},
	title    = {From Neural Network Descriptions to Neuromorphic Hardware --- A Signal-Flow Graph Compiler Approach},
	school   = {Universit{\"a}t Heidelberg},
	year     = 2021,
	month    = feb,
	type     = {Master's thesis}
}

@MISC{spicehomepage,
  author = {SPICE},
  title = {Website},
  howpublished = {http://bwrc.eecs.berkeley.edu/classes/icbook/spice/},
  year = {2012},
  timestamp = {2012.08.11}
}

@MISC{rrobinpage,
	author = {Krill, Benjamin},
	title = {Round-robin arbiter},
	howpublished = {http://www.krll.de/en/portfolio/round-robin-arbiter/}
}

@inproceedings{srivastava2012multimodal,
  title={Multimodal Learning with Deep Boltzmann Machines.},
  author={Srivastava, Nitish and Salakhutdinov, Ruslan},
  booktitle={NIPS},
  pages={2231--2239},
  year={2012}
}

@ARTICLE{srowig07,
  author = {Andr\'{e} Srowig and Jan-Peter Loock and Karlheinz Meier and Johannes
	Schemmel and Holger Eisenreich and Georg Ellguth and Ren\'{e} Sch\"{u}ffny},
  title = {Analog Floating Gate Memory in a 0.18 $\mu$m Single-Poly {CMOS} Process},
  journal = {FACETS internal documentation},
  year = {2007}
}

@ARTICLE{staba02,
  author = {Staba, R.J. and Wilson, C.L. and Fried, I. and Jr., J. Enge},
  title = {Single Neuron Burst Firing in the Human Hippocampus During Sleep},
  journal = {Hippocampus},
  year = {2002},
  volume = {12},
  pages = {724-734},
  key = {staba_2002},
  keywords = {spiking}
}

@ARTICLE{Staddon2008,
  author = {Staddon, J. E. R. and Niv, Y.},
  title = {Operant conditioning},
  journal = {Scholarpedia},
  year = {2008},
  volume = {3},
  pages = {2318},
  number = {9},
  owner = {simon},
  timestamp = {2013.05.03},
  url = {http://www.scholarpedia.org/article/Operant_conditioning}
}

@MANUAL{Stallman,
  title = {Using the {GNU} Compiler Collection},
  author = {Richard Stallman},
  organization = {Free Software Foundation},
  address = {51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA},
  edition = {For gcc version 4.5.4},
  year = {2012},
  owner = {simon},
  timestamp = {2012.11.04},
  url = {http://gcc.gnu.org}
}

@book{stallman2011debugging,
  title={Debugging with GDB},
  author={Stallman, Richard and Pesch, Roland and Shebs, Stan and others},
  publisher={Free Software Foundation},
  year=2011,
	ISBN={978-0-9831592-3-0}
}

@INPROCEEDINGS{stanley_gecco02coevolution,
  author = {Stanley, K. O. and Miikkulainen, R.},
  title = {Continual coevolution through complexification},
  booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference
	{GECCO} 2002},
  year = {2002},
  editor = {Langdon, W.B. and others},
  pages = {113--120},
  month = {July},
  publisher = {Morgan Kaufmann Publishers}
}

@INPROCEEDINGS{stanley_gecco02efficient,
  author = {Stanley, K. O. and Miikkulainen, R.},
  title = {Efficient reinforcement learning through evolving neural network
	topologies},
  booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference
	{GECCO} 2002},
  year = {2002},
  editor = {Langdon, W.B. and others},
  pages = {569--577},
  month = {July},
  publisher = {Morgan Kaufmann Publishers}
}

@ARTICLE{steil07adaptation,
  author = {Steil, Jochen J.},
  title = {Online reservoir adaptation by intrinsic plasticity for backpropagation-decorrelation
	and echo state learning},
  journal = {Neural Networks},
  year = {2007},
  volume = {20},
  pages = {353--364},
  number = {3},
  month = apr,
  abstract = {We propose to use a biologically motivated learning rule based on
	neural intrinsic plasticity to optimize reservoirs of analog neurons.
	This rule is based on an information maximization principle, it is
	local in time and space and thus computationally efficient. We show
	experimentally that it can drive the neurons' output activities to
	approximate exponential distributions. Thereby it implements sparse
	codes in the reservoir. Because of its incremental nature, the intrinsic
	plasticity learning is well suited for joint application with the
	online backpropagation-decorrelation or the least mean squares reservoir
	learning, whose performance can be strongly improved. We further
	show that classical echo state regression can also benefit from reservoirs,
	which are pre-trained on the given input signal with the implicit
	plasticity rule.},
  booktitle = {Echo State Networks and Liquid State Machines},
  keywords = {Online learning, Reservoir adaptation, Time series prediction, Intrinsic
	plasticity, liquid},
  owner = {mreuss},
  timestamp = {2007.06.14},
  url = {http://www.sciencedirect.com/science/article/B6T08-4NMWR88-5/2/4073a5d5ecc6807465238b2cfb62d1c2}
}

@article{steimer2009belief,
  title={Belief propagation in networks of spiking neurons},
  author={Steimer, Andreas and Maass, Wolfgang and Douglas, Rodney},
  journal={Neural Computation},
  volume={21},
  number={9},
  pages={2502--2523},
  year={2009},
  publisher={MIT Press}
}

@ARTICLE{stein67,
  author = {Stein, R.},
  title = {{Some Models of Neuronal Variability}},
  journal = {Biophysical Journal},
  year = {1967},
  volume = {7},
  pages = {37--68},
  number = {1},
  month = jan,
  abstract = {{ The pattern of nerve action potentials produced by unit permeability
	changes (quantal inputs) occurring at random is considered analytically
	and by computer simulation methods. The important parameters of a
	quantal input are size and duration. Varying both the mean and the
	probability density function of these parameters has calculable effects
	on the distribution of interspike intervals. Particular attention
	is paid to the relation between the mean rate of excitatory inputs
	and the mean frequency of nerve action potentials (input-output curve)
	and the relation between the coefficient of variation for the interval
	distribution and the mean interval (variability curve). In the absence
	of action potentials one can determine the parameters of the voltage
	distribution including the autocorrelation function and the power
	spectrum. These parameters can sometimes be used to approximate the
	variability of interspike intervals as a function of the threshold
	voltage. Different neuronal models are considered including one containing
	the Hodgkin-Huxley membrane equations. The negative feedback inherent
	in the Hodgkin-Huxley equations tends to produce a small negative
	serial correlation between successive intervals. The results are
	discussed in relation to the interpretation of experimental results.
	}},
  citeulike-article-id = {8685942},
  citeulike-linkout-0 = {http://dx.doi.org/10.1016/S0006-3495(67)86574-3},
  doi = {10.1016/S0006-3495(67)86574-3},
  issn = {00063495},
  keywords = {integrate-and-fire, mean-field, ornstein-uhlenbeck\_process, spiking\_neuron},
  posted-at = {2011-01-24 12:06:20},
  priority = {2},
  url = {http://dx.doi.org/10.1016/S0006-3495(67)86574-3}
}

@ARTICLE{stent73physmech,
  author = {Stent, G.S.},
  title = {A physiological mechanism for Hebb's postulate of learning},
  journal = {Proceedings of the National Academy of Sciences, USA},
  year = {1973},
  volume = {70},
  pages = {997--1001}
}

@ARTICLE{stepanyants09fractions,
  author = {Armen Stepanyants and L Martinez and Alex S Ferecsk{\'o} and Zolt{\'a}n
	F Kisv{\'a}rday},
  title = {The fractions of short- and long-range connections in the visual
	cortex},
  journal = {Proc Natl Acad Sci USA},
  year = {2009},
  month = {Feb},
  abstract = {When analyzing synaptic connectivity in a brain tissue slice, it is
	difficult to discern between synapses made by local neurons and those
	arising from long-range axonal projections. We analyzed a data set
	of excitatory neurons and inhibitory basket cells reconstructed from
	cat primary visual cortex in an attempt to provide a quantitative
	answer to the question: What fraction of cortical synapses is local,
	and what fraction is mediated by long-range projections? We found
	an unexpectedly high proportion of nonlocal synapses. For example,
	92% of excitatory synapses near the axis of a 200-mum-diameter iso-orientation
	column come from neurons located outside the column, and this fraction
	remains high-76%-even for an 800-mum ocular dominance column. The
	long-range nature of connectivity has dramatic implications for experiments
	in cortical tissue slices. Our estimate indicates that in a 300-mum-thick
	section cut perpendicularly to the cortical surface, the number of
	viable excitatory synapses is reduced to about 10%, and the number
	of synapses made by inhibitory basket cell axons is reduced to 38%.
	This uneven reduction in the numbers of excitatory and inhibitory
	synapses changes the excitation-inhibition balance by a factor of
	3.8 toward inhibition, and may result in cortical tissue that is
	less excitable than in vivo. We found that electrophysiological studies
	conducted in tissue sections may significantly underestimate the
	extent of cortical connectivity; for example, for some projections,
	the reported probabilities of finding connected nearby neuron pairs
	in slices could understate the in vivo probabilities by a factor
	of 3.},
  affiliation = {Department of Physics and Center for Interdisciplinary Research on
	Complex Systems, Northeastern University, Boston, MA 02115;}
}

@BOOK{steriade2001intact,
  title = {The Intact and Sliced Brain},
  publisher = {MIT Press},
  year = {2001},
  author = {Steriade, Mircea}
}

@BOOK{steriade1990brainstem,
  title = {Brainstem Control of Wakefulness and Sleep},
  publisher = {Plenum Press, New York},
  year = {1990},
  author = {Steriade, M. and McCarley, R.W.}
}

@ARTICLE{steriade2001natural,
  author = {Steriade, M. and Timofeev, I. and Grenier, F.},
  title = {Natural Waking and Sleep States: A View From Inside Neocortical Neurons},
  journal = {J Neurophysiol},
  year = {2001},
  volume = {85},
  pages = {1969 -- 1985}
}

@mastersthesis{sterzenbach2014auxpwr,
  author = {Lars Sterzenbach},
  title = {Entwicklung einer selbstüberwachenden Spannungsversorgungfür ein auf Wafer-Ebene
	integriertes neuromorphes Hardware-System},
  school = {Hochschule Mannheim University of Applied Sciences},
  year = 2014,
  type = {Bachelor thesis (German)}
}

@BOOK{stevens2003,
  title = {UNIX Network Programming, Vol. 1},
  publisher = {Pearson Education},
  year = {2003},
  author = {Stevens, W. Richard and Fenner, Bill and Rudoff, Andrew M.},
  isbn = {0131411551}
}

@INPROCEEDINGS{stoica_eh01,
  author = {Stoica, Adrian and Keymeulen, Didier and Zebulum, Ricardo Salem},
  title = {Evolvable Hardware Solutions for Extreme Temperature Electronics},
  booktitle = {Proc.\ of the Third NASA/DOD Workshop on Evolvable Hardware},
  year = {2001},
  pages = {93--97},
  address = {Long Beach, CA, USA},
  month = Jul,
  publisher = {IEEE Computer Society Press},
  annote = {AND, evolution in SPICE, Test also in reality -> good 27 deg performance!},
  groupsearch = {0},
  keywords = {eh}
}

@INPROCEEDINGS{stoica_eh00,
  author = {Stoica, Adrian and Keymeulen, Didier and Zebulum, Ricardo Salem and
	Thakoor, Anil and Daud, T.\ and Klimeck, Gerhard and Jin, Yili and
	Tawel, R.\ and Duong, Vu},
  title = {Evolution of Analog Circuits on Field Programmable Transistor Arrays},
  booktitle = {Proc.\ of the Second NASA/DOD Workshop on Evolvable Hardware},
  year = {2000},
  pages = {99--108},
  address = {Palo Alto, CA, USA},
  month = Jul,
  publisher = {IEEE Computer Society Press},
  annote = {T-NORM + Gauss (V-V,Simulation only)},
  groupsearch = {0},
  keywords = {eh}
}

@INPROCEEDINGS{stoica_cec99,
  author = {Stoica, Adrian and Klimeck, Gerhard and Salazar-Lazaro, Carlos and
	Keymeulen, Didier and Thakoor, Anil},
  title = {Evolutionary Design of Electronic Devices and Circuits},
  booktitle = {Proc.\ of the Congress on Evolutionary Computation (CEC-1999)},
  year = {1999},
  editor = {Fogel, D.\ and Schoenauer, M.\ u},
  pages = {1271--1278},
  address = {Washington DC, USA},
  month = Jul,
  publisher = {IEEE Press},
  annote = {Gauss, IV-Curve, simulation (cf. ASTNASA99)},
  groupsearch = {0},
  keywords = {eh}
}

@INPROCEEDINGS{stoica_ices01,
  author = {Stoica, Adrian and Zebulum, Ricardo Salem and Keymeulen, Didier},
  title = {Polymorphic Electronics},
  booktitle = {Proc.\ 4th Int.\ Conf.\ on Evolvable Systems: From Biology to Hardware
	(ICES2001)},
  year = {2001},
  editor = {Liu, Yong and Kiyoshi, Tanaka and Masaya, Iwata and Higuchi, Tetsuya
	and Yasunaga, Moritoshi},
  pages = {291--302},
  address = {Tokio, Japan},
  month = Oct,
  publisher = {Springer Verlag},
  annote = {AND, OR, XOR},
  groupsearch = {0},
  keywords = {eh}
}

@INPROCEEDINGS{stoica_ices00,
  author = {Stoica, Adrian and Zebulum, Ricardo Salem and Keymeulen, Didier},
  title = {Mixtrinsic Evolution},
  booktitle = {Proc.\ 3rd Int.\ Conf.\ on Evolvable Systems: From Biology to Hardware
	(ICES2000)},
  year = {2001},
  editor = {Miller, Julian and Thompson, Adrian and Thomson, Peter and Fogarty,
	Terence C.\ },
  pages = {208--217},
  address = {Edinburgh, Scotland, UK},
  month = Apr,
  publisher = {Springer Verlag},
  annote = {Mixtrinsic eben !!!},
  groupsearch = {0},
  keywords = {eh}
}

@INPROCEEDINGS{stoica_eh02,
  author = {Adrian Stoica and Ricardo S. Zebumul and M.I.Ferguson and Didier
	Keymeulen and Vu Dong},
  title = {{E}volving {C}ircuits in {S}econds: {E}xperiments with a {S}tand-{A}lone
	{B}oard-{L}evel {E}volvable {S}ystem},
  booktitle = {Proceedings of the 2002 {NASA}/{DoD} Conference an Evolvable Hardware},
  year = {2002},
  editor = {Stoica, Adrian et.al.},
  pages = {67-74},
  month = {July},
  groupsearch = {0},
  keywords = {eh}
}

@BOOK{storch01evolutionsbiologie,
  title = {Evolutionsbiologie},
  publisher = {Springer Verlag},
  year = {2001},
  author = {Storch, Volker and Welsch, Ulrich and Wink, Michael},
  address = {Berlin, Heidelberg, New York},
  isbn = {3-540-41880-6}
}

@MASTERSTHESIS{stradmann2016bachelor,
  author = "Yannik Stradmann",
  title = "Characterization and Calibration of a Mixed-Signal Leaky
           Integrate and Fire Neuron on {HICANN-DLS}",
  year = 2016,
  type = {Bachelor thesis},
  school = {Ruprecht-Karls-Universit{\"a}t Heidelberg}
}

@mastersthesis{stradmann2019msc,
  author   = {Yannik Stradmann},
  title    = {Verification and Commissioning of Mixed-Signal Neuromorphic Substrates},
  school   = {Ruprecht-Karls-Universit{\"a}t Heidelberg},
  year     = {2019},
  type     = {Master's Thesis}
}

@ARTICLE{stradmann2021demonstrating, crossref={stradmann2022demonstrating}}
@ARTICLE{stradmann2022demonstrating,
  title = {Demonstrating Analog Inference on the {BrainScaleS-2} Mobile System},
  author = {Stradmann, Yannik and Billaudelle, Sebastian and Breitwieser, Oliver and Ebert, Falk Leonard and Emmel, Arne and Husmann, Dan and Ilmberger, Joscha and M{\"u}ller, Eric and Spilger, Philipp and Weis, Johannes and Schemmel, Johannes},
  journal = {{IEEE} Open Journal of Circuits and Systems},
  year = 2022,
  volume = {3},
  pages = {252--262},
  doi = {10.1109/OJCAS.2022.3208413}
}

@BOOK{stroustrup00c++,
  title = {The C++ Programming Language},
  publisher = {Addison Wesley Longman},
  year = {2000},
  author = {Stroustrup, Bjarne},
  address = {Amsterdam},
  month = {February},
  isbn = {0201700735}
}

@BOOK{stroustrup97c++,
  title = {The C++ Programming Language},
  publisher = {Addison Wesley},
  year = {1997},
  author = {Stroustrup, Bjarne},
  address = {Reading, MA},
  month = {August},
  isbn = {0-201-88954-4}
}

@ARTICLE{strukov2008missing,
  author = {Strukov, Dmitri B and Snider, Gregory S and Stewart, Duncan R and
	Williams, R Stanley},
  title = {The missing memristor found},
  journal = {Nature},
  year = {2008},
  volume = {453},
  pages = {80--83},
  number = {7191},
  owner = {simon},
  publisher = {Nature Publishing Group},
  timestamp = {2013.04.19}
}

@BOOK{Stryer2002,
  title = {Biochemistry (5th ed.)},
  publisher = {W.H. Freeman and Company},
  year = {2002},
  author = {Lubert Stryer},
  address = {New York, USA},
  isbn = {0-7167-1843-X}
}

@article{sugawara1999physiology,
  title={Physiology of electrosensory lateral line lobe neurons in Gnathonemus petersii},
  author={Sugawara, YOSHIKO and Grant, KIRSTY and Han, VICTOR and Bell, CURTIS C},
  journal={Journal of experimental biology},
  volume={202},
  number={10},
  pages={1301--1309},
  year={1999},
  publisher={The Company of Biologists Ltd}
}

@MANUAL{sumatra_homepage,
  key = {Sumatra},
  organization = {The NeuralEnsemble Initiative},
  title = {An automated electronic lab notebook for Python},
  url = {http://www.neuralensemble.org/sumatra},
  year = 2014,
  note = {[Online; accessed: 2014-04-29]}
}

@BOOK{summerfield_pyqt2008,
  title = {Rapid {GUI} Programming with {Python} and {Qt}},
  publisher = {Prentice Hall},
  year = {2008},
  author = {Summerfield, Mark},
  isbn = {0132354187},
  keywords = {Python, Qt4, Softwareentwicklung}
}

@MISC{sunat_03phdthesis,
  author = {Sunat, Khamron},
  title = {Principles of convergent rate and generalization enhancement for
	feedforward sigmoid-like network},
  howpublished = {{\it PhD thesis}},
  year = {2003},
  institution = {Chulalongkorn University, Bangkok},
  number = {ISBN 974-17-5120-6}
}

@ARTICLE{sussillo07selftuning,
  author = {Sussillo, David and Toyoizumi, Taro and Maass, Wolfgang},
  title = {{Self-Tuning of Neural Circuits Through Short-Term Synaptic Plasticity}},
  journal = {J Neurophysiol},
  year = {2007},
  volume = {97},
  pages = {4079-4095},
  number = {6},
  abstract = {Numerous experimental data show that cortical networks of neurons
	are not silent in the absence of external inputs, but rather maintain
	a low spontaneous firing activity. This aspect of cortical networks
	is likely to be important for their computational function, but is
	hard to reproduce in models of cortical circuits of neurons because
	the low-activity regime is inherently unstable. Here we show--through
	theoretical analysis and extensive computer simulations--that short-term
	synaptic plasticity endows models of cortical circuits with a remarkable
	stability in the low-activity regime. This short-term plasticity
	works as a homeostatic mechanism that stabilizes the overall activity
	level in spite of drastic changes in external inputs and internal
	circuit properties, while preserving reliable transient responses
	to signals. The contribution of synaptic dynamics to this stability
	can be predicted on the basis of general principles from control
	theory.},
  doi = {10.1152/jn.01357.2006},
  eprint = {http://jn.physiology.org/cgi/reprint/97/6/4079.pdf}
}

@inproceedings{sutskever2010convergence,
  title={On the convergence properties of contrastive divergence},
  author={Sutskever, Ilya and Tieleman, Tijmen},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={789--795},
  year={2010}
}

@BOOK{sutton1998reinforcement,
  title = {Reinforcement learning: An introduction},
  publisher = {Cambridge Univ Press},
  year = {1998},
  author = {Sutton, R.S. and Barto, A.G.},
  volume = {1},
  number = {1}
}

@MISC{synopsis10dwram,
  author = {{Synopsis, Inc.}},
  title = {Write-Port, Dual-Read-Port RAM (Latch-Based),DW\_ram\_2r\_w\_a\_lat},
  howpublished = {DesignWare Building Block IP},
  year = {2002}
}

@MISC{designcompiler,
  author = {Synopsys, Inc.},
  title = {Design Compiler},
  howpublished = {www.synopsys.com},
  year = {2012},
  owner = {simon},
  timestamp = {2012.12.14}
}

@MANUAL{systemverilog,
  title = {SystemVerilog 3.1a Language Reference Manual},
  author = {{SystemVerilog}},
  organization = {Accellera},
  year = {2004}
}

@INPROCEEDINGS{syswerda_icga89uniform,
  author = {Syswerda, G.},
  title = {Uniform Crossover in Genetic Algorithms},
  booktitle = {Proceedings of the Third International Conference on Genetic Algorithms},
  year = {1989},
  editor = {Schaffer, J. D.},
  pages = {2--9},
  address = {San Francisco},
  publisher = {Morgan Kaufmann}
}

@MISC{scipyhomepage,
  author = {{S}ci{P}y},
  title = {Website},
  howpublished = {\url{http://www.scipy.org/}},
  year = {2012},
  owner = {mueller},
  timestamp = {2008.08.06}
}

@MISC{weavehomepage,
  author = {{S}ci{P}y},
  title = {{W}eave},
  howpublished = {\url{http://www.scipy.org/Weave}},
  year = {2008},
  owner = {mueller},
  timestamp = {2008.08.06}
}

@MISC{simtooldb_homepage,
  author = {{S}im{T}ool{DB}},
  title = {Website},
  howpublished = {\url{http://senselab.med.yale.edu/SimToolDB}},
  year = {2008}
}

@ARTICLE{saeckinger1992anna,
  author = {E. Säckinger and B. E. Boser and J. Bromley and Y. LeCun and L. Jackel},
  title = {Application of the ANNA neural network chip to high-speed character
	recognition},
  journal = {IEEE Transactions on Neural Networks},
  year = {1992},
  volume = {3},
  pages = {498--505},
  number = {3},
  file = {saeckinger1992anna.pdf:saeckinger1992anna.pdf:PDF},
  keywords = {vlsi, convolutional NN},
  owner = {fieres}
}

@ARTICLE{tanaka07cmosstdp,
  author = {Tanaka, Hideki and Morie, Takashi and Aihara, Kazuyuki},
  title = {A CMOS circuit for STDP with a symmetric time window},
  journal = {International Congress Series},
  year = {2007},
  volume = {1301},
  pages = {152--155},
  month = jul,
  abstract = {In some spiking neuron models, analog information is expressed by
	the timing of neuronal spike firing events, and synaptic weights
	change depending on the relative timing between asynchronous spikes,
	which is called spike-timing dependent synaptic plasticity (STDP).
	In this paper, we propose an analog CMOS circuit for STDP with a
	symmetric time window and a Hopfield-type VLSI neural network using
	the synapse circuit. We have confirmed by circuit simulations that
	the network can perform autocorrelation learning from input spike
	patterns.},
  booktitle = {Brain-Inspired IT III. Invited and selected papers of the 3rd International
	Conference on Brain-Inspired Information Technology "BrainIT 2006"
	held in Hibikino, Kitakyushu, Japan between 27 and 29 September 2006},
  keywords = {Spiking neuron, STDP, Neural network, LSI implementation},
  owner = {mreuss},
  timestamp = {2007.06.14},
  url = {http://www.sciencedirect.com/science/article/B7581-4NYJD61-19/2/7d56260b40766382a83ef2e3c4cf0793}
}

@ARTICLE{tanaka96inferotemporal,
  author = {K. Tanaka},
  title = {Inferotemporal cortex and object vision},
  journal = {Ann. Rev. Neuroscience},
  year = {1996},
  volume = {19},
  pages = {109--139},
  owner = {fieres}
}

@INPROCEEDINGS{tanese_icga87parallel,
  author = {Tanese, R.},
  title = {Parallel Genetic Algorithm for a Hypercube},
  booktitle = {Proceedings of the 2nd International Conference on Genetic Algorithms},
  year = {1987},
  editor = {J. J. Grefenstette},
  pages = {177--183},
  address = {Hillsdale, New Jersey},
  publisher = {Lawrence Erlbaum}
}

@ARTICLE{tao04egalitarian,
  author = {L. Tao and M. Shelley and D. McLaughlin and R. Shapley},
  title = {An egalitarian network model for the emergence of simple and complex
	cells in visual cortex},
  journal = {PNAS},
  year = {2004},
  volume = {101},
  pages = {366--371},
  keywords = {learning},
  owner = {fieres}
}

@INPROCEEDINGS{taylor97historical,
  author = {Taylor, J. G.},
  title = {The Historical Background},
  booktitle = {The Handbook of Neural Computation},
  year = {1997},
  editor = {Fiesler, E. and Beale, R.},
  address = {New York},
  month = {January},
  publisher = {Institute of Physics Publishing and Oxford University Publishing},
  chapter = {A1.1}
}

@techreport{tensorflow2015,
  title = {{TensorFlow}: Large-Scale Machine Learning on Heterogeneous Distributed Systems},
  type = {Whitepaper},
  author  = {Martín Abadi and Ashish Agarwal and Paul Barham and Eugene Brevdo and Zhifeng Chen and Craig Citro and Greg Corrado and Andy Davis and Jeffrey Dean and Matthieu Devin and Sanjay Ghemawat and Ian Goodfellow and Andrew Harp and Geoffrey Irving and Michael Isard and Yangqing Jia and Rafal Jozefowicz and Lukasz Kaiser and Manjunath Kudlur and Josh Levenberg and Dan Mané and Rajat Monga and Sherry Moore and Derek Murray and Chris Olah and Mike Schuster and Jonathon Shlens and Benoit Steiner and Ilya Sutskever and Kunal Talwar and Paul Tucker and Vincent Vanhoucke and Vijay Vasudevan and Fernanda Viégas and Oriol Vinyals and Pete Warden and Martin Wattenberg and Martin Wicke and Yuan Yu and Xiaoqiang Zheng},
  year  = 2015,
  URL = {http://download.tensorflow.org/paper/whitepaper2015.pdf}
}

@misc{tensorflow2015_nourl,
  title = {{TensorFlow}: Large-Scale Machine Learning on Heterogeneous Distributed Systems},
  author  = {Martín Abadi and Ashish Agarwal and Paul Barham and Eugene Brevdo and Zhifeng Chen and Craig Citro and Greg Corrado and Andy Davis and Jeffrey Dean and Matthieu Devin and Sanjay Ghemawat and Ian Goodfellow and Andrew Harp and Geoffrey Irving and Michael Isard and Yangqing Jia and Rafal Jozefowicz and Lukasz Kaiser and Manjunath Kudlur and Josh Levenberg and Dan Mané and Rajat Monga and Sherry Moore and Derek Murray and Chris Olah and Mike Schuster and Jonathon Shlens and Benoit Steiner and Ilya Sutskever and Kunal Talwar and Paul Tucker and Vincent Vanhoucke and Vijay Vasudevan and Fernanda Viégas and Oriol Vinyals and Pete Warden and Martin Wattenberg and Martin Wicke and Yuan Yu and Xiaoqiang Zheng},
  year  = 2015
}

@ARTICLE{tegner02adaptive,
  author = {Tegner, Jesper and Kepecs, Adam},
  title = {An adaptive spike-timing-dependent plasticity rule},
  journal = {Neurocomputing},
  year = {2002},
  volume = {44-46},
  pages = {189--194},
  month = jun,
  abstract = {We examine the conditions under which spike-timing-dependent plasticity
	(STDP) normalizes post-synaptic firing rates. Our simulations show
	that the rate normalization property of STDP is fragile and small
	changes in the LTD/LTP ratio or pre-synaptic input rates can lead
	to high firing rates. We propose an adaptive scheme to dynamically
	control the LTP/LTD ratio. The biophysics of synapses lead us to
	suggest a control mechanism using action potential-induced calcium
	influx, a known mediator of synaptic plasticity. This adaptive STDP
	rule is shown to stabilize the post-synaptic firing rates under a
	variety of perturbations.},
  keywords = {Learning, Homeostatic mechanism, Hebbian, Spike correlations},
  owner = {mreuss},
  timestamp = {2007.06.14},
  url = {http://www.sciencedirect.com/science/article/B6V10-45MWNFW-5/2/ee334959a169eaa778d8027eea9d9de1}
}

@MISC{tegn-why,
  author = {Jesper Tegn{\'e}r and {\'A}d{\'a}m Kepecs},
  title = {Why neuronal dynamics should control synaptic learning rules},
  abstract = {... additive STDP is remarkably versatile but also very fragile, wheras
	multiplicative STDP is more robust but lacks attractive features
	such as synaptic competition and rate stabilization. Here we address
	the problem of robustness in the additive STDP rule. [...] We show
	that this adaptive rule makes the additive STDP more robust. Finally,
	we give an example how meta plasticity of the adaptive rule can be
	used to guide STDP into different type of learning regimes.},
  citeseerurl = {citeseer.nj.nec.com/542266.html},
  keywords = {learning}
}

@INPROCEEDINGS{teichert_ijcnn03association,
  author = {Teichert, J. and Malaka, R.},
  title = {An association architecture for the detection of objects with changing
	topologies},
  booktitle = {Proceedings of the International Joint Conference on Neural Networks
	{IJCNN}'03},
  year = {2003},
  pages = {125--130},
  month = {July},
  publisher = {IEEE Press},
  file = {teichert_ijcnn03association.pdf:teichert_ijcnn03association.pdf:PDF},
  keywords = {convolutional NN}
}

@ARTICLE{telfeian2003,
  author = {Albert E. Telfeian and Barry W. Connors},
  title = {Widely integrative properties of layer 5 pyramidal cells support
	a role for processing of extralaminar synaptic inputs in rat neocortex},
  journal = {Neuroscience Letters},
  year = {2003},
  volume = {343},
  pages = {121 - 124},
  number = {2},
  doi = {10.1016/S0304-3940(03)00379-3},
  issn = {0304-3940},
  url = {http://www.sciencedirect.com/science/article/pii/S0304394003003793}
}

@ARTICLE{terman95,
  author = {David Terman and DeLiang Wang},
  title = {Global competition and local cooperation in a network of neural oscillators},
  journal = {Phys. D},
  year = {1995},
  volume = {81},
  pages = {148--176},
  number = {1-2},
  address = {Amsterdam, The Netherlands, The Netherlands},
  doi = {http://dx.doi.org/10.1016/0167-2789(94)00205-5},
  issn = {0167-2789},
  publisher = {Elsevier Science Publishers B. V.}
}

@INPROCEEDINGS{tetzlaff2005,
  author = {Tom Tetzlaff and Ad Aertsen and Markus Diesmann},
  title = {Time-scale dependence of inter-neuronal spike correlations},
  booktitle = {Proceedings of the 2005 NWG Conference, G\"ottingen},
  year = {2005}
}

@INPROCEEDINGS{tetzlaff2005heterogeneity,
  author = {Tetzlaff, T. and Morrison, A. and Timme, M. and Diesmann, M.},
  title = {Heterogeneity breaks global synchrony in large networks},
  booktitle = {Proceedings of the 30th G{\"o}ttingen neurobiology conference},
  year = {2005},
  pages = {206B}
}

@article{tetzlaff2012decorrelation,
  title={Decorrelation of neural-network activity by inhibitory feedback},
  author={Tetzlaff, Tom and Helias, Moritz and Einevoll, Gaute T and Diesmann, Markus},
  journal={PLoS computational biology},
  volume={8},
  number={8},
  pages={e1002596},
  year={2012},
  publisher={Public Library of Science}
}

@MISC{TexasInstruments2010,
  author = {{Texas Instruments}},
  title = {MSP430F543x, MSP430F541x Mixed Signal Microcontroller},
  month = {MArch},
  year = {2010},
  note = {Revision C},
  owner = {simon},
  timestamp = {2013.04.15},
  url = {http://www.ti.com/lit/gpn/msp430f5438}
}

@manual{TexasInstruments2020INA219,
    author = {{Texas Instruments}},
    title = {{INA219} Zer{\o}-Drift, Bidirectional Current/Power Monitor With {I2C} Interface},
    year = 2020,
    url = {https://www.ti.com/lit/ds/symlink/ina219.pdf}
}

@ARTICLE{Thakkur1999,
  author = {Thakkur, S. and Huff, T.},
  title = {Internet Streaming {SIMD} Extensions},
  journal = {Computer},
  year = {1999},
  volume = {32},
  pages = {26 -34},
  number = {12},
  month = {dec},
  doi = {10.1109/2.809248},
  issn = {0018-9162},
  keywords = {32 bit;3D geometry;IA-32 architecture;ISSE;Intel 32-bit processors;Intel
	Pentium III microprocessor;Internet Streaming SIMD Extensions;data
	use;floating-point computation;graphics applications;performance;streaming
	data;video applications;visual computing;visually perceptible difference;volume
	PC platform;Internet;floating point arithmetic;microprocessor chips;microprogramming;parallel
	processing;solid modelling;}
}

@MISC{nestinitiative08homepage,
  author = {{The {NEST} Initiative -- Website}},
  title = {Website},
  howpublished = {\url{http://www.nest-initiative.org}},
  year = {2008}
}

@MISC{nestinitiative07homepage,
  author = {{The {NEST} Initiative -- Website}},
  title = {Website},
  howpublished = {\url{http://www.nest-initiative.org}},
  year = {2007}
}

@ARTICLE{thakur2018mimicthebrain,
AUTHOR={Thakur, Chetan Singh and Molin, Jamal Lottier and Cauwenberghs, Gert and Indiveri, Giacomo and Kumar, Kundan and Qiao, Ning and Schemmel, Johannes and Wang, Runchun and Chicca, Elisabetta and Olson Hasler, Jennifer and Seo, Jae-sun and Yu, Shimeng and Cao, Yu and van Schaik, André and Etienne-Cummings, Ralph},
TITLE={Large-Scale Neuromorphic Spiking Array Processors: A Quest to Mimic the Brain},
JOURNAL={Frontiers in Neuroscience},
VOLUME={12},
PAGES={891},
YEAR={2018},
URL={https://www.frontiersin.org/article/10.3389/fnins.2018.00891},
DOI={10.3389/fnins.2018.00891},
ISSN={1662-453X},
}

@INPROCEEDINGS{thierens93genetic,
  author = {Thierens, D. and Suykens, J. and Vandewalle, J. and DeMoor, B.},
  title = {Genetic weight optimization of a feedforward neural network controller},
  booktitle = {Proceedings of the Conference on Artificial Neural Nets and Genetic
	Algorithms},
  year = {1993},
  pages = {658--663},
  address = {Berlin, Heidelberg, New York},
  publisher = {Springer Verlag},
  file = {thierens93genetic.pdf:thierens93genetic.pdf:PDF}
}

@inproceedings{thommes2022demonstrating,
  title = {Demonstrating {BrainScaleS-2} Inter-Chip Pulse Communication using {EXTOLL}},
  author = {Tobias Thommes and Sven Bordukat and Andreas Gr{\"u}bl and Vitali Karasenko and Eric M{\"u}ller and Johannes Schemmel},
  booktitle = {Neuro-inspired Computational Elements Workshop (NICE ’22), March 29 -- April 1, 2022},
  location = {Virtual Event, USA},
  year = 2022,
  isbn = 9781450395595,
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3517343.3517376},
  pages = {98--100},
  numpages = 3,
  eprint = {2202.12122},
  archivePrefix = {arXiv},
  primaryClass = {cs.AR}
}

@INPROCEEDINGS{thompson_ices98,
  author = {Thompson, Adrian},
  title = {On the Automatic Design of Robust Electronics Through Artificial
	Evolution},
  booktitle = {Proc.\ 2nd Int.\ Conf.\ on Evolvable Systems: From Biology to Hardware
	(ICES98)},
  year = {1998},
  editor = {Sipper, Moshe and Mange, Daniel and P{\'e}rez-Uribe, Andr{\'e}s},
  pages = {13--24},
  address = {Lausanne, Switzerland},
  month = Sep,
  publisher = {Springer-Verlag},
  annote = {Robustness !},
  groupsearch = {0},
  keywords = {eh}
}

@ARTICLE{thompson_tec99,
  author = {Thompson, Adrian and Layzell, Paul and Zebulum, Ricardo Salem},
  title = {Explorations in Design Space: Unconventional Electronics Design Through
	Artificial Evolution},
  journal = {IEEE Transactions on Evolutionary Computation},
  year = {1999},
  volume = {3},
  pages = {167--196},
  month = Sep,
  annote = {Summary/Overview, Journal !},
  groupsearch = {0},
  keywords = {eh}
}

@BOOK{thompson85brain,
  title = {The Brain},
  publisher = {W. H. Freeman and Company},
  year = {1985},
  author = {Thompson, Richard F.},
  address = {New York and Oxford}
}

@article{thomson1994temporal,
  title={Temporal and spatial properties of local circuits in neocortex},
  author={Thomson, Alex M and Deuchars, Jim},
  journal={Trends in neurosciences},
  volume={17},
  number={3},
  pages={119--126},
  year={1994},
  publisher={Elsevier}
}

@ARTICLE{thomson97,
  author = {Thomson, A. M.},
  title = {Activity-dependent properties of synaptic transmission at two classes
	of connections made by rat neocortical pyramidal},
  journal = {J. Physiology},
  year = {1997},
  volume = {502},
  pages = {131--147}
}

@ARTICLE{thomson07functional,
  author = {Alex M Thomson and Christophe Lamy},
  title = {Functional maps of neocortical local circuitry},
  journal = {Frontiers in neuroscience},
  year = {2007},
  volume = {1},
  pages = {19--42},
  number = {1},
  month = {Nov},
  abstract = {This review aims to summarize data obtained with different techniques
	to provide a functional map of the local circuit connections made
	by neocortical neurones, a reference for those interested in cortical
	circuitry and the numerical information required by those wishing
	to model the circuit. A brief description of the main techniques
	used to study circuitry is followed by outline descriptions of the
	major classes of neocortical excitatory and inhibitory neurones and
	the connections that each layer makes with other cortical and subcortical
	regions. Maps summarizing the projection patterns of each class of
	neurone within the local circuit and tables of the properties of
	these local circuit connections are provided.This review relies primarily
	on anatomical studies that have identified the classes of neurones
	and their local and long distance connections and on paired intracellular
	and whole-cell recordings which have documented the properties of
	the connections between them. A large number of different types of
	synaptic connections have been described, but for some there are
	only a few published examples and for others the details that can
	only be obtained with paired recordings and dye-filling are lacking.
	A further complication is provided by the range of species, technical
	approaches and age groups used in these studies. Wherever possible
	the range of available data are summarised and compared. To fill
	some of the more obvious gaps for the less well-documented cases,
	data obtained with other methods are also summarized.},
  affiliation = {The Department of Pharmacology, The School of Pharmacy, University
	of London, London UK.}
}

@ARTICLE{thomson91excitatory,
  author = {Alex M. Thomson and Shahrzad Radpour},
  title = {Excitatory Connections Between CA1 Pyramidal Cells Revealed by Spike
	Triggered Averaging in Slices of Rat Hippocampus are Partially NMDA
	Receptor Mediated},
  journal = {European Journal of Neuroscience},
  year = {1991},
  volume = {3},
  pages = {587-601},
  number = {6},
  abstract = {Spike triggered averaging was used to record local circuit connections
	between pairs of CA1 pyramidal neurons in isolated slices of rat
	hippocampus. Of 795 pairs of neurons tested, six were connected.
	These epsps were only partially blocked by 2-amino-5-phosphonovalerate
	(AP-5), which decreased the amplitude and half width of the epsp,
	but did not affect the early rising phase. In contrast, 6-cyano-7-nitroquinoxaline-2,3-dione
	(CNQX) blocked all phases of the epsp and combinations of AP-5 and
	CNQX blocked the epsp almost entirely. These results indicate that
	these epsps were mediated by both N-methyl-d-aspartate (NMDA) and
	non-NMDA excitatory amino acid receptors. Moreover, they exhibited
	a voltage relation typical of neuronal responses to NMDA, increasing
	in amplitude and duration as the postsynaptic cell was depolarized.
	These epsps were brief (10201390% rise time &lt; 5 ms, width at half
	amplitude &lt; 20 ms), indicating a proximal location. Increasing
	presynaptic firing rate (120134 spikes/s) reduced average epsp amplitude
	by almost 50%. When epsps were evoked by pairs of spikes (interval
	3201325 ms), a large response to the first spike precluded a large
	response to the second. No evidence for selective enhancement of
	the NMDA receptor component by paired spike activation was found.
	It is concluded that a significant NMDA receptor mediated input to
	CA1 is provided by local circuit CA12013CA1 connectionsx and that
	these synapses can be demonstrated under control conditions.}
}

@ARTICLE{turrigiano98,
  author = {Turrigiano, G. G. and Leslie, K. R. and Desai, N. S. and Rutherford, L. C. and Nelson, S. B.},
  title = {Activity-dependent scaling of quantal amplitude in neocortical neurons},
  journal = {Nature},
  year = {1998},
  volume = {391},
  pages = {892-896}
}

@Article{thomas1975meanfpt,
  Author         = {Thomas, Marlin U},
  Title          = {Some mean first-passage time approximations for the
                   Ornstein-Uhlenbeck process},
  Journal        = {Journal of Applied Probability},
  Pages          = {600--604},
  publisher      = {JSTOR},
  year           = 1975
}

@ARTICLE{thomson02synaptic,
  author = {Thomson, A. M. and West, D. C. and Wang, Y. and Bannister, A.
	P.},
  title = {Synaptic connections and small circuits involving excitatory and
	inhibitory neurons in layers 2-5 of adult rat and cat neocortex:
	triple intracellular recordings and biocytin labelling in vitro},
  journal = {Cerebral Cortex},
  year = {2002},
  volume = {12},
  pages = {936-953}
}

@ARTICLE{thorpe01rapid,
  author = {Simon Thorpe and Arnaud Delorme and Rufin Van Rullen},
  title = {Spike-based strategies for rapid processing},
  journal = {Neural Networks},
  year = {2001},
  volume = {14},
  pages = {715-725},
  owner = {bruederl}
}

@INPROCEEDINGS{thorpe_bmcv02waves,
  author = {Thorpe, Simon J.},
  title = {Ultra-Rapid Scene Categorization with a Wave of Spikes},
  booktitle = {Proceedings of the 2nd International Workshop on Biologically Motivated
	Computer Vision},
  year = {2002},
  pages = {1--15}
}

@ARTICLE{thorpe96speed,
  author = {Thorpe, Simon J. and Fize, D. and Marlot, C.},
  title = {Speed of processing in the human visual system},
  journal = {Nature},
  year = {1996},
  volume = {381},
  pages = {520--522}
}

@inproceedings{tieleman2008training,
  title={Training restricted Boltzmann machines using approximations to the likelihood gradient},
  author={Tieleman, Tijmen},
  booktitle={Proceedings of the 25th international conference on Machine learning},
  pages={1064--1071},
  year={2008},
  organization={ACM}
}

@ARTICLE{timofeev2000origin,
  author = {I Timofeev and F Grenier and M Bazhenov and T J Sejnowski and Mircea
	Steriade},
  title = {Origin of slow cortical oscillations in deafferented cortical slabs},
  journal = {Cereb Cortex},
  year = {2000},
  volume = {10},
  pages = {1185--99},
  number = {12},
  month = {Dec},
  abstract = {An in vivo preparation has been developed to study the mechanisms
	underlying spontaneous sleep oscillations. Dual and triple simultaneous
	intracellular recordings were made from neurons in small isolated
	cortical slabs (10 mm x 6 mm) in anesthetized cats. Spontaneously
	occurring slow sleep oscillations, present in the adjacent intact
	cortex, were absent in small slabs. However, the isolated slabs displayed
	brief active periods separated by long periods of silence, up to
	60 s in duration. During these silent periods, 60% of neurons showed
	non-linear amplification of low-amplitude depolarizing activity.
	Nearly 40% of the cells, twice as many as in intact cortex, were
	classified as intrinsically bursting. In cortical network models
	based on Hodgkin-Huxley-like neurons, the summation of simulated
	spontaneous miniature excitatory postsynaptic potentials was sufficient
	to activate a persistent sodium current, initiating action potentials
	in single neurons that then spread through the network. Consistent
	with this model, enlarging the isolated cortical territory to an
	isolated gyrus (30 mm x 20 mm) increased the probability of initiating
	large-scale activity. In these larger territories, both the frequency
	and regularity of the slow oscillation approached that generated
	in intact cortex. The frequency of active periods in an analytical
	model of the cortical network accurately predicted the scaling observed
	in simulations and from recordings in cortical slabs of increasing
	size.},
  affiliation = {Laboratory of Neurophysiology, School of Medicine, Laval University,
	Quebec, Canada.}
}

@INBOOK{toledo2002neurontypes,
  pages = {719-725},
  title = {Neocortex: Basic neuron types},
  publisher = {The MIT Press,},
  year = {2002},
  editor = {Arbib, Michael A. },
  author = {Toledo-Rodriguez, Maria and Gupta, Anirudh and Wang, Yun and Wu,
	Cai Zhi and Markram, Henry},
  address = {Cambridge, MA},
  edition = {Second},
  booktitle = {The handbook of brain theory and neural networks}
}

@INPROCEEDINGS{tomas06design,
  author = {J. Tomas and Y. Bornat and S. Saighi and T. Levi and S. Renaud},
  title = {Design of a modular and mixed neuromimetic ASIC},
  booktitle = {Proceedings of the 13th {IEEE} International Conference on Electronics,
	Circuits and Systems},
  year = {2006},
  pages = {946-949}
}

@ARTICLE{tong07learning,
  author = {Tong, Matthew H. and Bickett, Adam D. and Christiansen, Eric M. and
	Cottrell, Garrison W.},
  title = {Learning grammatical structure with Echo State Networks},
  journal = {Neural Networks},
  year = {2007},
  volume = {20},
  pages = {424--432},
  number = {3},
  month = apr,
  abstract = {Echo State Networks (ESNs) have been shown to be effective for a number
	of tasks, including motor control, dynamic time series prediction,
	and memorizing musical sequences. However, their performance on natural
	language tasks has been largely unexplored until now. Simple Recurrent
	Networks (SRNs) have a long history in language modeling and show
	a striking similarity in architecture to ESNs. A comparison of SRNs
	and ESNs on a natural language task is therefore a natural choice
	for experimentation. Elman applies SRNs to a standard task in statistical
	NLP: predicting the next word in a corpus, given the previous words.
	Using a simple context-free grammar and an SRN with backpropagation
	through time (BPTT), Elman showed that the network was able to learn
	internal representations that were sensitive to linguistic processes
	that were useful for the prediction task. Here, using ESNs, we show
	that training such internal representations is unnecessary to achieve
	levels of performance comparable to SRNs. We also compare the processing
	capabilities of ESNs to bigrams and trigrams. Due to some unexpected
	regularities of Elman's grammar, these statistical techniques are
	capable of maintaining dependencies over greater distances than might
	be initially expected. However, we show that the memory of ESNs in
	this word-prediction task, although noisy, extends significantly
	beyond that of bigrams and trigrams, enabling ESNs to make good predictions
	of verb agreement at distances over which these methods operate at
	chance. Overall, our results indicate a surprising ability of ESNs
	to learn a grammar, suggesting that they form useful internal representations
	without learning them.},
  booktitle = {Echo State Networks and Liquid State Machines},
  keywords = {Echo state networks, Simple recurrent networks, Grammar learning,,
	liquid},
  owner = {mreuss},
  timestamp = {2007.06.14},
  url = {http://www.sciencedirect.com/science/article/B6T08-4NMWR88-7/2/6698655fd344d7da8bbe14c54583c99a}
}

@ARTICLE{torres06golgi,
  author = {Torres-Fern\'andez, O. and Golgi, C. and Ram\'on y Cajal, S. },
  title = {{T}he {G}olgi silver impregnation method: commemorating the centennial
	of the {N}obel {P}rize in medicine (1906) shared by {C}amillo {G}olgi
	and {S}antiago {R}am\'on y {C}ajal},
  journal = {Biomedica},
  year = {2006},
  volume = {26},
  pages = {498--508},
  month = {Dec},
  abstract = { The Golgi silver impregnation technique is a simple histological
	procedure that reveals complete three-dimensional neuron morphology.
	This method is based in the formation of opaque intracellular deposits
	of silver chromate obtained by the reaction between potassium dichromate
	and silver nitrate (black reaction). Camillo Golgi, its discoverer,
	and Santiago Ram\'on y Cajal its main exponent, shared the Nobel
	Prize of Medicine and Physiology in 1906 for their contribution to
	the knowledge of the nervous system structure, Their successes were
	largely due to the application of the silver impregnation method.
	However, Golgi and Cajal had different views on the structure of
	nervous tissue. According to the Reticular Theory, defended by Golgi,
	the nervous system was formed by a network of cells connected via
	axons within a syncytium. In contrast, Cajal defended the Neuron
	Doctrine which maintained that the neurons were independent cells.
	In addition, Golgi had used a variant of his "black reaction" to
	discover the cellular organelle that became known as the Golgi apparatus.
	Electron microscopy studies confirmed the postulates of the Neuron
	Doctrine as well as the existence of the Golgi complex and contributed
	to a resurgence of use of the Golgi stain. Although modern methods
	of intracellular staining reveal excellent images of neuron morphology,
	the Golgi technique is an easier and less expensive method for the
	study of normal and pathological morphology of neurons. }
}

@ARTICLE{touboul08bifurcation,
  author = {Jonathan Touboul},
  title = {Bifurcation Analysis of a General Class of Nonlinear Integrate-and-Fire
	Neurons},
  journal = {SIAM Journal on Applied Mathematics},
  year = {2008},
  volume = {68},
  pages = {1045-1079},
  number = {4},
  doi = {10.1137/070687268},
  keywords = {neuron models; dynamical system analysis; nonlinear dynamics; Hopf
	bifurcation; saddle-node bifurcation; BogdanovTakens bifurcation;
	Bautin bifurcation; saddle homoclinic bifurcation; subthreshold neuron
	oscillations},
  publisher = {SIAM},
  url = {http://link.aip.org/link/?SMM/68/1045/1}
}

@ARTICLE{touboul08adex,
  author = {Touboul, Jonathan and Brette, Romain},
  title = {Dynamics and bifurcations of the adaptive exponential integrate-and-fire
	model},
  journal = {Biological Cybernetics},
  year = {2008},
  volume = {99},
  pages = {319--334},
  number = {4},
  month = {Nov},
  abstract = {Abstract\&nbsp;\&nbsp;Recently, several two-dimensional spiking neuron
	models have been introduced, with the aim of reproducing the diversity
	of electrophysiological features displayed by real neurons while
	keeping a simple model, for simulation and analysis purposes. Among
	these models, the adaptive integrate-and-fire model is physiologically
	relevant in that its parameters can be easily related to physiological
	quantities. The interaction of the differential equations with the
	reset results in a rich and complex dynamical structure. We relate
	the subthreshold features of the model to the dynamical properties
	of the differential system and the spike patterns to the properties
	of a Poincar{\'e} map defined by the sequence of spikes. We find
	a complex bifurcation structure which has a direct interpretation
	in terms of spike trains. For some parameter values, spike patterns
	are chaotic.},
  day = {01},
  doi = {10.1007/s00422-008-0267-4},
  url = {http://dx.doi.org/10.1007/s00422-008-0267-4}
}

@ARTICLE{toyoizumi07optimal,
  author = {Toyoizumi, Taro and Pfister, Jean-Pascal and Aihara, Kazuyuki and
	Gerstner, Wulfram},
  title = {{Optimality Model of Unsupervised Spike-Timing-Dependent Plasticity:
	Synaptic Memory and Weight Distribution}},
  journal = {Neural Comp.},
  year = {2007},
  volume = {19},
  pages = {639-671},
  number = {3},
  abstract = {We studied the hypothesis that synaptic dynamics is controlled by
	three basic principles: (1) synapses adapt their weights so that
	neurons can effectively transmit information, (2) homeostatic processes
	stabilize the mean firing rate of the postsynaptic neuron, and (3)
	weak synapses adapt more slowly than strong ones, while maintenance
	of strong synapses is costly. Our results show that a synaptic update
	rule derived from these principles shares features, with spike-timing-dependent
	plasticity, is sensitive to correlations in the input and is useful
	for synaptic memory. Moreover, input selectivity (sharply tuned receptive
	fields) of postsynaptic neurons develops only if stimuli with strong
	features are presented. Sharply tuned neurons can coexist with unselective
	ones, and the distribution of synaptic weights can be unimodal or
	bimodal. The formulation of synaptic dynamics through an optimality
	criterion provides a simple graphical argument for the stability
	of synapses, necessary for synaptic memory.},
  eprint = {http://neco.mitpress.org/cgi/reprint/19/3/639.pdf},
  keywords = {plasticity spiking},
  url = {http://neco.mitpress.org/cgi/content/abstract/19/3/639}
}

@INPROCEEDINGS{trefzer_eh04,
  author = {Martin Trefzer and J{\"o}rg Langeheine and Karlheinz Meier and Johannes
	Schemmel},
  title = {New genetic operators to facilitate understanding of evolved transistor
	circuits},
  booktitle = {Proceedings of the 2004 {NASA}/{DoD} Conference an Evolvable Hardware
	({EH}2004)},
  year = {2004},
  keywords = {vision eh}
}

@MISC{qt42homepage,
  author = {{Trolltech}},
  title = {{Q}t Cross-Platform Application Framework 4.2.0},
  howpublished = {http://trolltech.com/developer/resources/notes/changes/changes-4.2.0/},
  year = {2006},
  owner = {mueller},
  timestamp = {2008.08.05}
}

@MISC{trolltech_company,
  author = {{Trolltech~AS}},
  title = {The {Q}t application development framework},
  howpublished = {Waldemar Thranes gate, 98, NO-0175 Oslo, Norway, \\\texttt{http://www.trolltech.com/\-products/qt/}},
  address = {Waldemar Thranes gate, 98, NO-0175 Oslo, Norway},
  annote = {qt},
  groupsearch = {0},
  keywords = {spec}
}

@ARTICLE{Tsodyks97, crossref={tsodyks97neural}}
@ARTICLE{tsodyks97neural,
  author = {M. Tsodyks and H. Markram},
  title = {The neural code between neocortical pyramidal neurons depends on
	neurotransmitter release probability},
  journal = {Proceedings of the national academy of science USA},
  year = {1997},
  volume = {94},
  pages = {719--723},
  month = jan,
  visnote = {short term depression, precursor to markram98}
}

@ARTICLE{tsutsui2008improving,
  author = {Tsutsui, H. and Karasawa, S. and Okamura, Y. and Miyawaki, A.},
  title = {Improving membrane voltage measurements using FRET with new fluorescent
	proteins},
  journal = {Nature methods},
  year = {2008},
  volume = {5},
  pages = {683--685},
  number = {8},
  publisher = {Nature Publishing Group}
}

@ARTICLE{turing37computable,
  author = {Turing, A.},
  title = {On computable numbers, with an application to the {E}ntscheidungsproblem},
  journal = {Proceedings of the London Mathematical Society},
  year = {1937},
  volume = {42},
  pages = {230--265}
}

@ARTICLE{Turing1950,
  author = {Turing, Alan M},
  title = {Computing machinery and intelligence},
  journal = {Mind},
  year = {1950},
  volume = {59},
  pages = {433--460},
  number = {236},
  owner = {simon},
  publisher = {JSTOR},
  timestamp = {2013.04.29}
}

@ARTICLE{ullmann99computation,
  author = {S. Ullmann and S. Soloviev},
  title = {Computation of pattern invariance in brain-like structures},
  journal = {Neural Networks},
  year = {1999},
  volume = {12},
  pages = {1021--1036},
  keywords = {convolutional NN},
  owner = {fieres}
}

@ARTICLE{valle02analog,
  author = {Valle, M.},
  title = {Analog {VLSI} implementations of neural networks with supervised
	on-chip-learning},
  journal = {Analog Integrated Circuits and Signal Processing},
  year = {2002},
  volume = {33},
  pages = {263--287},
  keywords = {vlsi}
}

@MISC{vanrullen-surfing,
  author = {Rufin VanRullen and Simon J. Thorpe},
  title = {Surfing a Spike Wave down the Ventral Stream},
  citeseerurl = {citeseer.nj.nec.com/468065.html},
  keywords = {spiking}
}

@ARTICLE{vapnik71uniform,
  author = {Vapnik, V. N. and Chervonenkis, A. Y.},
  title = {On the uniform convergence of relative frequencies of events to their
	probabilities},
  journal = {Theoretical Probability and Its Applications},
  year = {1971},
  volume = {17},
  pages = {264--280}
}

@ARTICLE{venayagamoorthy07online,
  author = {Venayagamoorthy, Ganesh K.},
  title = {Online design of an echo state network based wide area monitor for
	a multimachine power system},
  journal = {Neural Networks},
  year = {2007},
  volume = {20},
  pages = {404--413},
  number = {3},
  month = apr,
  abstract = {With deregulation and growth of the power industry, many power system
	elements such as generators, transmission lines, are driven to operate
	near their maximum capacity, especially those serving heavy load
	centres. Wide Area Controllers (WACs) using wide area or global signals
	can provide remote auxiliary control signals to local controllers
	such as automatic voltage regulators, power system stabilizers, etc.
	to damp out system oscillations. However, since the power system
	is highly nonlinear with fast changing dynamics, it is a challenging
	problem to design an online system monitor/estimator, which can provide
	dynamic intra-area and inter-area information such speed deviations
	of generators to an adaptive WAC continuously. This paper presents
	a new kind of recurrent neural networks, called the Echo State Network
	(ESN), for the online design of a Wide Area Monitor (WAM) for a multimachine
	power system. A single ESN is used to predict the speed deviations
	of four generators in two different areas. The performance of this
	ESN WAM is evaluated for small and large disturbances on the power
	system. Results for an ESN based WAM and a Time-Delayed Neural Network
	(TDNN)-based WAM are presented and compared. The advantages of the
	ESN WAM are that it learns the dynamics of the power system in a
	shorter training time with a higher accuracy and with considerably
	fewer weights to be adapted compared to the design-based on a TDNN.},
  booktitle = {Echo State Networks and Liquid State Machines},
  keywords = {Echo state network, Multimachine power system, Online training, System
	identification, Time-delayed neural network, Wide area monitor, Wide
	area control system,, liquid},
  owner = {mreuss},
  timestamp = {2007.06.14},
  url = {http://www.sciencedirect.com/science/article/B6T08-4NMWR88-D/2/5365b74c22797f0503640400a89c4a12}
}

@ARTICLE{verstraeten07unification,
  author = {Verstraeten, D. and Schrauwen, B. and D'Haene, M. and Stroobandt,
	D.},
  title = {An experimental unification of reservoir computing methods},
  journal = {Neural Networks},
  year = {2007},
  volume = {20},
  pages = {391--403},
  number = {3},
  month = apr,
  abstract = {Three different uses of a recurrent neural network (RNN) as a reservoir
	that is not trained but instead read out by a simple external classification
	layer have been described in the literature: Liquid State Machines
	(LSMs), Echo State Networks (ESNs) and the Backpropagation Decorrelation
	(BPDC) learning rule. Individual descriptions of these techniques
	exist, but a overview is still lacking. Here, we present a series
	of experimental results that compares all three implementations,
	and draw conclusions about the relation between a broad range of
	reservoir parameters and network dynamics, memory, node complexity
	and performance on a variety of benchmark tests with different characteristics.
	Next, we introduce a new measure for the reservoir dynamics based
	on Lyapunov exponents. Unlike previous measures in the literature,
	this measure is dependent on the dynamics of the reservoir in response
	to the inputs, and in the cases we tried, it indicates an optimal
	value for the global scaling of the weight matrix, irrespective of
	the standard measures. We also describe the Reservoir Computing Toolbox
	that was used for these experiments, which implements all the types
	of Reservoir Computing and allows the easy simulation of a wide range
	of reservoir topologies for a number of benchmarks.},
  booktitle = {Echo State Networks and Liquid State Machines},
  keywords = {Reservoir computing, Memory capability, Chaos, Lyapunov exponent,,
	liquid},
  owner = {mreuss},
  timestamp = {2007.06.14},
  url = {http://www.sciencedirect.com/science/article/B6T08-4NKYNTD-3/2/200495219ef0a4d03b64694797f421f0}
}

@ARTICLE{victor1996,
  author = {Victor, J. D. and Purpura, K. P.},
  title = {{Nature and precision of temporal coding in visual cortex: a metric-space
	analysis}},
  journal = {J Neurophysiol},
  year = {1996},
  volume = {76},
  pages = {1310-1326},
  number = {2}
}

@software{vinkelis2020bitsery,
  author = {Mindaugas Vinkelis},
  title = {Bitsery},
  year = {2020},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/fraillt/bitsery}},
  commit = {db884a0656a3aabb87da1ae6edf12629507f76a7}
}

@MISC{CorinnaVinschen2013,
  author = {Corinna Vinschen and Jeff Johnston},
  title = {newlib C library},
  howpublished = {website},
  month = {March},
  year = {2013},
  owner = {simon},
  timestamp = {2013.03.27},
  url = {http://sourceware.org/newlib/}
}

@INPROCEEDINGS{vittoz97analog,
  author = {Vittoz, E. A.},
  title = {Analog {VLSI} implementation of neural networks},
  booktitle = {The Handbook of Neural Computation},
  year = {1997},
  editor = {Fiesler, E. and Beale, R.},
  address = {New York},
  month = {January},
  publisher = {Institute of Physics Publishing and Oxford University Publishing},
  chapter = {E1.4},
  keywords = {vlsi}
}

@ARTICLE{vittoz94analog,
  author = {Vittoz, E. A.},
  title = {Analog {VLSI} signal processing: {W}hy, where and how?},
  journal = {Analog Integrated Circuits and Signal Processing},
  year = {1994},
  volume = {8},
  pages = {27--44},
  number = {1},
  keywords = {vlsi}
}

@inproceedings{voelker2017extending,
  title={Extending the neural engineering framework for nonideal silicon synapses},
  author={Voelker, Aaron R and Benjamin, Ben V and Stewart, Terrence C and Boahen, Kwabena and Eliasmith, Chris},
  booktitle={2017 IEEE International Symposium on Circuits and Systems (ISCAS)},
  pages={1--4},
  year=2017,
  organization={IEEE}
}

@ARTICLE{vogels05signal,
  author = {Tim P Vogels and L F Abbott},
  title = {Signal propagation and logic gating in networks of integrate-and-fire
	neurons},
  journal = {J Neurosci},
  year = {2005},
  volume = {25},
  pages = {10786--95},
  number = {46},
  month = {Nov},
  abstract = {Transmission of signals within the brain is essential for cognitive
	function, but it is not clear how neural circuits support reliable
	and accurate signal propagation over a sufficiently large dynamic
	range. Two modes of propagation have been studied: synfire chains,
	in which synchronous activity travels through feedforward layers
	of a neuronal network, and the propagation of fluctuations in firing
	rate across these layers. In both cases, a sufficient amount of noise,
	which was added to previous models from an external source, had to
	be included to support stable propagation. Sparse, randomly connected
	networks of spiking model neurons can generate chaotic patterns of
	activity. We investigate whether this activity, which is a more realistic
	noise source, is sufficient to allow for signal transmission. We
	find that, for rate-coded signals but not for synfire chains, such
	networks support robust and accurate signal reproduction through
	up to six layers if appropriate adjustments are made in synaptic
	strengths. We investigate the factors affecting transmission and
	show that multiple signals can propagate simultaneously along different
	pathways. Using this feature, we show how different types of logic
	gates can arise within the architecture of the random network through
	the strengthening of specific synapses.},
  affiliation = {Volen Center for Complex Systems and Department of Biology, Brandeis
	University, Waltham, Massachusetts 02454-9110, USA. vogels@brandeis.edu}
}

@ARTICLE{vogelstein2007multichip,
  author = {R. Jacob Vogelstein and Udayan Mallik and Eugenio Culurciello and
	Gert Cauwenberghs, Ralph Etienne-Cummings},
  title = {A Multichip Neuromorphic System for Spike-Based Visual Information
	Processing},
  journal = {Neural Computation},
  year = {2007},
  volume = {19},
  pages = {2281-2300},
  keywords = {neuromorphic},
  owner = {bruederl},
  timestamp = {2008.09.02}
}

@ARTICLE{vogelstein2007reconfigurable,
  author = {R. Jacob Vogelstein and Udayan Mallik and Joshula T. Vogelstein and
	Gert Cauwenberghs},
  title = {Dynamically Reconfigurable Silicon Array of Spiking Neuron with Conductance-Based
	Synapses},
  journal = {IEEE Transactions on Neural Networks},
  year = {2007},
  volume = {18},
  pages = {253-265},
  keywords = {neuromorphic},
  owner = {bruederl},
  timestamp = {2008.09.02}
}

@INPROCEEDINGS{Vogelstein2003,
  author = {R. Jacob Vogelstein and Francesco Tenore and Ralf Philipp and Miriam
	S. Adlerstein and David H. Goldberg and Gert Cauwenberghs},
  title = {Spike Timing-Dependent Plasticity in the Address Domain},
  booktitle = {Advances in Neural Information Processing Systems 15},
  year = {2003},
  editor = {S. Becker, S. Thrun and K. Obermayer},
  pages = {1147--1154},
  address = {Cambridge, MA},
  publisher = {MIT Press},
  owner = {simon},
  timestamp = {2012.12.15}
}

@ARTICLE{vose91punctuated,
  author = {Vose, M. D. and Liepins, G. E.},
  title = {Punctuated equilibria in genetic search},
  journal = {Complex Systems},
  year = {1992},
  volume = {5},
  pages = {31--44},
  number = {1}
}

@ARTICLE{vanvreeswijk96chaos,
  author = {Carl van Vreeswijk and H Sompolinsky},
  title = {Chaos in neuronal networks with balanced excitatory and inhibitory
	activity},
  journal = {Science},
  year = {1996},
  volume = {274},
  pages = {1724--6},
  number = {5293},
  month = {Dec},
  abstract = {Neurons in the cortex of behaving animals show temporally irregular
	spiking patterns. The origin of this irregularity and its implications
	for neural processing are unknown. The hypothesis that the temporal
	variability in the firing of a neuron results from an approximate
	balance between its excitatory and inhibitory inputs was investigated
	theoretically. Such a balance emerges naturally in large networks
	of excitatory and inhibitory neuronal populations that are sparsely
	connected by relatively strong synapses. The resulting state is characterized
	by strongly chaotic dynamics, even when the external inputs to the
	network are constant in time. Such a network exhibits a linear response,
	despite the highly nonlinear dynamics of single neurons, and reacts
	to changing external stimuli on time scales much smaller than the
	integration time constant of a single neuron.},
  affiliation = {Racah Institute of Physics and Center for Neural Computation, Hebrew
	University, Jerusalem, 91904 Israel.}
}

@ARTICLE{vreeswijk96,
  author = {{Vreeswijk}, C.~V.},
  title = {Partial synchronization in populations of pulse-coupled oscillators},
  journal = {Physical Review E},
  year = {1996},
  volume = {54},
  pages = {5522-5537},
  month = Nov
}

@ARTICLE{waldert08hand,
  author = {Waldert, S. and Preissl, H. and Demandt, E. and Braun, C. and Birbaumer,
	N. and Aertsen, A. and Mehring, C. },
  title = {Hand movement direction decoded from {MEG} and {EEG}},
  journal = {J Neurosci},
  year = {2008},
  volume = {28},
  pages = {1000--1008},
  number = {4},
  month = {January},
  abstract = {Brain activity can be used as a control signal for brain-machine interfaces
	(BMIs). A powerful and widely acknowledged BMI approach, so far only
	applied in invasive recording techniques, uses neuronal signals related
	to limb movements for equivalent, multidimensional control of an
	external effector. Here, we investigated whether this approach is
	also applicable for noninvasive recording techniques. To this end,
	we recorded whole-head MEG during center-out movements with the hand
	and found significant power modulation of MEG activity between rest
	and movement in three frequency bands: an increase for < or = 7 Hz
	(low-frequency band) and 62-87 Hz (high-gamma band) and a decrease
	for 10-30 Hz (beta band) during movement. Movement directions could
	be inferred on a single-trial basis from the low-pass filtered MEG
	activity as well as from power modulations in the low-frequency band,
	but not from the beta and high-gamma bands. Using sensors above the
	motor area, we obtained a surprisingly high decoding accuracy of
	67\% on average across subjects. Decoding accuracy started to rise
	significantly above chance level before movement onset. Based on
	simultaneous MEG and EEG recordings, we show that the inference of
	movement direction works equally well for both recording techniques.
	In summary, our results show that neuronal activity associated with
	different movements of the same effector can be distinguished by
	means of noninvasive recordings and might, thus, be used to drive
	a noninvasive BMI.},
  keywords = {classification, humans, meg, movements, offline, peri-movement}
}

@article{wang2001efficient,
  title={Efficient, multiple-range random walk algorithm to calculate the density of states},
  author={Wang, Fugao and Landau, David P},
  journal={Physical Review Letters},
  volume={86},
  number={10},
  pages={2050},
  year={2001},
  publisher={APS}
}

@ARTICLE{Wang2005,
  author = {Wang, Huai-Xing and Gerkin, Richard C and Nauen, David W and Bi,
	Guo-Qiang},
  title = {Coactivation and timing-dependent integration of synaptic potentiation
	and depression},
  journal = {Nature Neuroscience},
  year = {2005},
  volume = {8},
  pages = {187--193},
  number = {2},
  owner = {simon},
  publisher = {Nature Publishing Group},
  timestamp = {2013.04.25}
}

@ARTICLE{wang04improvement,
  author = {Wang, S. and J. Xu and F. Liua and W. Wang},
  title = {Improvement of signal transmission through spike-timing-dependent
	plasticity in neural networks},
  journal = {Eur. Phys. J. B},
  year = {2004},
  volume = {39},
  pages = {351-356},
  abstract = {We explore the e?ects of spike-timing-dependent plasticity (STDP)
	on weak signal transmission in a noisy neural network. We ?rst consider
	the network where an ensemble of independent neurons, which are subjected
	to a common weak signal, are connected in parallel to a single postsynaptic
	neuron via excitatory synapses. STDP can make the signal transmission
	more e?cient, and this e?ect is more prominent when the presynaptic
	activities exhibit some correlations. We further consider a two-layer
	network where there are only couplings between two layers and ?nd
	that postsynaptic neurons can ?re synchronously under suitable conditions.
	Both the reliability and timing precision of neuronal ?ring in the
	output layer are remarkably improved with STDP. These results indicate
	that STDP can play crucial roles in information processing in nervous
	systems.},
  file = {wang04improvement.pdf:wang04improvement.pdf:PDF},
  keywords = {plasticity},
  owner = {mreuss}
}

@ARTICLE{wang2011,
  author = {Yingxue Wang and Shih-Chii Liu},
  title = {Multilayer Processing of Spatiotemporal Spike Patterns in a Neuron
	with Active Dendrites},
  journal = {Neural Computation},
  year = {2011},
  volume = {28},
  pages = {2086-2112},
  issue = {8}
}

@ARTICLE{wang2011array,
  author = {Yingxue Wang and Shih-Chii Liu},
  title = {A Two-Dimensional Configurable Active Silicon Dendritic Neuron Array},
  journal = {Circuits and Systems I: Regular Papers, IEEE Transactions on},
  year = {2011},
  volume = {58},
  pages = {2159 -2171},
  number = {9},
  month = {sept. },
  abstract = {This paper presents a 2-D programmable dendritic neuron array consisting
	of a 3 #x00D7; 32 dendritic compartment array and a 1 #x00D7; 32
	somatic compartment array. Each dendritic compartment contains two
	types of regenerative nonlinearities: a NMDA synaptic nonlinearity
	and a dendritic spike nonlinearity. The chip supports the programmability
	of local synaptic weights and the configuration of dendritic morphology
	for individual neurons through the address-event representation protocol.
	Neurons can be stimulated and recorded using the same protocol. A
	novel local cable circuit between neighboring compartments allows
	one to construct different dendritic morphologies. This chip provides
	a hardware platform for studying the network behavior of neurons
	with active dendrites and for investigating the role of different
	dendritic morphologies in neuronal computation. Based on experimental
	results from a chip fabricated in a 4-metal, 2-poly, 0.35 #x03BC;m
	CMOS technology, this work shows one instance of how dendritic nonlinearities
	can contribute to neuronal computation, that is, the dendritic spike
	mechanism can dynamically reduce the mismatch-induced coefficient
	of variation of the somatic response amplitude from about 40% to
	3.5%, and the response timing jitter by a factor of 2.},
  doi = {10.1109/TCSI.2011.2112570},
  issn = {1549-8328},
  keywords = {2D programmable dendritic neuron array;4-metal 2-poly CMOS technology;N-methyl-D-aspartate;NMDA
	synaptic nonlinearity;Si;address-event representation protocol;analog-digital
	integrated circuit;cable circuit;dendritic compartment array;dendritic
	morphology;dendritic spike nonlinearity;local synaptic weight programmability;mismatch-induced
	coefficient;size 0.35 mum;somatic compartment array;somatic response
	amplitude;timing jitter;two-dimensional configurable active silicon
	dendritic neuron array;CMOS analogue integrated circuits;CMOS logic
	circuits;active networks;mixed analogue-digital integrated circuits;neural
	nets;programmable logic arrays;silicon;timing jitter;}
}

@article{wang2015neural,
  author = {Wang, Ming-Shyan and Chen, Seng and Chuang, Po-Hsiang and Wu, Shih-Yu and Hsu, Fu-Shung},
  year = 2015,
  month = oct,
  pages = {1--9},
  title = {Neural Network Control-Based Drive Design of Servomotor and Its Application to Automatic Guided Vehicle},
  volume = 2015,
  journal = {Mathematical Problems in Engineering},
  doi = {10.1155/2015/612932}
}

@article{wang2015neuromorphic,
	abstract = "We present a neuromorphic implementation of multiple synaptic plasticity learning rules, which include both Spike Timing Dependent Plasticity (STDP) and Spike Timing Dependent Delay Plasticity (STDDP). We present a fully digital implementation as well as a mixed-signal implementation, both of which use a novel dynamic-assignment time-multiplexing approach and support up to 226 (64M) synaptic plasticity elements. Rather than implementing dedicated synapses for particular types of synaptic plasticity, we implemented a more generic synaptic plasticity adaptor array that is separate from the neurons in the neural network. Each adaptor performs synaptic plasticity according to the arrival times of the pre- and post-synaptic spikes assigned to it, and sends out a weighted or delayed pre-synaptic spike to the post-synaptic neuron in the neural network. This strategy provides great flexibility for building complex large-scale neural networks, as a neural network can be configured for multiple synaptic plasticity rules without changing its structure. We validate the proposed neuromorphic implementations with measurement results and illustrate that the circuits are capable of performing both STDP and STDDP. We argue that it is practical to scale the work presented here up to 236 (64G) synaptic adaptors on a current high-end FPGA platform.",
	author = "Wang, Runchun Mark and Hamilton, Tara Julia and Tapson, Jonathan and van Schaik, Andr{\'e}",
	doi = "10.3389/fnins.2015.00180",
	issn = "1662-453X",
	journal = "Frontiers in Neuroscience",
	number = "180",
	title = "{A neuromorphic implementation of multiple spike-timing synaptic plasticity rules for large-scale neural networks}",
	url = "http://www.frontiersin.org/neuromorphic_engineering/10.3389/fnins.2015.00180/abstract",
	volume = "9",
	year = "2015"
}

@article{wang2018fpga,
  title={An {FPGA}-based massively parallel neuromorphic cortex simulator},
  author={Wang, Runchun M and Thakur, Chetan S and van Schaik, Andr{\'e}},
  journal={Frontiers in Neuroscience},
  volume=12,
  pages=213,
  year=2018,
  publisher={Frontiers}
}

@article{waters2005backprop,
    title = "Backpropagating action potentials in neurones: measurement, mechanisms and potential functions ",
    journal = "Progress in Biophysics and Molecular Biology ",
    volume = "87",
    number = "1",
    pages = "145 - 170",
    year = "2005",
    note = "<ce:title>Biophysics of Excitable Tissues</ce:title> ",
    issn = "0079-6107",
    doi = "http://dx.doi.org/10.1016/j.pbiomolbio.2004.06.009",
    url = "http://www.sciencedirect.com/science/article/pii/S0079610704000653",
    author = "Jack Waters and Andreas Schaefer and Bert Sakmann"
}

@INPROCEEDINGS{watrous87learning,
  author = {Watrous, R. L.},
  title = {Learning algorithms for connectionist networks: {A}pplied gradient
	methods for nonlinear optimization},
  booktitle = {Proceedings of the IEEE First International Conference on Neural
	Networks},
  year = {1987},
  editor = {Caudill, M. and Butler, C.},
  volume = {II},
  pages = {619--627},
  address = {San Diego},
  publisher = {IEEE}
}

@ARTICLE{watts2005connectionSelectivity,
  author = {Watts J, Thomson AM},
  title = {Excitatory and inhibitory connections show selectivity in the neocortex},
  journal = {Journal of Physiology},
  year = {2005},
  volume = {562},
  pages = {89-97},
  owner = {bkaplan},
  timestamp = {2008.11.12}
}

@MISC{nestinitiative09homepage,
  author = {{The {NEST} Initiative} -- Website},
  howpublished = {\url{http://www.nest-initiative.org}},
  year = {2009}
}

@mastersthesis{wehrheim2019bsc,
  author   = {Malte Wehrheim},
  title    = {Reconstruction of Synaptic Weight on the Neuromorphic BrainscaleS-1 System},
  school   = {Universit{\"a}t Heidelberg},
  year     = {2019},
  type     = {Bachelor thesis},
}

@ARTICLE{weicker1984dhrystone,
  author = {Weicker, Reinhold P},
  title = {Dhrystone: a synthetic systems programming benchmark},
  journal = {Communications of the ACM},
  year = {1984},
  volume = {27},
  pages = {1013--1030},
  number = {10},
  owner = {simon},
  publisher = {ACM},
  timestamp = {2013.02.20}
}

@mastersthesis{weidner2019ba,
  author   = {Jonas Weidner},
  title    = {Experiment Visualization and Simulations towards a Cortical Microcircuit on the BrainScaleS Neuromorphic Hardware},
  school   = {Universit{\"a}t Heidelberg},
  year     = {2019},
  type     = {Bachelor thesis},
}

@MASTERSTHESIS{weilbach2015bachelorthesis,
  author = {Weilbach, Christian},
  title = {An online learning algorithm for {LIF}-based Boltzmann machines},
  year = 2015,
  type = {Bachelor thesis},
  school = {Ruprecht-Karls-Universit{\"a}t Heidelberg},
}

@ARTICLE{Weiller1992,
  author = {Weiller, Cornelius and Chollet, François and Friston, Karl J. and
	Wise, Richard J. S. and Frackowiak, Richard S. J.},
  title = {Functional reorganization of the brain in recovery from striatocapsular
	infarction in man},
  journal = {Annals of Neurology},
  year = {1992},
  volume = {31},
  pages = {463--472},
  number = {5},
  doi = {10.1002/ana.410310502},
  issn = {1531-8249},
  owner = {simon},
  publisher = {Wiley Subscription Services, Inc., A Wiley Company},
  timestamp = {2013.05.03},
  url = {http://dx.doi.org/10.1002/ana.410310502}
}

@INPROCEEDINGS{weis2020inference,
	author = {Weis, Johannes and Spilger, Philipp and Billaudelle, Sebastian and Stradmann, Yannik and Emmel, Arne and M\"{u}ller, Eric and Breitwieser, Oliver and Gr\"{u}bl, Andreas and Ilmberger, Joscha and Karasenko, Vitali and Kleider, Mitja and Mauch, Christian and Schreiber, Korbinian and Schemmel, Johannes},
	title = {Inference with Artificial Neural Networks on Analog Neuromorphic Hardware},
	year = 2020,
	booktitle = {IoT Streams for Data-Driven Predictive Maintenance and IoT, Edge, and Mobile for Embedded Machine Learning},
	publisher={Springer International Publishing},
	address={Cham},
	pages={201--212},
	abstract={The neuromorphic BrainScaleS-2 ASIC comprises mixed-signal neurons and synapse circuits as well as two versatile digital microprocessors. Primarily designed to emulate spiking neural networks, the system can also operate in a vector-matrix multiplication and accumulation mode for artificial neural networks. Analog multiplication is carried out in the synapse circuits, while the results are accumulated on the neurons' membrane capacitors. Designed as an analog, in-memory computing device, it promises high energy efficiency. Fixed-pattern noise and trial-to-trial variations, however, require the implemented networks to cope with a certain level of perturbations. Further limitations are imposed by the digital resolution of the input values (5 bit), matrix weights (6 bit) and resulting neuron activations (8 bit). In this paper, we discuss BrainScaleS-2 as an analog inference accelerator and present calibration as well as optimization strategies, highlighting the advantages of training with hardware in the loop. Among other benchmarks, we classify the MNIST handwritten digits dataset using a two-dimensional convolution and two dense layers. We reach 98.0{\%} test accuracy, closely matching the performance of the same network evaluated in software.},
	isbn={978-3-030-66770-2},
	doi={10.1007/978-3-030-66770-2_15}
}

@mastersthesis{weis2020msc,
  author   = {Johannes Weis},
  title    = {Inference with Artificial Neural Networks on Neuromorphic Hardware},
  school   = {Universit{\"a}t Heidelberg},
  year     = 2020,
  type     = {Master's thesis},
  month    = sep
}

@INPROCEEDINGS{wendt07,
  author = { Karsten Wendt and Matthias Ehrlich and Christian Mayr and Ren\'{e} Sch\"{u}ffny},
  title = {Abbildung komplexer, pulsierender, neuronaler Netzwerke auf spezielle
	Neuronale VLSI Hardware},
  booktitle = {DASS'07: Proceedings of Dresdener Arbeitstagung Schaltungs- und Systementwurf},
  year = {2007},
  pages = {127--132},
  location = {Dresden, Germany}
}

@INPROCEEDINGS{wendt2010anniip,
  author = {Wendt, K. and Ehrlich, M. and Sch\"uffny, R.},
  title = {{GMPath} - a path language for navigation, information query and
	modification of data graphs},
  booktitle = {Proceedings of the Artificial Neural Networks and Intelligent Information
	Processing Conference (ANNIIP) 2010},
  year = {2010},
  pages = {31--42}
}

@INPROCEEDINGS{wendt08,
  author = {Karsten Wendt and Matthias Ehrlich and Ren\'{e} Sch\"{u}ffny},
  title = {A graph theoretical approach for a multistep mapping software for
	the FACETS project},
  booktitle = {CEA'08: Proceedings of the 2nd WSEAS International Conference on
	Computer Engineering and Applications},
  year = {2008},
  pages = {189--194},
  address = {Stevens Point, Wisconsin, USA},
  publisher = {World Scientific and Engineering Academy and Society (WSEAS)},
  isbn = {978-960-6766-33-6},
  location = {Acapulco, Mexico}
}

@ARTICLE{weng1997cresceptron,
  author = {J. Weng and N. Ahuja and T. S. Huang},
  title = {Learning recognition and segmentation using the Cresceptron},
  journal = {International Journal of Computer Vision},
  year = {1997},
  volume = {25},
  pages = {109-143},
  number = {2},
  file = {weng1997cresceptron.pdf, weng1997cresceptron2.pdf:weng1997cresceptron.pdf, weng1997cresceptron2.pdf:PDF},
  keywords = {convolutional NN},
  owner = {fieres}
}

@misc{wenzel2019pybind11,
   author = {Wenzel Jakob and Jason Rhinelander and Dean Moldovan},
   year = 2019,
   url = {https://github.com/pybind/pybind11},
   title = {pybind11 -- Seamless operability between {C++11} and {Python}}
}

@ARTICLE{white90learn,
  author = {White, H.},
  title = {Connectionist Nonparametric Regression: Multilayer Feedforward Networks
	can Learn Arbitrary Mappings},
  journal = {Neural Networks},
  year = {1990},
  volume = {3},
  pages = {535--549},
  number = {5}
}

@ARTICLE{white99,
  author = {P. White and B. Biskup and J. Elzenga and U. Homann and G. Thiel
	and F. Wissing and F. Maathuis},
  title = {Advanced patch-clamp techniques and single-channel analysis},
  journal = {Journal of Experimental Botany},
  year = {1999},
  volume = {50},
  pages = {1037-1054}
}

@INPROCEEDINGS{white_icga97diffusion,
  author = {White, P. M. and Pettey, C. C.},
  title = {Double selection vs. single selection in diffusion model {GA}s},
  booktitle = {Proceedings of the 7th International Conference on Genetic Algorithms},
  year = {1997},
  editor = {B{\"a}ck, T.},
  pages = {174--180},
  address = {San Fransiso},
  publisher = {Morgan Kaufmann}
}

@ARTICLE{whitehead03dynamical,
  author = {Whitehead, A. and Rabinovich, M. I. and Huerta , R. and Zhigulin,
	V. P. and Henry D. I. Abarbanel},
  title = {Dynamical synaptic plasticity: a model and connection to some experiments},
  journal = {Biol. Cybern.},
  year = {2003},
  volume = {88},
  pages = {229-235},
  abstract = {Abstract. Using a modified version of a phenomenological model for
	the dynamics of synaptic plasticity, we examine some recent experiments
	of Wu et al. [(2001) J Physiol 533:745-755]. We show that the model
	is quantitatively consistent with their experimental protocols producing
	long-term potentiation (LTP) and long-term depression (LTD) in slice
	preparations of rat hippocampus. We also predict the outcome of similar
	experiments using different frequencies and depolarization levels
	than reported in their results.},
  file = {whitehead03dynamical.pdf:whitehead03dynamical.pdf:PDF},
  keywords = {plasticity},
  owner = {mreuss},
  url = {http://www.springerlink.com/link.asp?id=00pcat3g9p3ntpn1}
}

@INPROCEEDINGS{whitley_icga93cellular,
  author = {Whitley, D.},
  title = {Cellular Genetic Algorithms},
  booktitle = {Proceedings of the 5th International Conference on Genetic Algorithms},
  year = {1993},
  editor = {Forrest, S.},
  pages = {658},
  address = {San Francisco},
  publisher = {Morgan Kaufmann}
}

@INPROCEEDINGS{whitley_icga89genitor,
  author = {Whitley, D.},
  title = {The {GENITOR} algorithm and selection pressure: {W}hy rank-based
	allocation of reproductive trials is best},
  booktitle = {Proceedings of the Third International Conference on Genetic Algorithms},
  year = {1989},
  editor = {Schaffer, J. D.},
  pages = {116--121},
  publisher = {Morgan Kaufmann}
}

@INPROCEEDINGS{whitley89optimizing,
  author = {Whitley, D. and Hanson, T.},
  title = {Optimizing neural networks using faster, more accurate genetic search},
  booktitle = {Proceedings of the Third International Conference on Genetic Algorithms},
  year = {1989},
  editor = {Schaffer, J. D.},
  pages = {391--395}
}

@ARTICLE{whitley90genetic,
  author = {Whitley, D. and Starkweather, T. and Bogart, C.},
  title = {Genetic algorithms and neural networks: {O}ptimizing connections
	and connectivity},
  journal = {Parallel Computing},
  year = {1990},
  volume = {14},
  pages = {347--361},
  number = {3}
}

@INPROCEEDINGS{whitley91fundamental,
  author = {Whitley, L. D.},
  title = {Fundamental principles of deception in genetic search},
  booktitle = {Foundations of Genetic Algorithms},
  year = {1991},
  editor = {Rawlins, G.},
  volume = {1},
  pages = {221--241},
  file = {whitley91fundamental.pdf:whitley91fundamental.pdf:PDF}
}

@INPROCEEDINGS{widrow_ire60adaptive,
  author = {Widrow, Bernard and Hoff, Marcian E.},
  title = {Adaptive switching circuits},
  booktitle = {IRE WESCON Convention Record},
  year = {1960},
  pages = {96-104},
  address = {New York},
  publisher = {IRE}
}

@ARTICLE{widrow90thirty,
  author = {Widrow, B. and Lehr, M. A.},
  title = {30 years of adaptive neural networks: {P}erceptron, {M}adaline, and
	backpropagation},
  journal = {Proceedings of the IEEE},
  year = {1990},
  volume = {78},
  pages = {1415--1442},
  number = {9}
}

@ARTICLE{wielaard01simple,
  author = {Wielaard, D. J. and Shelley, Michael and McLaughlin, David and Shapley,
	Robert},
  title = {How Simple Cells Are Made in a Nonlinear Network Model of the Visual
	Cortex},
  journal = {J. Neurosci.},
  year = {2001},
  volume = {21},
  pages = {5203-5211},
  number = {14},
  abstract = {Simple cells in the striate cortex respond to visual stimuli in an
	approximately linear manner, although the LGN input to the striate
	cortex, and the cortical network itself, are highly nonlinear. Although
	simple cells are vital for visual perception, there has been no satisfactory
	explanation of how they are produced in the cortex. To examine this
	question, we have developed a large-scale neuronal network model
	of layer 4C[alpha] in V1 of the macaque cortex that is based on,
	and constrained by, realistic cortical anatomy and physiology. This
	paper has two aims: (1) to show that neurons in the model respond
	like simple cells. (2) To identify how the model generates this linearized
	response in a nonlinear network. Each neuron in the model receives
	nonlinear excitation from the lateral geniculate nucleus (LGN). The
	cells of the model receive strong (nonlinear) lateral inhibition
	from other neurons in the model cortex. Mathematical analysis of
	the dependence of membrane potential on synaptic conductances, and
	computer simulations, reveal that the nonlinearity of corticocortical
	inhibition cancels the nonlinear excitatory input from the LGN. This
	interaction produces linearized responses that agree with both extracellular
	and intracellular measurements. The model correctly accounts for
	experimental results about the time course of simple cell responses
	and also generates testable predictions about variation in linearity
	with position in the cortex, and the effect on the linearity of signal
	summation, caused by unbalancing the relative strengths of excitation
	and inhibition pharmacologically or with extrinsic current.},
  file = {5203.pdf:http\://www.jneurosci.org/cgi/reprint/21/14/5203.pdf:PDF},
  keywords = {spiking}
}

@INPROCEEDINGS{Wijekoon2011,
  author = {Wijekoon, Jayawan HB and Dudek, Piotr},
  title = {Analogue CMOS circuit implementation of a dopamine modulated synapse},
  booktitle = {Circuits and Systems (ISCAS), 2011 IEEE International Symposium on},
  year = {2011},
  pages = {877--880},
  organization = {IEEE},
  owner = {simon},
  timestamp = {2013.06.04}
}

@ARTICLE{wijekoon2008,
  author = {Jayawan H.B. Wijekoon and Piotr Dudek},
  title = {Compact silicon neuron circuit with spiking and bursting behaviour},
  journal = {Neural Networks},
  year = {2008},
  volume = {21},
  pages = {524 - 534},
  number = {2-3},
  note = {Advances in Neural Networks Research: IJCNN '07, 2007 International
	Joint Conference on Neural Networks IJCNN '07},
  abstract = { A silicon neuron circuit that produces spiking and bursting firing
	patterns, with biologically plausible spike shape, is presented.
	The circuit mimics the behaviour of known classes of cortical neurons:
	regular spiking (RS), fast spiking (FS), chattering (CH) and intrinsic
	bursting (IB). The paper describes the operation of the circuit,
	provides simulation results, a simplified analytical model, and a
	phase-plane analysis of its behaviour. The functionality of the circuit
	has been verified experimentally. The paper introduces a proof-of-concept
	analogue integrated circuit, implemented in a 0.35 [mu]m CMOS technology,
	and presents preliminary measurement results. The neuron cell provides
	an area and energy efficient implementation of the silicon cortical
	neuron, and could be used as a universal neuron circuit in VLSI neuromorphic
	networks that closely resemble the circuits of the cortex.},
  doi = {DOI: 10.1016/j.neunet.2007.12.037},
  issn = {0893-6080},
  keywords = {VLSI},
  url = {http://www.sciencedirect.com/science/article/B6T08-4RFSCV3-5/2/c005fcc0c2482bf724210a079932484e}
}

@MISC{wikicommons,
  author = {{Wikimedia commons}},
  title = {A database of 13,455,000 freely usable media files to which anyone
	can contribute},
  howpublished = {commons.wikimedia.org},
  year = {2012}
}

@ARTICLE{wilkes1969growth,
  author = {Wilkes, Maurice V.},
  title = {The growth of interest in microprogramming: A literature survey},
  journal = {ACM Computing Surveys (CSUR)},
  year = {1969},
  volume = {1},
  pages = {139--145},
  number = {3},
  owner = {simon},
  publisher = {ACM},
  timestamp = {2013.02.20}
}

@ARTICLE{williams1988control,
  author = {Williams, R.W. and Herrup, K.},
  title = {The control of neuron number},
  journal = {Annual Review of Neuroscience},
  year = {1988},
  volume = {11},
  pages = {423--453},
  number = {1},
  publisher = {Annual Reviews 4139 El Camino Way, PO Box 10139, Palo Alto, CA 94303-0139,
	USA}
}

@article{wilson1984passive,
  title={Passive cable properties of dendritic spines and spiny neurons},
  author={Wilson, Charles J},
  journal={The Journal of Neuroscience},
  volume={4},
  number={1},
  pages={281--297},
  year={1984},
  publisher={Soc Neuroscience}
}

@BOOK{witten2005data,
  author = {Witten, Ian H. and Frank, Eibe},
  title = {Data Mining: Practical Machine Learning Tools and Techniques},
  year = {2005},
  publisher = {Morgan Kaufmann}
}

@ARTICLE{wittenbrink2011fermi,
  author = {Wittenbrink, C.M. and Kilgariff, E. and Prabhu, A.},
  title = {Fermi GF100 GPU architecture},
  journal = {Micro, IEEE},
  year = {2011},
  volume = {31},
  pages = {50--59},
  number = {2},
  owner = {simon},
  publisher = {IEEE},
  timestamp = {2013.01.17}
}

@ARTICLE{wolf03decade,
  author = {Wolf, W.},
  title = {A decade of hardware/software co-design},
  journal = {IEEE Computer},
  year = {2003},
  pages = {38--43}
}

@ARTICLE{wolpert97nofree,
  author = {Wolpert, David H. and Macready, William G. },
  title = {No Free Lunch Theorems for Optimization},
  journal = {IEEE Transactions on Evolutionary Computation},
  year = {1997},
  volume = {1},
  pages = {67--82},
  number = {1},
  month = {April},
  url = {citeseer.ist.psu.edu/wolpert96no.html}
}

@ARTICLE{worgotter05tslreview,
  author = {Worgotter, Florentin and Porr, Bernd},
  title = {{Temporal Sequence Learning, Prediction, and Control: A Review of
	Different Models and Their Relation to Biological Mechanisms}},
  journal = {Neural Comp.},
  year = {2005},
  volume = {17},
  pages = {245-319},
  number = {2},
  abstract = {In this review, we compare methods for temporal sequence learning
	(TSL) across the disciplines machine-control, classical conditioning,
	neuronal models for TSL as well as spike-timing-dependent plasticity
	(STDP). This review introduces the most influential models and focuses
	on two questions: To what degree are reward-based (e.g., TD learning)
	and correlation-based (Hebbian) learning related? and How do the
	different models correspond to possibly underlying biological mechanisms
	of synaptic plasticity? We first compare the different models in
	an open-loop condition, where behavioral feedback does not alter
	the learning. Here we observe that reward-based and correlation-based
	learning are indeed very similar. Machine control is then used to
	introduce the problem of closed-loop control (e.g., actor-critic
	architectures). Here the problem of evaluative (rewards) versus nonevaluative
	(correlations) feedback from the environment will be discussed, showing
	that both learning approaches are fundamentally different in the
	closed-loop condition. In trying to answer the second question, we
	compare neuronal versions of the different learning architectures
	to the anatomy of the involved brain structures (basal-ganglia, thalamus,
	and cortex) and the molecular biophysics of glutamatergic and dopaminergic
	synapses. Finally, we discuss the different algorithms used to model
	STDP and compare them to reward-based learning rules. Certain similarities
	are found in spite of the strongly different timescales. Here we
	focus on the biophysics of the different calcium-release mechanisms
	known to be involved in STDP.},
  eprint = {http://neco.mitpress.org/cgi/reprint/17/2/245.pdf},
  keywords = {plasticity learning spiking},
  url = {http://neco.mitpress.org/cgi/content/abstract/17/2/245}
}

@ARTICLE{wu2022improved,
  title = {Improved Generalization in Signal Identification with Unsupervised Spiking Neuron Networks for Fiber-optic Distributed Acoustic Sensor},
  author = {Wu, Huijuan and Gan, Dengke and Xu, Chenrui and Liu, Yimeng and Liu, Xinyu and Song, Yuanfeng and Rao, Yun-Jiang},
  journal = {Journal of Lightwave Technology},
  year = 2022,
  publisher = {IEEE},
  number={9},
  pages={3072--3083},
  doi={10.1109/JLT.2022.3144147}
}

@MASTERSTHESIS{wunderlich2016bachelor,
  author = "Timo Wunderlich",
  year = 2016,
  title = "Synaptic Calibration on the {HICANN-DLS} Neuromorphic Chip",
  type = {Bachelor thesis},
  school = {Ruprecht-Karls-Universit{\"a}t Heidelberg}
}

@ARTICLE{wunderlich2019demonstrating, crossref={wunderlich2019advantages}}
@ARTICLE{wunderlich2019advantages,
	AUTHOR={Wunderlich, Timo and Kungl, Akos F. and M{\"u}ller, Eric and Hartel, Andreas and Stradmann, Yannik and Aamir, Syed Ahmed and Gr{\"u}bl, Andreas and Heimbrecht, Arthur and Schreiber, Korbinian and St{\"o}ckel, David and Pehle, Christian and Billaudelle, Sebastian and Kiene, Gerd and Mauch, Christian and Schemmel, Johannes and Meier, Karlheinz and Petrovici, Mihai A.},
	TITLE={Demonstrating Advantages of Neuromorphic Computation: A Pilot Study},
	JOURNAL={Frontiers in Neuroscience},
	VOLUME={13},
	PAGES={260},
	YEAR={2019},
	URL={https://www.frontiersin.org/article/10.3389/fnins.2019.00260},
	DOI={10.3389/fnins.2019.00260},
	ISSN={1662-453X},
	ABSTRACT={Neuromorphic devices represent an attempt to mimic aspects of the brain's architecture and dynamics with the aim of replicating its hallmark functional capabilities in terms of computational power, robust learning and energy efficiency. We employ a single-chip prototype of the {BrainScaleS} 2 neuromorphic system to implement a proof-of-concept demonstration of reward-modulated spike-timing-dependent plasticity in a spiking network that learns to play a simplified version of the Pong video game by smooth pursuit. This system combines an electronic mixed-signal substrate for emulating neuron and synapse dynamics with an embedded digital processor for on-chip learning, which in this work also serves to simulate the virtual environment and learning agent. The analog emulation of neuronal membrane dynamics enables a 1000-fold acceleration with respect to biological real-time, with the entire chip operating on a power budget of 57~mW. Compared to an equivalent simulation using state-of-the-art software, the on-chip emulation is at least one order of magnitude faster and three orders of magnitude more energy-efficient. We demonstrate how on-chip learning can mitigate the effects of fixed-pattern noise, which is unavoidable in analog substrates, while making use of temporal variability for action exploration. Learning compensates imperfections of the physical substrate, as manifested in neuronal parameter variability, by adapting synaptic weights to match respective excitability of individual neurons.}
}

@ARTICLE{wunderlich2019demonstrating_nourl,
	AUTHOR={Wunderlich, Timo and Kungl, Akos F. and M{\"u}ller, Eric and Hartel, Andreas and Stradmann, Yannik and Aamir, Syed Ahmed and Gr{\"u}bl, Andreas and Heimbrecht, Arthur and Schreiber, Korbinian and St{\"o}ckel, David and Pehle, Christian and Billaudelle, Sebastian and Kiene, Gerd and Mauch, Christian and Schemmel, Johannes and Meier, Karlheinz and Petrovici, Mihai A.},
	TITLE={Demonstrating Advantages of Neuromorphic Computation: A Pilot Study},
	JOURNAL={Frontiers in Neuroscience},
	VOLUME={13},
	PAGES={260},
	YEAR={2019},
	DOI={10.3389/fnins.2019.00260},
	ISSN={1662-453X}
}

@article{wunderlich2021event,
  title={Event-based backpropagation can compute exact gradients for spiking neural networks},
  author={Wunderlich, Timo C and Pehle, Christian},
  journal={Scientific Reports},
  volume=11,
  number=1,
  pages={1--17},
  year=2021,
  publisher={Nature Publishing Group},
  doi={10.1038/s41598-021-91786-z}
}

@MANUAL{xenomai_homepage,
  key = {Xenomai},
  organization = {Xenomai},
  title = {Real-Time Framework for Linux},
  url = {http://www.xenomai.org/},
  year = 2014,
  note = {[Online; accessed: 2014-04-29]}
}

@ARTICLE{xu96multiphoton,
  author = {Xu, C. and Zipfel, W. and Shear, J. B. and Williams, R. M. and Webb,
	W. W. },
  title = {{M}ultiphoton fluorescence excitation: new spectral windows for biological
	nonlinear microscopy},
  journal = {Proc. Natl. Acad. Sci. U.S.A.},
  year = {1996},
  volume = {93},
  pages = {10763--10768},
  month = {Oct},
  abstract = { Intrinsic, three-dimensionally resolved, microscopic imaging of dynamical
	structures and biochemical processes in living preparations has been
	realized by nonlinear laser scanning fluorescence microscopy. The
	search for useful two-photon and three-photon excitation spectra,
	motivated by the emergence of nonlinear microscopy as a powerful
	biophysical instrument, has now discovered a virtual artist's palette
	of chemical indicators, fluorescent markers, and native biological
	fluorophores, including NADH, flavins, and green fluorescent proteins,
	that are applicable to living biological preparations. More than
	25 two-photon excitation spectra of ultraviolet and visible absorbing
	molecules reveal useful cross sections, some conveniently blue-shifted,
	for near-infrared absorption. Measurements of three-photon fluorophore
	excitation spectra now define alternative windows at relatively benign
	wavelengths to excite deeper ultraviolet fluorophores. The inherent
	optical sectioning capability of nonlinear excitation provides three-dimensional
	resolution for imaging and avoids out-of-focus background and photodamage.
	Here, the measured nonlinear excitation spectra and their photophysical
	characteristics that empower nonlinear laser microscopy for biological
	imaging are described. }
}

@ARTICLE{xue07decoupled,
  author = {Xue, Yanbo and Yang, Le and Haykin, Simon},
  title = {Decoupled echo state networks with lateral inhibition},
  journal = {Neural Networks},
  year = {2007},
  volume = {20},
  pages = {365--376},
  number = {3},
  month = apr,
  abstract = {Building on some prior work, in this paper we describe a novel structure
	termed the decoupled echo state network (DESN) involving the use
	of lateral inhibition. Two low-complexity implementation schemes,
	namely, the DESN with reservoir prediction (DESN + RP) and DESN with
	maximum available information (DESN + MaxInfo), are developed: (1)
	In the multiple superimposed oscillator (MSO) problem, DESN + MaxInfo
	exhibits three important attributes: lower generalization mean-square
	error (MSE), better robustness with respect to the random generation
	of reservoir weight matrix and feedback connections, and robustness
	to variations in the sparseness of reservoir weight matrix, compared
	to DESN + RP. (2) For a noiseless nonlinear prediction task, DESN
	+ RP outperforms the DESN + MaxInfo and single reservoir-based ESN
	approach in terms of lower prediction MSE and better robustness to
	a change in the number of inputs and sparsity of the reservoir weight
	matrix. Finally, in a real-life prediction task using noisy sea clutter
	data, both schemes exhibit higher prediction accuracy and successful
	design ratio than a conventional ESN with a single reservoir.},
  booktitle = {Echo State Networks and Liquid State Machines},
  keywords = {Echo state network (ESN), Decoupled ESN (DESN), DESN with reservoir
	prediction (DESN + RP), DESN with maximum available information (DESN
	+ MaxInfo), Multiple superimposed oscillator (MSO), Predictive modeling,,
	liquid},
  owner = {mreuss},
  timestamp = {2007.06.14},
  url = {http://www.sciencedirect.com/science/article/B6T08-4NMWR88-8/2/ad2b77320d205c249ea472001c6ef64d}
}

@ARTICLE{yamaguchi2019energy,
    title={An Energy-efficient Time-domain Analog VLSI Neural Network Processor Based on a Pulse-width Modulation Approach},
    author={Masatoshi Yamaguchi and Goki Iwamoto and Hakaru Tamukoh and Takashi Morie},
    year={2019},
    eprint={1902.07707},
    archivePrefix={arXiv},
    primaryClass={cs.ET},
    journal={arXiv preprint},
    url={https://arxiv.org/abs/1902.07707}
}

@ARTICLE{yamazaki07cerebellum,
  author = {Yamazaki, Tadashi and Tanaka, Shigeru},
  title = {The cerebellum as a liquid state machine},
  journal = {Neural Networks},
  year = {2007},
  volume = {20},
  pages = {290--297},
  number = {3},
  month = apr,
  abstract = {We examined closely the cerebellar circuit model that we have proposed
	previously. The model granular layer generates a finite but very
	long sequence of active neuron populations without recurrence, which
	is able to represent the passage of time. For all the possible binary
	patterns fed into mossy fibres, the circuit generates the same number
	of different sequences of active neuron populations. Model Purkinje
	cells that receive parallel fiber inputs from neurons in the granular
	layer learn to stop eliciting spikes at the timing instructed by
	the arrival of signals from the inferior olive. These functional
	roles of the granular layer and Purkinje cells are regarded as a
	liquid state generator and readout neurons, respectively. Thus, the
	cerebellum that has been considered to date as a biological counterpart
	of a perceptron is reinterpreted to be a liquid state machine that
	possesses powerful information processing capability more than a
	perceptron.},
  booktitle = {Echo State Networks and Liquid State Machines},
  keywords = {Cerebellum, Perceptron, Liquid state machine, Spatiotemporal activity
	patterns, Sparse coding, Recurrent inhibitory network, Long-term
	depression, Hybrid network, liquid},
  owner = {mreuss},
  timestamp = {2007.06.14}
}

@article{yamazaki2021humanscale,
	title = {Human-scale Brain Simulation via Supercomputer: A Case Study on the Cerebellum},
	journal = {Neuroscience},
	volume = 462,
	pages = {235--246},
	year = 2021,
	note = {In Memoriam: Masao Ito—A Visionary Neuroscientist with a Passion for the Cerebellum},
	issn = {0306-4522},
	doi = {10.1016/j.neuroscience.2021.01.014},
	author = {Tadashi Yamazaki and Jun Igarashi and Hiroshi Yamaura},
}

@ARTICLE{yao04mechanism,
  author = {Haishan Yao and Yaosong Shen and Yang Dan},
  title = {Intracortical mechanism of stimulus-timing-dependent plasticity in
	visual cortical orientation tuning},
  journal = {PNAS},
  year = {2004},
  volume = {101},
  pages = {5081-5086},
  number = {14},
  month = {April},
  abstract = {Visual stimuli are known to induce various changes in the receptive
	field properties of adult cortical neurons, but the underlying mechanisms
	are not well understood. Repetitive pairing of stimuli at two orientations
	can induce a shift in cortical orientation tuning, with the direction
	and magnitude of the shift depending on the temporal order and interval
	between the pair. Although the temporal specificity of the effect
	on the order of tens of milliseconds strongly suggests spike-timing-dependent
	synaptic plasticity (STDP) as the underlying mechanism, it remains
	unclear whether the modification occurs within the cortex or at earlier
	stages of the visual pathway. In the present study, we examined the
	involvement of an intracortical mechanism in this functional modification.
	First, we measured interocular transfer of the shift induced by monocular
	conditioning. We found complete transfer of the effect at both the
	physiological and psychophysical levels, indicating that the modification
	occurs largely in the cortex. Second, we analyzed the spike timing
	of cortical neurons during conditioning and found it commensurate
	with the requirement of STDP. Finally, we compared the measured shift
	in orientation tuning with the prediction of a model circuit that
	exhibits STDP at intracortical connections. This model can account
	for not only the temporal specificity of the effect but also the
	dependence of the shift on both orientations in the conditioning
	pair. These results indicate that modification of intracortical connections
	is a key mechanism in the stimulus-timing-dependent plasticity in
	orientation tuning.},
  file = {yao04mechanism.pdf:yao04mechanism.pdf:PDF},
  keywords = {spiking plasticity},
  owner = {mreuss}
}

@ARTICLE{yao99evolving,
  author = {Yao, X.},
  title = {Evolving Artificial Neural Networks},
  journal = {Proceedings of the IEEE},
  year = {1999},
  volume = {87},
  pages = {1423--1447},
  number = {9},
  booktitle = {Proceedings of the IEEE},
  file = {yao99evolving.pdf:yao99evolving.pdf:PDF},
  url = {citeseer.ist.psu.edu/yao99evolving.html}
}

@ARTICLE{yao98population,
  author = {Yao, X. and Liu, Y.},
  title = {Making Use of Population Information in Evolutionary Neural Networks},
  journal = {IEEE Transactions on Systems, Man and Cybernetics, Part B: Cybernetics},
  year = {1998},
  volume = {28},
  pages = {417--425 },
  number = {3},
  booktitle = {IEEE Transactions on Systems, Man and Cybernetics, Part B: Cybernetics},
  file = {yao98population.pdf:yao98population.pdf:PDF}
}

@ARTICLE{yao97system,
  author = {Yao, X. and Liu, Y.},
  title = {A New Evolutionary System for Evolving Artificial Neural Networks},
  journal = {IEEE Transactions on Neural Networks},
  year = {1997},
  volume = {8},
  pages = {694--713},
  number = {3},
  month = {May},
  booktitle = {IEEE Transactions on Neural Networks},
  file = {yao97system.pdf:yao97system.pdf:PDF}
}

@INPROCEEDINGS{yao_icec96ensemble,
  author = {Yao, X. and Liu, Y.},
  title = {Ensemble Structure of Evolutionary Artificial Neural Networks},
  booktitle = {Proceedings of the Third IEEE International Conference on Evolutionary
	Computation ({ICEC}'96)},
  year = {1996},
  pages = {659--664},
  address = {Nagoya, Japan},
  month = {May},
  file = {yao_icec96ensemble.pdf:yao_icec96ensemble.pdf:PDF}
}

@article{yavuz2016genn,
  title={{GeNN}: a code generation framework for accelerated brain simulations},
  author={Yavuz, Esin and Turner, James and Nowotny, Thomas},
  journal={Scientific reports},
  volume=6,
  number=1,
  pages={1--14},
  year=2016,
  publisher={Nature Publishing Group}
}

@ARTICLE{yazdanbakhsh02,
  author = {Arash Yazdanbakhsh and Baktash Babadi and Shahin Rouhani and Ehsan
	Arabzadeh and Abdolhosein Abbassian},
  title = {New attractor states for synchronous activity in synfire chains with
	excitatory and inhibitory coupling},
  journal = {Biological Cybernetics},
  year = {2002},
  pages = {367-378}
}

@ARTICLE{yeomans1979absolute,
  author = {Yeomans, J.S.},
  title = {The absolute refractory periods of self-stimulation neurons},
  journal = {Physiology \& Behavior},
  year = {1979},
  volume = {22},
  pages = {911--919},
  number = {5},
  publisher = {Elsevier}
}

@INPROCEEDINGS{yin20171,
  title={A 1.06-to-5.09 {TOPS/W} reconfigurable hybrid-neural-network processor for deep learning applications},
  author={Yin, Shouyi and Ouyang, Peng and Tang, Shibin and Tu, Fengbin and Li, Xiudong and Liu, Leibo and Wei, Shaojun},
  booktitle={2017 Symposium on VLSI Circuits},
  pages={C26--C27},
  year={2017},
  organization={IEEE}
}

@ARTICLE{yuille89,
  author = {Alan L. Yuille and Norberto M. Grzywacz},
  title = {A winner-take-all mechanism based on presynaptic inhibition feedback},
  journal = {Neural Comput.},
  year = {1989},
  volume = {1},
  pages = {334--347},
  number = {3},
  address = {Cambridge, MA, USA},
  doi = {http://dx.doi.org/10.1162/neco.1989.1.3.334},
  issn = {0899-7667},
  publisher = {MIT Press}
}

@BOOK{zell94simulation,
  title = {Simulation neuronaler Netze},
  publisher = {Addison-Wesley Germany GmbH},
  year = {1994},
  author = {A. Zell},
  edition = {1st Ed},
  owner = {fieres}
}

@ARTICLE{zenke2014auryn,
	AUTHOR={Zenke, Friedemann  and  Gerstner, Wulfram},
	TITLE={Limits to high-speed simulations of spiking neural networks using general-purpose computers},
	JOURNAL={Frontiers in Neuroinformatics},
	VOLUME=8,
	YEAR=2014,
	NUMBER=76,
	URL={http://www.frontiersin.org/neuroinformatics/10.3389/fninf.2014.00076/abstract},
	DOI={10.3389/fninf.2014.00076},
	ISSN={1662-5196},
	ABSTRACT={To understand how the central nervous system performs computations using recurrent neuronal circuitry, simulations have become an indispensable tool for theoretical neuroscience. To study neuronal circuits and their ability to self-organize, increasing attention has been directed toward synaptic plasticity. In particular spike-timing-dependent plasticity (STDP) creates specific demands for simulations of spiking neural networks. On the one hand a high temporal resolution is required to capture the millisecond timescale of typical STDP windows. On the other hand network simulations have to evolve over hours up to days, to capture the timescale of long-term plasticity. To do this efficiently, fast simulation speed is the crucial ingredient rather than large neuron numbers. Using different medium-sized network models consisting of several thousands of neurons and off-the-shelf hardware, we compare the simulation speed of the simulators: Brian, NEST and Neuron as well as our own simulator Auryn. Our results show that real-time simulations of different plastic network models are possible in parallel simulations in which numerical precision is not a primary concern. Even so, the speed-up margin of parallelism is limited and boosting simulation speeds beyond one tenth of real-time is difficult. By profiling simulation code we show that the run times of typical plastic network simulations encounter a hard boundary. This limit is partly due to latencies in the inter-process communications and thus cannot be overcome by increased parallelism. Overall, these results show that to study plasticity in medium-sized spiking neural networks, adequate simulation tools are readily available which run efficiently on small clusters. However, to run simulations substantially faster than real-time, special hardware is a prerequisite.}
}

@ARTICLE{Zhang2009,
  author = {Zhang, Ji-Chuan and Lau, Pak-Ming and Bi, Guo-Qiang},
  title = {Gain in sensitivity and loss in temporal contrast of STDP by dopaminergic
	modulation at hippocampal synapses},
  journal = {Proceedings of the National Academy of Sciences},
  year = {2009},
  volume = {106},
  pages = {13028--13033},
  number = {31},
  owner = {simon},
  publisher = {National Acad Sciences},
  timestamp = {2013.04.25}
}

@ARTICLE{Zhang1998,
  author = {Li I. Zhang and Huizhong W. Tao and Christine E. Holt and William
	A. Harris and Mu-ming Poo},
  title = {A critical window for cooperation and competition among developing
	retinotectal synapses},
  year = {1998},
  institution = {Nature Publishing Group Metadata Repository [http://www.nature.com/oai/request]
	(Switzerland)},
  publisher = {Nature Publishing Group},
  url = {http://www.scientificcommons.org/53155268}
}

@ARTICLE{zhang_transcomp96,
  author = {Zhang, M. and Vassiliadis, S. and Delgado-Frias, JG.},
  title = {Sigmoid generators for neural computing using piecewise approximation},
  journal = {IEEE Trans Computers},
  year = {1996},
  volume = {45},
  pages = {1045-1049},
  number = {2}
}

@ARTICLE{Zhuang2007,
  author = {Zhuang, Xiaotong and Pande, Santosh},
  title = {Power-efficient prefetching for embedded processors},
  journal = {ACM Trans. Embed. Comput. Syst.},
  year = {2007},
  volume = {6},
  number = {1},
  month = feb,
  acmid = {1210271},
  address = {New York, NY, USA},
  articleno = {3},
  doi = {10.1145/1210268.1210271},
  issn = {1539-9087},
  issue_date = {February 2007},
  keywords = {Data prefetching, bit-differential addressing, embedded processors,
	offset assignment},
  owner = {simon},
  publisher = {ACM},
  timestamp = {2013.02.18},
  url = {http://doi.acm.org/10.1145/1210268.1210271}
}

@MISC{zoglauer09diplomathesis,
  author = {Holter Zoglauer},
  title = { Entwicklung und Testergebnisse eines Prototypensystems f\"ur die
	Wafer-Scale-Integration},
  howpublished = {Diploma thesis (German), University of Heidelberg, HD-KIP-09-28},
  year = {2009},
  keywords = {PCB, brainscales}
}

@article{zoschkeguettler2017rdlembedding,
  author   = {Kai Zoschke and Maurice Guettler and Lars Boettcher and Andreas Gruebl and Dan Husmann and Johannes Schemmel and Karlheinz Meier and Oswin Ehrmann},
  title    = {Full Wafer Redistribution and Wafer Embedding as Key Technologies for a Multi-Scale Neuromorphic Hardware Cluster},
  journal  = {EPTC 2017},
  year     = {2017}
}

@ARTICLE{zou2006analogneurons,
  author = {Zou, Q. and Bornat, Y. and Tomas, J. and Renaud, S. and Destexhe,
	A.},
  title = {Real-time simulations of networks of Hodgkin-Huxley neurons using
	analog circuits},
  journal = {Neurocomputing},
  year = {2006},
  volume = {69},
  pages = {1137-1140}
}

@ARTICLE{zucker02stp,
  author = {Robert S. Zucker and Wade G. Regehr},
  title = {Short-term synaptic plasticity},
  journal = {Annu. Rev. Physiol.},
  year = {2002},
  volume = {64},
  pages = {355--405},
  owner = {bruederl},
  timestamp = {2008.07.18}
}

@MISC{Zucker1995,
  author = {Steve Zucker and Kari Karhi},
  title = {System V Application Binary Interface PowerPC Processor Supplement},
  month = {September},
  year = {1995},
  note = {Revision A},
  owner = {simon},
  timestamp = {2013.03.27}
}

@INPROCEEDINGS{Zyuban2000,
  author = {Zyuban, Victor and Kogge, Peter},
  title = {Optimization of high-performance superscalar architectures for energy
	efficiency},
  booktitle = {Proceedings of the 2000 international symposium on Low power electronics
	and design},
  year = {2000},
  pages = {84--89},
  organization = {ACM},
  owner = {simon},
  timestamp = {2013.04.22}
}

@BOOK{fogel98fossil,
  title = {{E}volutionary {C}omputation: the {F}ossil {R}ecord},
  publisher = {IEEE Press},
  year = {1998},
  editor = {Fogel, D. B.},
  address = {Piscataway, NJ}
}

@BOOK{sakmann95channel,
  title = {Single-channel recording},
  publisher = {Plenum press},
  year = {1995},
  editor = {Sakmann, B. and Neher, E.},
  owner = {bruederl},
  timestamp = {2008.07.18}
}

@BOOK{shepherd04synaptic,
  title = {The Synaptic Organization of the Brain},
  publisher = {Oxford University Press},
  year = {2004},
  editor = {Gordon M. Shepherd},
  address = {198 Madison Avenue, New York, New York},
  edition = {5},
  isbn = {0-19-515955-1}
}

@MISC{ace_library,
  title = {{ACE}: {T}he {A}daptive {C}ommunication {E}nvironment},
  howpublished = {{D}istributed {O}bject {C}omputing ({DOC}) group, Vanderbilt University,
	Nashville, Washington University, and University of California, Irvine,
	\\\texttt{http://www.cs.wustl.edu/~schmidt/ACE.html}},
  groupsearch = {0},
  key = {acelibrary},
  keywords = {spec}
}

@MISC{amdati_homepage,
  title = {ATI Technologies ULC Website:},
  howpublished = {\texttt{http://www.ati.com}},
  key = {amdati}
}

@MISC{directxdevel,
  title = {Microsoft DirectX Developer Center Website:},
  howpublished = {\texttt{http://msdn.microsoft.com/directx}},
  key = {directxdevel}
}

@MISC{gcc_compiler,
  title = {{T}he {GNU} {C}ompiler {C}ollection },
  howpublished = {Website},
  note = {Free Software Foundation Inc. 59 Temple Place Boston MA, USA},
  address = {59 Temple Place, Boston, MA, USA},
  groupsearch = {0},
  keywords = {spec},
  url = {http://gcc.gnu.org/}
}


@software{gcc_compiler_8_1,
  title = {{T}he {GNU} {C}ompiler {C}ollection 8.1},
  author={{{GNU} {P}roject}},
  howpublished = {Website},
  note = {Free Software Foundation Inc.},
  address = {59 Temple Place, Boston, MA, USA},
  year=2018,
  url = {http://gcc.gnu.org/gcc-8}
}

@MISC{heise_intel_14nm,
  title = {Intel: Keine Verzögerungen bei 14-Nanometer-Fertigung},
  owner = {simon},
  timestamp = {2013.05.05},
  url = {http://heise.de/-1769893}
}

@MISC{intel_compiler,
  title = {{I}ntel {C}ompiler for {L}inux},
  howpublished = {{I}ntel {I}nc., \\ \texttt{http://www.intel.com/software/products/compilers/clin/}},
  groupsearch = {0},
  key = {intelcompiler},
  keywords = {spec}
}

@MISC{nvidia_homepage,
  title = {NVidia Corp. Website:},
  howpublished = {\texttt{http://www.nvidia.com}},
  key = {nvidiahomepage}
}

@MISC{stl_homepage,
  title = {{Silicon Graphics, Inc. Standard Template Library Website:}},
  howpublished = {\texttt{http://www.sgi.com/tech/stl/}},
  key = {stlhomepage}
}

@MANUAL{xilinx_virtexe,
  title = {Virtex E Datasheet},
  organization = {Xilinx, Inc.},
  address = {2100 Logic Drive, San Jose, CA 95124-3400, USA},
  annote = {fpgas},
  groupsearch = {0},
  keywords = {spec}
}

@MISC{braininets_d47,
  title = {Brain-i-Nets Deliverable D4.7},
  year = {2012},
  owner = {simon},
  timestamp = {2013.01.16}
}

@MISC{braininets_m11,
  title = {Brain-i-Nets Milestone 11},
  year = {2012},
  owner = {simon},
  timestamp = {2013.01.16}
}

@MANUAL{MicroblazeRefGuide2012,
  title = {MicroBlaze Processor Reference Guide},
  organization = {Xilinx, Inc.},
  month = {January},
  year = {2012},
  note = {version 13.4},
  owner = {simon},
  timestamp = {2013.04.15},
  url = {http://www.xilinx.com/support/documentation/sw_manuals/xilinx13_4/mb_ref_guide.pdf}
}

@MISC{ncsim,
  title = {Incisive},
  howpublished = {Cadence Design Systems},
  year = {2012},
  institution = {Cadence Design Systems},
  owner = {simon},
  timestamp = {2012.11.04},
  url = {http://www.cadence.com/products/fv/Pages/default.aspx}
}

@MANUAL{plan_ahead_134,
  title = {PlanAhead User Guide},
  organization = {Xilinx, Inc.},
  month = {January},
  year = {2012},
  note = {v13.4},
  owner = {simon},
  timestamp = {2013.04.26}
}

@MISC{rtlinuxwiki,
  title = {{R}eal-{T}ime {L}inux {W}iki},
  howpublished = {Website},
  month = {10},
  year = {2012},
  owner = {simon},
  timestamp = {2012.10.28},
  url = {https://rt.wiki.kernel.org}
}

@MANUAL{synplify_premier,
  title = {Synplify Premier Fast, Reliable FPGA Implementation and Debug},
  organization = {Synopsys, Inc.},
  address = {700 East Middlefield Road, Mountain View, CA 94043},
  year = {2012},
  owner = {simon},
  timestamp = {2013.04.26}
}

@MISC{braininets_y2,
  title = {Brain-i-Nets year 2 periodic report},
  year = {2011},
  owner = {simon},
  timestamp = {2013.01.16}
}

@MANUAL{aurora_proto_spec,
  title = {Aurora 8B/10B Protocol Specification},
  organization = {Xilinx, Inc.},
  address = {www.xilinx.com},
  year = {2010},
  groupsearch = {0},
  keywords = {spec}
}

@MISC{braininets_y1,
  title = {Brain-i-Nets Year 1 periodic report},
  year = {2010},
  owner = {simon},
  timestamp = {2013.01.16}
}

@MANUAL{virtex4_ppc405,
  title = {Virtex-4 FPGA Embedded Processor Block with PowerPC 405 Processor},
  organization = {Xilinx, Inc.},
  month = {April},
  year = {2009},
  note = {version 2.01b},
  owner = {simon},
  timestamp = {2013.04.15},
  url = {http://www.xilinx.com/support/documentation/ip_documentation/ppc405_virtex4.pdf}
}

@MANUAL{xilinx_gtp_ug,
  title = {Virtex-5 FPGA RocketIO GTP Transceiver User Guide},
  organization = {Xilinx, Inc.},
  address = {www.xilinx.com},
  year = {2009},
  groupsearch = {0},
  keywords = {spec}
}

@MANUAL{xilinx_viretx5_ug,
  title = {Virtex-5 FPGA User Guide},
  organization = {Xilinx, Inc.},
  year = {2009},
  groupsearch = {0},
  keywords = {spec},
  url = {http://www.xilinx.com}
}

@MANUAL{xilinx_mpmc_ug,
  title = {LogiCORE IP Multi-Port Memory
  Controller (MPMC)},
  organization = {Xilinx, Inc.},
  year = {2011},
  url = {http://www.xilinx.com}
}

@MANUAL{xilinx_virtex5_switching,
  title = {Virtex-5 FPGA Data Sheet: DC and Switching Characteristics},
  organization = {Xilinx, Inc.},
  address = {www.xilinx.com},
  year = {2009},
  groupsearch = {0},
  keywords = {spec}
}

@MANUAL{ml505_ug_2008,
  title = {ML505/ML506/M ML505/ML506/ML507 Evaluation Platform User Guide},
  organization = {Xilinx, Inc.},
  month = {November},
  year = {2008},
  note = {v3.1},
  owner = {simon},
  timestamp = {2013.04.26}
}

@MISC{ocp,
  author = {{OCP}},
  title = {Open Core Protocol Specification 2.2},
  year = {2008},
  company = {OCP IP},
  url = {http://www.ocpip.org/home}
}

@MISC{echostates07editorial,
  title = {Editorial Board},
  month = apr,
  year = {2007},
  booktitle = {Echo State Networks and Liquid State Machines},
  journal = {Neural Networks},
  keywords = {liquid},
  number = {3},
  owner = {mreuss},
  pages = {IFC--},
  timestamp = {2007.06.14},
  url = {http://www.sciencedirect.com/science/article/B6T08-4NWWPVY-1/2/e24bbe24def38c67dc627f010ee7d9ea},
  volume = {20}
}

@PROCEEDINGS{iscas2007,
  title = {International Symposium on Circuits and Systems (ISCAS 2007), 27-20
	May 2007, New Orleans, Louisiana, USA},
  year = {2007},
  publisher = {IEEE}
}

@techreport{ISO14882cxx,
  type = {Standard},
  key = {ISO 14882:2017},
  month = dec,
  year = 2017,
  title = {Programming languages --- C++},
  volume = 2017,
  address = {Geneva, Swiss},
  institution = {International Organization for Standardization}
}

@TECHREPORT{iso7498,
    abstract = {{Cancels and replaces the first edition (1984). The model provides a common basis for the coordination of standards development for the purpose of systems interconnection, while allowing existing standards to be placed into perspective within the overall Reference Model. The model identifies areas for developing or improving standards. It does not intend to serve as an implementation specification.}},
    address = {Geneva, Switzerland},
    day = {15},
    howpublished = {Also published as ITU-T Recommendation X.200},
    institution = {ISO},
    key = {ISO/IEC 7498-1:1994},
    keywords = {design, iec, iso, model, network, osi, standard},
    month = nov,
    number = {7498-1:1994},
    publisher = {ISO},
    title = {{Information Technology --- Open Systems Interconnection --- Basic Reference Model: The Basic Model}},
    type = {ISO/IEC},
    url = {http://www.iso.org/iso/iso\_catalogue/catalogue\_tc/catalogue\_detail.htm?csnumber=20269},
    year = {1994}
}

@MANUAL{xilinx_cpld,
  title = {Xilinx XC9536XL High Performance CPLD},
  organization = {Xilinx Inc.},
  address = {San Jose},
  month = {September},
  year = {2004}
}

@MANUAL{xilinx_ppc405_blockref,
  title = {{P}ower{PC} 405 processor block reference guide},
  organization = {Xilinx, Inc.},
  address = {www.xilinx.com},
  month = {August},
  year = {2004},
  keywords = {spec}
}

@MANUAL{windriver6,
  title = {Windriver 6 User's Manual},
  organization = {Jungo Ltd.},
  address = {Netanya},
  year = {2003}
}

@MANUAL{xilinxtrimac,
  title = {LogiCORE IP
  Tri-Mode Ethernet
  MAC v4.5 User Guide},
  organization = {Xilinx, Inc.},
  year = {2011}
}

@MANUAL{xilinx_ppc_ref,
  title = {{P}ower{PC} processor reference guide},
  organization = {Xilinx, Inc.},
  address = {www.xilinx.com},
  month = {September},
  year = {2003},
  keywords = {spec}
}

@MANUAL{micron_ddr-sdram,
  title = {Small-Outline DDR SDRAM Module},
  organization = {Micron Technology, Inc.},
  address = {www.micron.com},
  month = {Jan},
  year = {2002},
  groupsearch = {0},
  keywords = {spec}
}

@MANUAL{xilinx_handbook_vII-pro,
  title = {Virtex-{II} Pro Platform FPGA Handbook},
  organization = {Xilinx, Inc.},
  address = {www.xilinx.com},
  year = {2002},
  groupsearch = {0},
  keywords = {spec}
}

@ARTICLE{jtagieee,
  title = {{IEEE Standard Test Access Port and Boundary-Scan Architecture}},
  publisher = {Institute of Electrical and Electronics Engineers},
  author = {IEEE},
  journal = {{IEEE} Std 1149.1-2001},
  year = {2001},
  pages = {i -200},
  month = { },
  abstract = {Circuitry that may be built into an integrated circuit to assist in
	the test, maintenance, and support of assembled printed circuit boards
	is defined. The circuitry includes a standard interface through which
	instructions and test data are communicated. A set of test features
	is defined, including a boundary-scan register, such that the component
	is able to respond to a minimum set of instructions designed to assist
	with testing of assembled printed circuit boards. Also, a language
	is defined that allows rigorous description of the component-specific
	aspects of such testability features.},
  doi = {10.1109/IEEESTD.2001.92950}
}

@MANUAL{xilinx_handbook_vII,
  title = {Virtex-{II} 1.5V {F}ield-{P}rogrammable {G}ate {A}rrays},
  organization = {Xilinx, Inc.},
  address = {www.xilinx.com},
  month = {October},
  year = {2001},
  note = {DS031-1},
  groupsearch = {0},
  key = {xilinxhandbookvII},
  keywords = {spec}
}

@MANUAL{plx_9054,
  title = {PLX 9054 Data Book},
  organization = {PLX Technology, Inc.},
  address = {Sunnyvale},
  edition = {Version 2.1},
  month = {January},
  year = {2000},
  annote = {plx chip on darkwing},
  keywords = {spec}
}

@MANUAL{cpp_standard,
  title = {Programming Language {C++}},
  organization = {ISO/IEC 14882},
  month = {July},
  year = {1998},
  groupsearch = {0},
  key = {cpp},
  keywords = {spec}
}

@MANUAL{vhdl_reference97,
  title = {{VHDL} Language Reference Manual, IEEE Std. 1076.1},
  organization = {Design Automation Standards Committee of the IEEE Computer Society},
  address = {New York},
  year = {1997}
}

@ARTICLE{walt2011numpy,
author={S. {van der Walt} and S. C. {Colbert} and G. {Varoquaux}},
journal={Computing in Science Engineering},
title={The NumPy Array: A Structure for Efficient Numerical Computation},
year=2011,
volume={13},
number={2},
pages={22-30},
keywords={data structures;high level languages;mathematics computing;numerical analysis;numerical computation;numpy array;numerical data;high level language;Python programming language;Arrays;Numerical analysis;Performance evaluation;Computational efficiency;Finite element methods;Vector quantization;Resource management;Python;NumPy;scientific programming;numerical computations;programming libraries},
doi={10.1109/MCSE.2011.37},
ISSN={1558-366X},
month=3
}

@MASTERSTHESIS{jeltsch10diplomathesis,
  author = {Jeltsch, Sebastian},
  title = {Computing with Transient States on a Neuromorphic Multi-Chip Environment},
  year = 2010,
  type = {Diploma thesis},
  school = {Ruprecht-Karls-Universit{\"a}t Heidelberg},
  note = {HD-KIP-10-54},
}

@phdthesis{jeltsch2014phdthesis,
  author   = {Sebastian Jeltsch},
  title    = {A Scalable Workflow for a Configurable Neuromorphic Platform},
  school   = {Universit\"at Heidelberg},
  year     = {2014}
}

@MASTERSTHESIS{kaplan08diplomathesis,
  author = {Kaplan, Bernhard},
  title = {Self-Organization Experiments for a Neuromorphic Hardware Device},
  year = 2008,
  type = {Diploma thesis},
  school = {Ruprecht-Karls-Universit{\"a}t Heidelberg},
  note = {HD-KIP-08-42},
}

@MISC{mattermost,
  author = {{Mattermost, Inc}},
  title = {Mattermost: Open Source, Self-hosted Slack Alternative},
  url = {https://mattermost.com},
}

@ARTICLE{moradi2014eventbased,
author={S. {Moradi} and G. {Indiveri}},
journal={IEEE Transactions on Biomedical Circuits and Systems},
title={An Event-Based Neural Network Architecture With an Asynchronous Programmable Synaptic Memory},
year=2014,
volume={8},
number={1},
pages={98-107},
keywords={asynchronous circuits;biomedical electronics;current-mode circuits;digital-analogue conversion;learning (artificial intelligence);neural nets;neurophysiology;protocols;SRAM chips;transceivers;VLSI;event-based neural network architecture;asynchronous programmable synaptic memory;hybrid analog-digital VLSI implementation;VLSI;spiking neural network;synaptic weight values;static random access memory;SRAM module;current-mode event-driven DAC;current-mode integrator synapses;synapse output currents;transceiver;digital asynchronous events;communication protocol;address event representation;spike-timing dependent plasticity learning algorithms;Address event representation (AER);analog/digital;asynchronous;circuit;event-based;learning;neural network;neuromorphic;programmable weights;real-time;sensory-motor;silicon neuron;silicon synapse;spike-timing dependent plasticity (STDP);spiking;static random access memory (SRAM);synaptic dynamics;very large scale integration (VLSI);Algorithms;Electrical Equipment and Supplies;Equipment Design;Models, Neurological;Neural Networks (Computer);Synapses},
doi={10.1109/TBCAS.2013.2255873},
ISSN={1940-9990},
month=2
}

@ARTICLE{moradi2018dynaps,
  title     = "A scalable multicore architecture with heterogeneous memory
               structures for Dynamic Neuromorphic Asynchronous Processors
               ({DYNAPs})",
  author    = "Moradi, Saber and Qiao, Ning and Stefanini, Fabio and Indiveri,
               Giacomo",
  journal   = "IEEE Trans. Biomed. Circuits Syst.",
  publisher = "IEEE",
  volume    =  12,
  number    =  1,
  pages     = "106--122",
  year      =  2018
}

@MASTERSTHESIS{mueller11diplomathesis,
  author = {M{\"u}ller, Paul},
  title = {Distortions of Neural Network Models Induced by Their Emulation on Neuromorphic Hardware Devices},
  year = 2011,
  type = {Diploma thesis},
  school = {Ruprecht-Karls-Universit{\"a}t Heidelberg},
  note = {HD-KIP-11-172},
}

@MASTERSTHESIS{mueller08diplomathesis,
  author = {M{\"u}ller, Eric},
  title = {Operation Of An Imperfect Neuromorphic Hardware Device},
  year = 2008,
  type = {Diploma thesis},
  school = {Ruprecht-Karls-Universit{\"a}t Heidelberg},
  note = {HD-KIP-08-43},
}

@MASTERSTHESIS{vpetkov12diplomathesis,
  author = {Petkov, Venelin},
  title = {Toward Belief Propagation on Neuromorphic Hardware},
  year = 2012,
  type = {Diploma thesis},
  school = {Ruprecht-Karls-Universit{\"a}t Heidelberg},
  note = {HD-KIP 12-23},
}

@MASTERSTHESIS{schilling10diplomathesis,
  author = {Schilling, Moritz},
  title = {A Highly Efficient Transport Layer for the Connection of Neuromorphic Hardware Systems},
  year = 2010,
  type = {Diploma thesis},
  school = {Ruprecht-Karls-Universit{\"a}t Heidelberg},
  note = {HD-KIP-10-09},
}

@MASTERSTHESIS{vogginger10diplomathesis,
  author = {Vogginger, Bernhard},
  title = {Testing the Operation Workflow of a Neuromorphic Hardware System with a Functionally Accurate Model},
  year = 2010,
  type = {Diploma thesis},
  school = {Ruprecht-Karls-Universit{\"a}t Heidelberg},
  note = {HD-KIP-10-12},
}

@MASTERSTHESIS{bill08diplomathesis,
  author = {Bill, Johannes},
  title = {Self-Stabilizing Network Architectures on a Neuromorphic Hardware System},
  year = 2008,
  type = {Diploma thesis},
  school = {Ruprecht-Karls-Universit{\"a}t Heidelberg},
  note = {HD-KIP-08-44},
}

@MASTERSTHESIS{pfeil11diplomathesis,
  author = {Pfeil, Thomas},
  title = {Configuration Strategies for Neurons and Synaptic Learning in Large-Scale Neuromorphic Hardware Systems},
  year = 2011,
  type = {Diploma thesis},
  school = {Ruprecht-Karls-Universit{\"a}t Heidelberg},
  note = {HD-KIP-11-34},
}

@MASTERSTHESIS{friedmann09diplomathesis,
  author = {Friedmann, Simon},
  title = {Extending a Hardware Neural Network Beyond Chip Boundaries},
  year = 2009,
  type = {Diploma thesis},
  school = {Ruprecht-Karls-Universit{\"a}t Heidelberg},
  note = {HD-KIP-09-41},
}

@MASTERSTHESIS{leng2014masterthesis,
  author = {Leng, Luziwei},
  title = {Deep Learning Architectures for Neuromorphic Hardware},
  year = 2014,
  type = {Master thesis},
  school = {Ruprecht-Karls-Universit{\"a}t Heidelberg},
  note = {HD-KIP 14-26},
}

@MASTERSTHESIS{probst2014masterthesis,
  author = {Probst, Dimitri},
  title = {A Neural Implementation of Probabilistic Inference in Binary Probability Spaces},
  year = 2014,
  type = {Master thesis},
  school = {Ruprecht-Karls-Universit{\"a}t Heidelberg},
}

@MASTERSTHESIS{roth2014bachelorthesis,
  author = {Roth, Marco},
  title = {Predictive Stochastic Inference - From Abstract Models to Neuromorphic Implementation},
  year = 2014,
  type = {Bachelor thesis},
  school = {Ruprecht-Karls-Universit{\"a}t Heidelberg},
}

@MASTERSTHESIS{rivkin2014bachelorthesis,
  author = {Rivkin, Boris},
  title = {On the Memory Characteristic of a Cortical Atractor Network},
  year = 2014,
  type = {Bachelor thesis},
  school = {Ruprecht-Karls-Universit{\"a}t Heidelberg},
}

@MASTERSTHESIS{stoeckel2015bachelorthesis,
  author = {St{\"o}ckel, David},
  title = {Boltzmann Sampling with Neuromorphic Hardware},
  year = 2015,
  type = {Bachelor thesis},
  school = {Ruprecht-Karls-Universit{\"a}t Heidelberg},
}

@article{palm2013neural,
  title={Neural associative memories and sparse coding},
  author={Palm, G{\"u}nther},
  journal={Neural Networks},
  volume={37},
  pages={165--171},
  year={2013},
  publisher={Elsevier}
}

@article{faisal2008noise,
  title={Noise in the nervous system},
  author={Faisal, A Aldo and Selen, Luc PJ and Wolpert, Daniel M},
  journal={Nature reviews neuroscience},
  volume={9},
  number={4},
  pages={292--303},
  year={2008},
  publisher={Nature Publishing Group}
}

@inproceedings{you2016neuromorphic,
  title={Neuromorphic implementation of attractor dynamics in decision circuit with NMDARs},
  author={You, Hongzhi and Wang, Dahui},
  booktitle={Circuits and Systems (ISCAS), 2016 IEEE International Symposium on},
  pages={369--372},
  year={2016},
  organization={IEEE}
}

@article{rachmuth2011biophysically,
  title={A biophysically-based neuromorphic model of spike rate-and timing-dependent plasticity},
  author={Rachmuth, Guy and Shouval, Harel Z and Bear, Mark F and Poon, Chi-Sang},
  journal={Proceedings of the National Academy of Sciences},
  volume={108},
  number={49},
  pages={E1266--E1274},
  year={2011},
  publisher={National Acad Sciences}
}

@inproceedings{hussain2016morphological,
  title={Morphological learning in multicompartment neuron model with binary synapses},
  author={Hussain, Shaista and Basu, Arindam},
  booktitle={Circuits and Systems (ISCAS), 2016 IEEE International Symposium on},
  pages={2527--2530},
  year={2016},
  organization={IEEE}
}

@inproceedings{banerjee2015current,
  title={A current-mode spiking neural classifier with lumped dendritic nonlinearity},
  author={Banerjee, Amitava and Kar, Sougata and Roy, Subhrajit and Bhaduri, Aritra and Basu, Arindam},
  booktitle={2015 IEEE International Symposium on Circuits and Systems (ISCAS)},
  pages={714--717},
  year={2015},
  organization={IEEE}
}

@inproceedings{kunkel2013supercomputers,
  title={Supercomputers ready for use as discovery machines for neuroscience},
  author={Kunkel, Susanne and Masumoto, Gen and Fukai, Tomoki and Eppler, Jochen Martin and Plesser, Hans Ekkehard and Igarashi, Jun and Diesmann, Markus and Morrison, Abigail and Schmidt, Maximilian and Helias, Moritz and others},
  booktitle={10th Meeting of the German Neuroscience Society},
  number={FZJ-2013-03827},
  year={2013},
  organization={Computational and Systems Neuroscience}
}

@article{kunkel2014spiking,
  title={Spiking network simulation code for petascale computers},
  author={Kunkel, Susanne and Schmidt, Maximilian and Eppler, Jochen M and Plesser, Hans E and Masumoto, Gen and Igarashi, Jun and Ishii, Shin and Fukai, Tomoki and Morrison, Abigail and Diesmann, Markus and others},
  journal={Frontiers in neuroinformatics},
  volume=8,
  pages=78,
  year=2014,
  publisher={Frontiers}
}

@article{losonczy2006integrative,
  title={Integrative properties of radial oblique dendrites in hippocampal CA1 pyramidal neurons},
  author={Losonczy, Attila and Magee, Jeffrey C},
  journal={Neuron},
  volume={50},
  number={2},
  pages={291--307},
  year={2006},
  publisher={Elsevier}
}

@article{destexhe1994synthesis,
  title={Synthesis of models for excitable membranes, synaptic transmission and neuromodulation using a common kinetic formalism},
  author={Destexhe, Alain and Mainen, Zachary F and Sejnowski, Terrence J},
  journal={Journal of computational neuroscience},
  volume=1,
  number=3,
  pages={195--230},
  year=1994,
  publisher={Springer}
}

@MISC{carbon,
  author = {Graphite Project},
  title = {Carbon},
  url = {https://github.com/graphite-project/carbon},
}

@MISC{logstash,
  author = {elastic},
  title = {Logstash: Collect, Parse, Transform Logs},
  url = {https://www.elastic.co/logstash},
}

@MISC{elasticsearch,
  author = {elastic},
  title = {Elasticsearch: The Official Distributed Search \& Analytics Engine},
  url = {https://www.elastic.co/elasticsearch},
}

@MISC{kibana,
  author = {elastic},
  title = {Kibana: Explore, Visualize, Discover Data},
  url = {https://www.elastic.co/kibana},
}

@techreport{syslog,
  author = {R. Gerhards},
  title = {The Syslog Protocol},
  howpublished = {Internet Requests for Comments},
  type = {RFC},
  number = 5424,
  year = 2009,
  month = 10,
  publisher = {RFC Editor},
  institution = {RFC Editor},
  url = {https://www.rfc-editor.org/rfc/rfc5424.txt}
}

@MISC{hbp-neuromorphic-client,
  author = {Human Brain Project},
  title = {Python client for the Human Brain Project Neuromorphic Computing Platform},
  url = {https://github.com/HumanBrainProject/hbp-neuromorphic-client},
}

@Article{amunts2016hbp,
author={Amunts, Katrin
and Ebell, Christoph
and Muller, Jeff
and Telefont, Martin
and Knoll, Alois
and Lippert, Thomas},
title={The Human Brain Project: Creating a European Research Infrastructure to Decode the Human Brain},
journal={Neuron},
year=2016,
month=11,
day={02},
publisher={Elsevier},
volume={92},
number={3},
pages={574-581},
issn={0896-6273},
doi={10.1016/j.neuron.2016.10.046},
url={https://doi.org/10.1016/j.neuron.2016.10.046}
}

@Article{Einevoll2019scientific,
author={Einevoll, Gaute T.
and Destexhe, Alain
and Diesmann, Markus
and Gr{\"u}n, Sonja
and Jirsa, Viktor
and de Kamps, Marc
and Migliore, Michele
and Ness, Torbj{\o}rn V.
and Plesser, Hans E.
and Sch{\"u}rmann, Felix},
title={The Scientific Case for Brain Simulations},
journal={Neuron},
year=2019,
month=5,
day={22},
publisher={Elsevier},
volume={102},
number={4},
pages={735-744},
issn={0896-6273},
doi={10.1016/j.neuron.2019.03.027},
url={https://doi.org/10.1016/j.neuron.2019.03.027}
}

@BOOK{barr1999embedded,
  author={Barr, Michael},
  title={Programming embedded systems in C and C++},
  edition={1st ed},
  publisher={O'Reilly},
  address={Sebastopol, Calif.},
  year={1999},
  language={eng},
  isbn={978-1-56592-354-6},
}

@Article{serb2020brain,
author={Serb, Alexantrou
and Corna, Andrea
and George, Richard
and Khiat, Ali
and Rocchi, Federico
and Reato, Marco
and Maschietto, Marta
and Mayr, Christian
and Indiveri, Giacomo
and Vassanelli, Stefano
and Prodromakis, Themistoklis},
title={Memristive synapses connect brain and silicon spiking neurons},
journal={Scientific Reports},
year=2020,
volume=10,
number=1,
pages=2590,
abstract={Brain function relies on circuits of spiking neurons with synapses playing the key role of merging transmission with memory storage and processing. Electronics has made important advances to emulate neurons and synapses and brain-computer interfacing concepts that interlink brain and brain-inspired devices are beginning to materialise. We report on memristive links between brain and silicon spiking neurons that emulate transmission and plasticity properties of real synapses. A memristor paired with a metal-thin film titanium oxide microelectrode connects a silicon neuron to a neuron of the rat hippocampus. Memristive plasticity accounts for modulation of connection strength, while transmission is mediated by weighted stimuli through the thin film oxide leading to responses that resemble excitatory postsynaptic potentials. The reverse brain-to-silicon link is established through a microelectrode-memristor pair. On these bases, we demonstrate a three-neuron brain-silicon network where memristive synapses undergo long-term potentiation or depression driven by neuronal firing rates.},
issn={2045-2322},
doi={10.1038/s41598-020-58831-9},
url={https://doi.org/10.1038/s41598-020-58831-9}
}

@MISC{grafana,
  author = {Grafana Labs},
  title = {Grafana: The open observability platform},
  url = {https://grafana.com},
}

@InProceedings{lattner2004llvm,
    Author  = {Chris Lattner and Vikram Adve},
    Title = {{LLVM}: A Compilation Framework for Lifelong Program Analysis and Transformation},
    Address = {San Jose, CA, USA},
    Month = 3,
    Year  = 2004,
    pages = {75--88},
}

@MISC{githubpywrap,
  author = {{Electronic Visions(s), Heidelberg University}},
  title = {pywrap},
  publisher = {GitHub},
  journal = {Github repository},
  url = {https://github.com/electronicvisions/pywrap},
  keywords = {own_software}
}

@MISC{githubhalbe,
  author = {{Electronic Visions(s), Heidelberg University}},
  title = {halbe},
  publisher = {GitHub},
  journal = {Github repository},
  url = {https://github.com/electronicvisions/halbe},
  keywords = {own_software}
}

@MISC{githubhalco,
  author = {{Electronic Visions(s), Heidelberg University}},
  title = {halco},
  publisher = {GitHub},
  journal = {Github repository},
  url = {https://github.com/electronicvisions/halco},
  keywords = {own_software}
}

@MISC{githubsthal,
  author = {{Electronic Visions(s), Heidelberg University}},
  title = {sthal},
  publisher = {GitHub},
  journal = {Github repository},
  url = {https://github.com/electronicvisions/sthal},
  keywords = {own_software}
}

@MISC{githubess,
  author = {{UHEI, TUD}},
  title = {ESS},
  publisher = {GitHub},
  journal = {Github repository},
  url = {https://github.com/electronicvisions/systemsim-stage2},
  keywords = {own_software}
}

@MISC{githubpythonic,
  author = {{Electronic Visions(s), Heidelberg University}},
  title = {pythonic},
  publisher = {GitHub},
  journal = {Github repository},
  url = {https://github.com/electronicvisions/pythonic},
  keywords = {own_software}
}

@MISC{githubcake,
  author = {{Electronic Visions(s), Heidelberg University}},
  title = {cake},
  publisher = {GitHub},
  journal = {Github repository},
  url = {https://github.com/electronicvisions/cake},
  keywords = {own_software}
}

@MISC{githubredman,
  author = {{Electronic Visions(s), Heidelberg University}},
  title = {redman},
  publisher = {GitHub},
  journal = {Github repository},
  url = {https://github.com/electronicvisions/redman},
  keywords = {own_software}
}

@MISC{githubcalibtic,
  author = {{Electronic Visions(s), Heidelberg University}},
  title = {calibtic},
  publisher = {GitHub},
  journal = {Github repository},
  url = {https://github.com/electronicvisions/calibtic},
  keywords = {own_software}
}

@MISC{githubpyhmf,
  author = {{Electronic Visions(s), Heidelberg University}},
  title = {pyhmf},
  publisher = {GitHub},
  journal = {Github repository},
  url = {https://github.com/electronicvisions/pyhmf},
  keywords = {own_software}
}

@MISC{githubmarocco,
  author = {{Electronic Visions(s), Heidelberg University}},
  title = {marocco},
  publisher = {GitHub},
  journal = {Github repository},
  url = {https://github.com/electronicvisions/marocco},
  keywords = {own_software}
}

@MISC{githubhaldls,
  author = {{Electronic Visions(s), Heidelberg University}},
  title = {haldls},
  publisher = {GitHub},
  journal = {Github repository},
  url = {https://github.com/electronicvisions/haldls},
  keywords = {own_software}
}

@software{githublibnux,
  author = {{Electronic Visions(s), Heidelberg University}},
  title = {libnux},
  publisher = {GitHub},
  journal = {Github repository},
  url = {https://github.com/electronicvisions/libnux},
  year = 2022,
  keywords = {own_software}
}

@misc{typescript,
  author = {Microsoft},
  title = {TypeScript: JavaScript For Any Scale.},
  url = {https://www.typescriptlang.org/}
}

@inproceedings{yoo2003slurm,
  title={Slurm: Simple linux utility for resource management},
  author={Yoo, Andy B and Jette, Morris A and Grondona, Mark},
  booktitle={Workshop on Job Scheduling Strategies for Parallel Processing},
  pages={44--60},
  year=2003,
  organization={Springer}
}

@software{RCF,
  author = {{Delta V Software}},
  title = {Remote Call Framework},
  url = {www.deltavsoft.com},
  year = 2020
}

@misc{pygccxml,
  author = {Yakovenko, Roman},
  title = {pygccxml/py++},
  url= {https://sourceforge.net/projects/pygccxml}
}

@misc{githubbitter,
  author = {Jeltsch, Sebastian},
  title = {bitter},
  url= {https://github.com/ignatz/bitter},
  keywords = {own_software}
}

@misc{githubrant,
  author = {Jeltsch, Sebastian},
  title = {rant},
  url= {https://github.com/ignatz/rant},
  keywords = {own_software}
}

@misc{gcc_libstdc++,
  author = {Free Software Foundation},
  title = {The GNU C++ Library},
  url= {https://gcc.gnu.org/onlinedocs/libstdc++},
}

@article{cramer2019control,
    title={Control of criticality and computation in spiking neuromorphic networks with plasticity},
    author={Benjamin Cramer and David Stöckel and Markus Kreft and Michael Wibral and Johannes Schemmel and Karlheinz Meier and Viola Priesemann},
    year={2019},
    eprint={1909.08418},
    archivePrefix={arXiv},
    primaryClass={cs.ET}
}

@article{cramer2020training,
    title={Training spiking multi-layer networks with surrogate gradients on an analog neuromorphic substrate},
    author={Benjamin Cramer and Sebastian Billaudelle and Simeon Kanya and Aron Leibfried and Andreas Gr\"{u}bl and Vitali Karasenko and Christian Pehle and Korbinian Schreiber and Yannik Stradmann and Johannes Weis and Johannes Schemmel and Friedemann Zenke},
    year={2020},
    eprint={2006.07239},
    archivePrefix={arXiv},
    primaryClass={cs.NE},
    journal={arXiv preprint},
    url={https://arxiv.org/abs/2006.07239}
}

@article{cramer2020training_nourl,
    title={Training spiking multi-layer networks with surrogate gradients on an analog neuromorphic substrate},
    author={Benjamin Cramer and Sebastian Billaudelle and Simeon Kanya and Aron Leibfried and Andreas Gr\"{u}bl and Vitali Karasenko and Christian Pehle and Korbinian Schreiber and Yannik Stradmann and Johannes Weis and Johannes Schemmel and Friedemann Zenke},
    year=2020,
    eprint={2006.07239},
    archivePrefix={arXiv},
    primaryClass={cs.NE},
    journal={arXiv preprint}
}

@article{cramer2022surrogate,
  title={Surrogate gradients for analog neuromorphic computing},
  author={Cramer, Benjamin and Billaudelle, Sebastian and Kanya, Simeon and Leibfried, Aron and Gr{\"u}bl, Andreas and Karasenko, Vitali and Pehle, Christian and Schreiber, Korbinian and Stradmann, Yannik and Weis, Johannes and others},
  journal={Proceedings of the National Academy of Sciences},
  volume=119,
  number=4,
  year=2022,
  publisher={National Acad Sciences}
}

@ARTICLE{cramer2022heidelberg,
  author = {Cramer, Benjamin and Stradmann, Yannik and Schemmel, Johannes and Zenke, Friedemann},
  journal = {{IEEE} Transactions on Neural Networks and Learning Systems},
  title = {The {H}eidelberg Spiking Data Sets for the Systematic Evaluation of Spiking Neural Networks},
  year = 2022,
  volume = 33,
  number = 7,
  pages = {2744--2757},
  doi = {10.1109/TNNLS.2020.3044364}
}

@article{cremonesi2020analytic,
	author = {Francesco Cremonesi and Georg Hager and Gerhard Wellein and Felix Sch{\"u}rmann},
	title = {Analytic performance modeling and analysis of detailed neuron simulations},
	journal = {The International Journal of High Performance Computing Applications},
	volume = 34,
	number = 4,
	pages = {428--449},
	year = 2020,
	doi = {10.1177/1094342020912528},
}

@article{bohnstingl2019neuromorphic,
  title = {Neuromorphic Hardware Learns to Learn},
  abstract = {Hyperparameters and learning algorithms for neuromorphic hardware are usually chosen by hand to suit a particular task. In contrast, networks of neurons in the brain were optimized through extensive evolutionary and developmental processes to work well on a range of computing and learning tasks. Occasionally this process has been emulated through genetic algorithms, but these require themselves hand-design of their details and tend to provide a limited range of improvements. We employ instead other powerful gradient-free optimization tools, such as cross-entropy methods and evolutionary strategies, in order to port the function of biological optimization processes to neuromorphic hardware. As an example, we show these optimization algorithms enable neuromorphic agents to learn very efficiently from rewards. In particular, meta-plasticity, i.e., the optimization of the learning rule which they use, substantially enhances reward-based learning capability of the hardware. In addition, we demonstrate for the first time Learning-to-Learn benefits from such hardware, in particular, the capability to extract abstract knowledge from prior learning experiences that speeds up the learning of new but related tasks. Learning-to-Learn is especially suited for accelerated neuromorphic hardware, since it makes it feasible to carry out the required very large number of network computations.},
  author = {Thomas Bohnstingl and Franz Scherr and Christian Pehle and Karlheinz Meier and Wolfgang Maass},
  year = 2019,
  month = 5,
  day = 21,
  volume = 2019,
  pages = {1--14},
  journal = {Frontiers in Neuroscience},
  issn = {1662-4548},
  publisher = {Frontiers Research Foundation},
  number = 13,
  doi = {10.3389/fnins.2019.00483},
}

@patent{hill1987serial,
title = "Queued serial peripheral interface for use in a data processing system",
author = "Susan C. Hill and Joseph Jelemensky and Mark R. Heene and Stanley E. Groves and Daniel N. DeBrito",
number = "US4958277A",
year = 1987
}

@misc{tis1995elf,
title = "Executable and Linking Format (ELF) Specification",
author = "Tool Interface Standard Commitee",
number = "1.2",
year = 1995,
url = {http://refspecs.linuxbase.org/elf/elf.pdf}
}

@INPROCEEDINGS{tyler1999altivec,
author={Jon Tyler and Jeff Lent and Anh Mather and Huy Nguyen},
booktitle={1999 IEEE International Performance, Computing and Communications Conference},
title={AltiVec: bringing vector technology to the PowerPC processor family},
year=1999,
volume={},
number={},
pages={437--444},
keywords={microprocessor chips;computer architecture;floating point arithmetic;vector processor systems;Motorola AltiVec;vector technology;PowerPC processor family;SIMD vector extension;IEEE single-precision floating-point numbers;fine grained data prefetch instructions;Microprocessors;Clocks;Frequency;Registers;Pipelines;Power generation;USA Councils;Prefetching;Delay;Bandwidth},
doi={10.1109/PCCC.1999.749469},
ISSN={1097-2641},
month=feb,}


@INPROCEEDINGS{ruiz2019opentcp,
author = {Ruiz, Mario and Sidler, David and Sutter, Gustavo and Alonso, Gustavo and López-Buedo, Sergio},
year = {2019},
month = {09},
pages = {},
title = {Limago: an FPGA-based Open-source 100 GbE TCP/IP Stack},
doi = {10.1109/FPL.2019.00053}
}

@article{kingma2014adam,
  title   = {Adam: A Method for Stochastic Optimization},
  author  = {Kingma, Diederik P. and Ba, Jimmy},
  year    = 2014,
  eprint  = {1412.6980},
  archivePrefix = {arXiv},
  primaryClass = {cs.LG},
  journal = {International Conference on Learning Representations}
}

@online{google-edge-tpu,
  author={Coral},
  title={Edge {TPU} performance benchmarks},
  url={https://coral.ai/docs/edgetpu/benchmarks/},
  year=2020,
  month=aug
}

@Article{polsky2004computational,
  author          = {Polsky, Alon and Mel, Bartlett W. and Schiller, Jackie},
  journal         = {Nature Neuroscience},
  title           = {Computational subunits in thin dendrites of pyramidal cells},
  year            = {2004},
  issn            = {1097-6256},
  month           = jun,
  pages           = {621--627},
  volume          = {7},
  abstract        = {The thin basal and oblique dendrites of cortical pyramidal neurons receive most of the synaptic inputs from other cells, but their integrative properties remain uncertain. Previous studies have most often reported global linear or sublinear summation. An alternative view, supported by biophysical modeling studies, holds that thin dendrites provide a layer of independent computational 'subunits' that sigmoidally modulate their inputs prior to global summation. To distinguish these possibilities, we combined confocal imaging and dual-site focal synaptic stimulation of identified thin dendrites in rat neocortical pyramidal neurons. We found that nearby inputs on the same branch summed sigmoidally, whereas widely separated inputs or inputs to different branches summed linearly. This strong spatial compartmentalization effect is incompatible with a global summation rule and provides the first experimental support for a two-layer 'neural network' model of pyramidal neuron thin-branch integration. Our findings could have important implications for the computing and memory-related functions of cortical tissue.},
  doi             = {10.1038/nn1253},
}

@manual{xilinx2019zynqultra,
    author={Xilinx},
    title={Zync {UltraScale+} {MPSoC} Data Sheet},
    url={https://www.xilinx.com/support/documentation/data_sheets/ds891-zynq-ultrascale-plus-overview.pdf},
    year=2019
}

@Article{larkum1999cellular,
  author          = {Larkum, M. E. and Zhu, J. J. and Sakmann, B.},
  journal         = {Nature},
  title           = {A new cellular mechanism for coupling inputs arriving at different cortical layers},
  year            = {1999},
  issn            = {0028-0836},
  month           = mar,
  pages           = {338--341},
  volume          = {398},
  abstract        = {Pyramidal neurons in layer 5 of the neocortex of the brain extend their axons and dendrites into all layers. They are also unusual in having both an axonal and a dendritic zone for the initiation of action potentials. Distal dendritic inputs, which normally appear greatly attenuated at the axon, must cross a high threshold at the dendritic initiation zone to evoke calcium action potentials but can then generate bursts of axonal action potentials. Here we show that a single back-propagating sodium action potential generated in the axon facilitates the initiation of these calcium action potentials when it coincides with distal dendritic input within a time window of several milliseconds. Inhibitory dendritic input can selectively block the initiation of dendritic calcium action potentials, preventing bursts of axonal action potentials. Thus, excitatory and inhibitory postsynaptic potentials arising in the distal dendrites can exert significantly greater control over action potential initiation in the axon than would be expected from their electrotonically isolated locations. The coincidence of a single back-propagating action potential with a subthreshold distal excitatory postsynaptic potential to evoke a burst of axonal action potentials represents a new mechanism by which the main cortical output neurons can associate inputs arriving at different cortical layers.},
  doi             = {10.1038/18686},
}

@Article{larkum2001dendritic,
  author          = {M. E. Larkum and J. J. Zhu and B. Sakmann},
  journal         = {The Journal of physiology},
  title           = {Dendritic mechanisms underlying the coupling of the dendritic with the axonal action potential initiation zone of adult rat layer 5 pyramidal neurons},
  year            = {2001},
  issn            = {0022-3751},
  month           = jun,
  number          = {2},
  pages           = {447--466},
  volume          = {533},
  abstract        = {1. Double, triple and quadruple whole-cell voltage recordings were made simultaneously from different parts of the apical dendritic arbor and the soma of adult layer 5 (L5) pyramidal neurons. We investigated the membrane mechanisms that support the conduction of dendritic action potentials (APs) between the dendritic and axonal AP initiation zones and their influence on the subsequent AP pattern. 2. The duration of the current injection to the distal dendritic initiation zone controlled the degree of coupling with the axonal initiation zone and the AP pattern. 3. Two components of the distally evoked regenerative potential were pharmacologically distinguished: a rapidly rising peak potential that was TTX sensitive and a slowly rising plateau-like potential that was Cd(2+) and Ni(2+) sensitive and present only with longer-duration current injection. 4. The amplitude of the faster forward-propagating Na(+)-dependent component and the amplitude of the back-propagating AP fell into two classes (more distinctly in the forward-propagating case). Current injection into the dendrite altered propagation in both directions. 5. Somatic current injections that elicited single Na(+) APs evoked bursts of Na(+) APs when current was injected simultaneously into the proximal apical dendrite. The mechanism did not depend on dendritic Na(+)-Ca(2+) APs. 6. A three-compartment model of a L5 pyramidal neuron is proposed. It comprises the distal dendritic and axonal AP initiation zones and the proximal apical dendrite. Each compartment contributes to the initiation and to the pattern of AP discharge in a distinct manner. Input to the three main dendritic arbors (tuft dendrites, apical oblique dendrites and basal dendrites) has a dominant influence on only one of these compartments. Thus, the AP pattern of L5 pyramids reflects the laminar distribution of synaptic activity in a cortical column.},
  doi             = {10.1111/j.1469-7793.2001.0447a.x},
}

@Article{gidon2020dendritic,
  author          = {Gidon, Albert and Zolnik, Timothy Adam and Fidzinski, Pawel and Bolduan, Felix and Papoutsi, Athanasia and Poirazi, Panayiota and Holtkamp, Martin and Vida, Imre and Larkum, Matthew Evan},
  journal         = {Science},
  title           = {Dendritic action potentials and computation in human layer 2/3 cortical neurons},
  year            = {2020},
  issn            = {1095-9203},
  month           = jan,
  pages           = {83--87},
  volume          = {367},
  abstract        = {The active electrical properties of dendrites shape neuronal input and output and are fundamental to brain function. However, our knowledge of active dendrites has been almost entirely acquired from studies of rodents. In this work, we investigated the dendrites of layer 2 and 3 (L2/3) pyramidal neurons of the human cerebral cortex ex vivo. In these neurons, we discovered a class of calcium-mediated dendritic action potentials (dCaAPs) whose waveform and effects on neuronal output have not been previously described. In contrast to typical all-or-none action potentials, dCaAPs were graded; their amplitudes were maximal for threshold-level stimuli but dampened for stronger stimuli. These dCaAPs enabled the dendrites of individual human neocortical pyramidal neurons to classify linearly nonseparable inputs-a computation conventionally thought to require multilayered networks.},
  doi             = {10.1126/science.aax6239},
}

@Article{major2013active,
  author          = {Major, Guy and Larkum, Matthew E. and Schiller, Jackie},
  journal         = {Annual Review of Neuroscience},
  title           = {Active properties of neocortical pyramidal neuron dendrites},
  year            = {2013},
  issn            = {1545-4126},
  month           = jul,
  pages           = {1--24},
  volume          = {36},
  abstract        = {Dendrites are the main recipients of synaptic inputs and are important sites that determine neurons' input-output functions. This review focuses on thin neocortical dendrites, which receive the vast majority of synaptic inputs in cortex but also have specialized electrogenic properties. We present a simplified working-model biophysical scheme of pyramidal neurons that attempts to capture the essence of their dendritic function, including the ability to behave under plausible conditions as dynamic computational subunits. We emphasize the electrogenic capabilities of NMDA receptors (NMDARs) because these transmitter-gated channels seem to provide the major nonlinear depolarizing drive in thin dendrites, even allowing full-blown NMDA spikes. We show how apparent discrepancies in experimental findings can be reconciled and discuss the current status of dendritic spikes in vivo; a dominant NMDAR contribution would indicate that the input-output relations of thin dendrites are dynamically set by network activity and cannot be fully predicted by purely reductionist approaches.},
  doi             = {10.1146/annurev-neuro-062111-150343},
}

@Article{euler2002directionally,
  author          = {Euler, Thomas and Detwiler, Peter B. and Denk, Winfried},
  journal         = {Nature},
  title           = {Directionally selective calcium signals in dendrites of starburst amacrine cells.},
  year            = {2002},
  issn            = {0028-0836},
  month           = aug,
  pages           = {845--852},
  volume          = {418},
  abstract        = {The detection of image motion is fundamental to vision. In many species, unique classes of retinal ganglion cells selectively respond to visual stimuli that move in specific directions. It is not known which retinal cell first performs the neural computations that give rise to directional selectivity in the ganglion cell. A prominent candidate has been an interneuron called the 'starburst amacrine cell'. Using two-photon optical recordings of intracellular calcium concentration, here we find that individual dendritic branches of starburst cells act as independent computation modules. Dendritic calcium signals, but not somatic membrane voltage, are directionally selective for stimuli that move centrifugally from the cell soma. This demonstrates that direction selectivity is computed locally in dendritic branches at a stage before ganglion cells.},
  doi             = {10.1038/nature00931},
}

@Article{tukker2004direction,
  author          = {Tukker, John J. and Taylor, W. Rowland and Smith, Robert G.},
  journal         = {Visual Neuroscience},
  title           = {Direction selectivity in a model of the starburst amacrine cell},
  year            = {2004},
  issn            = {0952-5238},
  pages           = {611--625},
  volume          = {21},
  abstract        = {The starburst amacrine cell (SBAC), found in all mammalian retinas, is thought to provide the directional inhibitory input recorded in On-Off direction-selective ganglion cells (DSGCs). While voltage recordings from the somas of SBACs have not shown robust direction selectivity (DS), the dendritic tips of these cells display direction-selective calcium signals, even when gamma-aminobutyric acid (GABAa,c) channels are blocked, implying that inhibition is not necessary to generate DS. This suggested that the distinctive morphology of the SBAC could generate a DS signal at the dendritic tips, where most of its synaptic output is located. To explore this possibility, we constructed a compartmental model incorporating realistic morphological structure, passive membrane properties, and excitatory inputs. We found robust DS at the dendritic tips but not at the soma. Two-spot apparent motion and annulus radial motion produced weak DS, but thin bars produced robust DS. For these stimuli, DS was caused by the interaction of a local synaptic input signal with a temporally delayed "global" signal, that is, an excitatory postsynaptic potential (EPSP) that spread from the activated inputs into the soma and throughout the dendritic tree. In the preferred direction the signals in the dendritic tips coincided, allowing summation, whereas in the null direction the local signal preceded the global signal, preventing summation. Sine-wave grating stimuli produced the greatest amount of DS, especially at high velocities and low spatial frequencies. The sine-wave DS responses could be accounted for by a simple mathematical model, which summed phase-shifted signals from soma and dendritic tip. By testing different artificial morphologies, we discovered DS was relatively independent of the morphological details, but depended on having a sufficient number of inputs at the distal tips and a limited electrotonic isolation. Adding voltage-gated calcium channels to the model showed that their threshold effect can amplify DS in the intracellular calcium signal.},
  doi             = {10.1017/S0952523804214109},
}


@Article{poirazi2001impact,
  author          = {Poirazi, Panayiota and Mel, Bartlett W.},
  journal         = {Neuron},
  title           = {Impact of active dendrites and structural plasticity on the memory capacity of neural tissue},
  year            = {2001},
  issn            = {0896-6273},
  month           = mar,
  pages           = {779--796},
  volume          = {29},
  abstract        = {We consider the combined effects of active dendrites and structural plasticity on the storage capacity of neural tissue. We compare capacity for two different modes of dendritic integration: (1) linear, where synaptic inputs are summed across the entire dendritic arbor, and (2) nonlinear, where each dendritic compartment functions as a separately thresholded neuron-like summing unit. We calculate much larger storage capacities for cells with nonlinear subunits and show that this capacity is accessible to a structural learning rule that combines random synapse formation with activity-dependent stabilization/elimination. In a departure from the common view that memories are encoded in the overall connection strengths between neurons, our results suggest that long-term information storage in neural tissue could reside primarily in the selective addressing of synaptic contacts onto dendritic subunits.},
  doi             = {10.1016/s0896-6273(01)00252-5},
}

@Article{poirazi2003pyramidal,
  author          = {Poirazi, Panayiota and Brannon, Terrence and Mel, Bartlett W.},
  journal         = {Neuron},
  title           = {Pyramidal neuron as two-layer neural network},
  year            = {2003},
  issn            = {0896-6273},
  month           = mar,
  pages           = {989--999},
  volume          = {37},
  abstract        = {The pyramidal neuron is the principal cell type in the mammalian forebrain, but its function remains poorly understood. Using a detailed compartmental model of a hippocampal CA1 pyramidal cell, we recorded responses to complex stimuli consisting of dozens of high-frequency activated synapses distributed throughout the apical dendrites. We found the cell's firing rate could be predicted by a simple formula that maps the physical components of the cell onto those of an abstract two-layer "neural network." In the first layer, synaptic inputs drive independent sigmoidal subunits corresponding to the cell's several dozen long, thin terminal dendrites. The subunit outputs are then summed within the main trunk and cell body prior to final thresholding. We conclude that insofar as the neural code is mediated by average firing rate, a two-layer neural network may provide a useful abstraction for the computing function of the individual pyramidal neuron.},
  doi             = {10.1016/s0896-6273(03)00149-1},
}

@Article{vetter2001propagation,
  author          = {Vetter, P. and Roth, A. and H\"{a}usser, M.},
  journal         = {Journal of Neurophysiology},
  title           = {Propagation of action potentials in dendrites depends on dendritic morphology},
  year            = {2001},
  issn            = {0022-3077},
  month           = feb,
  pages           = {926--937},
  volume          = {85},
  abstract        = {Action potential propagation links information processing in different regions of the dendritic tree. To examine the contribution of dendritic morphology to the efficacy of propagation, simulations were performed in detailed reconstructions of eight different neuronal types. With identical complements of voltage-gated channels, different dendritic morphologies exhibit distinct patterns of propagation. Remarkably, the range of backpropagation efficacies observed experimentally can be reproduced by the variations in dendritic morphology alone. Dendritic geometry also determines the extent to which modulation of channel densities can affect propagation. Thus in Purkinje cells and dopamine neurons, backpropagation is relatively insensitive to changes in channel densities, whereas in pyramidal cells, backpropagation can be modulated over a wide range. We also demonstrate that forward propagation of dendritically initiated action potentials is influenced by morphology in a similar manner. We show that these functional consequences of the differences in dendritic geometries can be explained quantitatively using simple anatomical measures of dendritic branching patterns, which are captured in a reduced model of dendritic geometry. These findings indicate that differences in dendritic geometry act in concert with differences in voltage-gated channel density and kinetics to generate the diversity in dendritic action potential propagation observed between neurons. They also suggest that changes in dendritic geometry during development and plasticity will critically affect propagation. By determining the spatial pattern of action potential signaling, dendritic morphology thus helps to define the size and interdependence of functional compartments in the neuron.},
  doi             = {10.1152/jn.2001.85.2.926},
}

@Article{schaefer2003coincidence,
  author          = {Schaefer, Andreas T. and Larkum, Matthew E. and Sakmann, Bert and Roth, Arnd},
  journal         = {Journal of Neurophysiology},
  title           = {Coincidence detection in pyramidal neurons is tuned by their dendritic branching pattern},
  year            = {2003},
  issn            = {0022-3077},
  month           = jun,
  pages           = {3143--3154},
  volume          = {89},
  abstract        = {Neurons display a variety of complex dendritic morphologies even within the same class. We examined the relationship between dendritic arborization and the coupling between somatic and dendritic action potential (AP) initiation sites in layer 5 (L5) neocortical pyramidal neurons. Coupling was defined as the relative reduction in threshold for initiation of a dendritic calcium AP due to a coincident back-propagating AP. Simulations based on reconstructions of biocytin-filled cells showed that addition of oblique branches of the main apical dendrite in close proximity to the soma (d < 140 microm) increases the coupling between the apical and axosomatic AP initiation zones, whereas incorporation of distal branches decreases coupling. Experimental studies on L5 pyramids in acute brain slices revealed a highly significant (n = 28, r = 0.63, P < 0.0005) correlation: increasing the fraction of proximal oblique dendrites (d < 140 microm), e.g., from 30 to 60\% resulted on average in an increase of the coupling from approximately 35\% to almost 60\%. We conclude that variation in dendritic arborization may be a key determinant of variability in coupling (49 +/- 17\%; range 19-83\%; n = 37) and is likely to outweigh the contribution made by variations in active membrane properties. Thus coincidence detection of inputs arriving from different cortical layers is strongly regulated by differences in dendritic arborization.},
  doi             = {10.1152/jn.00046.2003},
}

@Article{williams2002dependence,
  author          = {Williams, Stephen R. and Stuart, Greg J.},
  journal         = {Science},
  title           = {Dependence of {EPSP} efficacy on synapse location in neocortical pyramidal neurons},
  year            = {2002},
  issn            = {1095-9203},
  month           = mar,
  pages           = {1907--1910},
  volume          = {295},
  abstract        = {Neurons receive thousands of synaptic inputs throughout elaborate dendritic trees. Here we determine the somatic impact of excitatory postsynaptic potentials (EPSPs) generated at known dendritic sites in neocortical pyramidal neurons. As inputs became more distal, somatic EPSP amplitude decreased, whereas use-dependent depression increased. Despite marked attenuation (>40-fold), when coactivated within a narrow time window (approximately 10 milliseconds), distal EPSPs could directly influence action potential output following dendritic spike generation. These findings reveal that distal EPSPs are ineffective sources of background somatic excitation, but through coincidence detection have a powerful transient signaling role.},
  doi             = {10.1126/science.1067903},
}

@Article{sacramento2018dendritic,
  author        = {Jo{\~a}o Sacramento and Rui Ponte Costa and Yoshua Bengio and Walter Senn},
  title         = {Dendritic cortical microcircuits approximate the backpropagation algorithm},
  year          = {2018},
  month         = oct,
  abstract      = {Deep learning has seen remarkable developments over the last years, many of them inspired by neuroscience. However, the main learning mechanism behind these advances - error backpropagation - appears to be at odds with neurobiology. Here, we introduce a multilayer neuronal network model with simplified dendritic compartments in which error-driven synaptic plasticity adapts the network towards a global desired output. In contrast to previous work our model does not require separate phases and synaptic learning is driven by local dendritic prediction errors continuously in time. Such errors originate at apical dendrites and occur due to a mismatch between predictive input from lateral interneurons and activity from actual top-down feedback. Through the use of simple dendritic compartments and different cell-types our model can represent both error and normal activity within a pyramidal neuron. We demonstrate the learning capabilities of the model in regression and classification tasks, and show analytically that it approximates the error backpropagation algorithm. Moreover, our framework is consistent with recent observations of learning between brain areas and the architecture of cortical microcircuits. Overall, we introduce a novel view of learning on dendritic cortical circuits and on how the brain may solve the long-standing synaptic credit assignment problem.},
  archiveprefix = {arXiv},
  eprint        = {1810.11393},
  primaryclass  = {q-bio.NC},
}

@Article{aru2020cellular,
  author          = {Aru, Jaan and Suzuki, Mototaka and Larkum, Matthew E.},
  journal         = {Trends in Cognitive Sciences},
  title           = {Cellular mechanisms of conscious processing},
  year            = {2020},
  issn            = {1879-307X},
  month           = oct,
  pages           = {814--825},
  volume          = {24},
  abstract        = {Recent breakthroughs in neurobiology indicate that the time is ripe to understand how cellular-level mechanisms are related to conscious experience. Here, we highlight the biophysical properties of pyramidal cells, which allow them to act as gates that control the evolution of global activation patterns. In conscious states, this cellular mechanism enables complex sustained dynamics within the thalamocortical system, whereas during unconscious states, such signal propagation is prohibited. We suggest that the hallmark of conscious processing is the flexible integration of bottom-up and top-down data streams at the cellular level. This cellular integration mechanism provides the foundation for Dendritic Information Theory, a novel neurobiological theory of consciousness.},
  doi             = {10.1016/j.tics.2020.07.006},
}

@Article{suzuki2020general,
  author          = {Suzuki, Mototaka and Larkum, Matthew E.},
  journal         = {Cell},
  title           = {General anesthesia decouples cortical pyramidal neurons},
  year            = {2020},
  issn            = {1097-4172},
  month           = feb,
  pages           = {666--676.e13},
  volume          = {180},
  abstract        = {The mystery of general anesthesia is that it specifically suppresses consciousness by disrupting feedback signaling in the brain, even when feedforward signaling and basic neuronal function are left relatively unchanged. The mechanism for such selectiveness is unknown. Here we show that three different anesthetics have the same disruptive influence on signaling along apical dendrites in cortical layer 5 pyramidal neurons in mice. We found that optogenetic depolarization of the distal apical dendrites caused robust spiking at the cell body under awake conditions that was blocked by anesthesia. Moreover, we found that blocking metabotropic glutamate and cholinergic receptors had the same effect on apical dendrite decoupling as anesthesia or inactivation of the higher-order thalamus. If feedback signaling occurs predominantly through apical dendrites, the cellular mechanism we found would explain not only how anesthesia selectively blocks this signaling but also why conscious perception depends on both cortico-cortical and thalamo-cortical connectivity.},
  doi             = {10.1016/j.cell.2020.01.024},
}

@Article{richards2019dendritic,
  author       = {Richards, Blake A. and Lillicrap, Timothy P.},
  journal      = {Current Opinion in Neurobiology},
  title        = {Dendritic solutions to the credit assignment problem},
  year         = {2019},
  issn         = {1873-6882},
  month        = feb,
  pages        = {28--36},
  volume       = {54},
  abstract     = {Guaranteeing that synaptic plasticity leads to effective learning requires a means for assigning credit to each neuron for its contribution to behavior. The 'credit assignment problem' refers to the fact that credit assignment is non-trivial in hierarchical networks with multiple stages of processing. One difficulty is that if credit signals are integrated with other inputs, then it is hard for synaptic plasticity rules to distinguish credit-related activity from non-credit-related activity. A potential solution is to use the spatial layout and non-linear properties of dendrites to distinguish credit signals from other inputs. In cortical pyramidal neurons, evidence hints that top-down feedback signals are integrated in the distal apical dendrites and have a distinct impact on spike-firing and synaptic plasticity. This suggests that the distal apical dendrites of pyramidal neurons help the brain to solve the credit assignment problem.},
  doi          = {10.1016/j.conb.2018.08.003},
}

@Article{urbanczik2014learning,
  author          = {Urbanczik, Robert and Senn, Walter},
  journal         = {Neuron},
  title           = {Learning by the dendritic prediction of somatic spiking},
  year            = {2014},
  issn            = {1097-4199},
  month           = feb,
  number          = {3},
  pages           = {521--528},
  volume          = {81},
  abstract        = {Recent modeling of spike-timing-dependent plasticity indicates that plasticity involves as a third factor a local dendritic potential, besides pre- and postsynaptic firing times. We present a simple compartmental neuron model together with a non-Hebbian, biologically plausible learning rule for dendritic synapses where plasticity is modulated by these three factors. In functional terms, the rule seeks to minimize discrepancies between somatic firings and a local dendritic potential. Such prediction errors can arise in our model from stochastic fluctuations as well as from synaptic input, which directly targets the soma. Depending on the nature of this direct input, our plasticity rule subserves supervised or unsupervised learning. When a reward signal modulates the learning rate, reinforcement learning results. Hence a single plasticity rule supports diverse learning paradigms.},
  doi             = {10.1016/j.neuron.2013.11.030},
}


@Article{guerguiev2017towards,
  author          = {Guerguiev, Jordan and Lillicrap, Timothy P. and Richards, Blake A.},
  journal         = {eLife},
  title           = {Towards deep learning with segregated dendrites},
  year            = {2017},
  issn            = {2050-084X},
  month           = dec,
  volume          = {6},
  abstract        = {Deep learning has led to significant advances in artificial intelligence, in part, by adopting strategies motivated by neurophysiology. However, it is unclear whether deep learning could occur in the real brain. Here, we show that a deep learning algorithm that utilizes multi-compartment neurons might help us to understand how the neocortex optimizes cost functions. Like neocortical pyramidal neurons, neurons in our model receive sensory information and higher-order feedback in electrotonically segregated compartments. Thanks to this segregation, neurons in different layers of the network can coordinate synaptic weight updates. As a result, the network learns to categorize images better than a single layer network. Furthermore, we show that our algorithm takes advantage of multilayer architectures to identify useful higher-order representations-the hallmark of deep learning. This work demonstrates that deep learning can be achieved using segregated dendritic compartments, which may help to explain the morphology of neocortical pyramidal neurons.},
  doi             = {10.7554/eLife.22901},
}

@Article{poirazi2020illuminating,
  author          = {Poirazi, Panayiota and Papoutsi, Athanasia},
  journal         = {Nature reviews. Neuroscience},
  title           = {Illuminating dendritic function with computational models},
  year            = {2020},
  issn            = {1471-0048},
  month           = jun,
  pages           = {303--321},
  volume          = {21},
  abstract        = {Dendrites have always fascinated researchers: from the artistic drawings by Ramon y Cajal to the beautiful recordings of today, neuroscientists have been striving to unravel the mysteries of these structures. Theoretical work in the 1960s predicted important dendritic effects on neuronal processing, establishing computational modelling as a powerful technique for their investigation. Since then, modelling of dendrites has been instrumental in driving neuroscience research in a targeted manner, providing experimentally testable predictions that range from the subcellular level to the systems level, and their relevance extends to fields beyond neuroscience, such as machine learning and artificial intelligence. Validation of modelling predictions often requires - and drives - new technological advances, thus closing the loop with theory-driven experimentation that moves the field forward. This Review features the most important, to our understanding, contributions of modelling of dendritic computations, including those pending experimental verification, and highlights studies of successful interactions between the modelling and experimental neuroscience communities.},
  doi             = {10.1038/s41583-020-0301-7},
}

@Article{bono2017modelling,
  author          = {Bono, Jacopo and Wilmes, Katharina A. and Clopath, Claudia},
  journal         = {Current Opinion in Neurobiology},
  title           = {Modelling plasticity in dendrites: from single cells to networks},
  year            = {2017},
  issn            = {1873-6882},
  month           = oct,
  pages           = {136--141},
  volume          = {46},
  abstract        = {One of the key questions in neuroscience is how our brain self-organises to efficiently process information. To answer this question, we need to understand the underlying mechanisms of plasticity and their role in shaping synaptic connectivity. Theoretical neuroscience typically investigates plasticity on the level of neural networks. Neural network models often consist of point neurons, completely neglecting neuronal morphology for reasons of simplicity. However, during the past decades it became increasingly clear that inputs are locally processed in the dendrites before they reach the cell body. Dendritic properties enable local interactions between synapses and location-dependent modulations of inputs, rendering the position of synapses on dendrites highly important. These insights changed our view of neurons, such that we now think of them as small networks of nearly independent subunits instead of a simple point. Here, we propose that understanding how the brain processes information strongly requires that we consider the following properties: which plasticity mechanisms are present in the dendrites and how do they enable the self-organisation of synapses across the dendritic tree for efficient information processing? Ultimately, dendritic plasticity mechanisms can be studied in networks of neurons with dendrites, possibly uncovering unknown mechanisms that shape the connectivity in our brains.},
  doi             = {10.1016/j.conb.2017.08.013},
}

@Article{ujfalussy2018global,
  author       = {Ujfalussy, Bal\'{a}zs B. and Makara, Judit K. and Lengyel, M\'{a}t\'{e} and Branco, Tiago},
  journal      = {Neuron},
  title        = {Global and multiplexed dendritic computations under in vivo-like conditions},
  year         = {2018},
  issn         = {1097-4199},
  month        = nov,
  pages        = {579--592.e5},
  volume       = {100},
  abstract     = {Dendrites integrate inputs nonlinearly, but it is unclear how these nonlinearities contribute to the overall input-output transformation of single neurons. We developed statistically principled methods using a hierarchical cascade of linear-nonlinear subunits (hLN) to model the dynamically evolving somatic response of neurons receiving complex, in vivo-like spatiotemporal synaptic input patterns. We used the hLN to predict the somatic membrane potential of an in vivo-validated detailed biophysical model of a L2/3 pyramidal cell. Linear input integration with a single global dendritic nonlinearity achieved above 90% prediction accuracy. A novel hLN motif, input multiplexing into parallel processing channels, could improve predictions as much as conventionally used additional layers of local nonlinearities. We obtained similar results in two other cell types. This approach provides a data-driven characterization of a key component of cortical circuit computations: the input-output transformation of neurons during in vivo-like conditions.},
  doi          = {10.1016/j.neuron.2018.08.032},
}

@Article{golding1998dendritic,
  author          = {Golding, N. L. and Spruston, N.},
  journal         = {Neuron},
  title           = {Dendritic sodium spikes are variable triggers of axonal action potentials in hippocampal {CA1} pyramidal neurons},
  year            = {1998},
  issn            = {0896-6273},
  month           = nov,
  pages           = {1189--1200},
  volume          = {21},
  abstract        = {Several early studies suggested that spikes can be generated in the dendrites of CA1 pyramidal neurons, but their functional significance and the conditions under which they occur remain poorly understood. Here, we provide direct evidence from simultaneous dendritic and somatic patch-pipette recordings that excitatory synaptic inputs can elicit dendritic sodium spikes prior to axonal action potential initiation in hippocampal CA1 pyramidal neurons. Both the probability and amplitude of dendritic spikes depended on the previous synaptic and firing history of the cell. Moreover, some dendritic spikes occurred in the absence of somatic action potentials, indicating that their propagation to the soma and axon is unreliable. We show that dendritic spikes contribute a variable depolarization that summates with the synaptic potential and can act as a trigger for action potential initiation in the axon.},
  doi             = {10.1016/s0896-6273(00)80635-2},
}

@Article{yi2017action,
  author          = {Yi, Guosheng and Wang, Jiang and Wei, Xile and Deng, Bin},
  journal         = {Scientific reports},
  title           = {Action potential initiation in a two-compartment model of pyramidal neuron mediated by dendritic {Ca$^{2+}$} spike},
  year            = {2017},
  issn            = {2045-2322},
  month           = apr,
  pages           = {45684},
  volume          = {7},
  abstract        = {Dendritic Ca  spike endows cortical pyramidal cell with powerful ability of synaptic integration, which is critical for neuronal computation. Here we propose a two-compartment conductance-based model to investigate how the Ca  activity of apical dendrite participates in the action potential (AP) initiation to affect the firing properties of pyramidal neurons. We have shown that the apical input with sufficient intensity triggers a dendritic Ca  spike, which significantly boosts dendritic inputs as it propagates to soma. Such event instantaneously shifts the limit cycle attractor of the neuron and results in a burst of APs, which makes its firing rate reach a plateau steady-state level. Delivering current to two chambers simultaneously increases the level of neuronal excitability and decreases the threshold of input-output relation. Here the back-propagating APs facilitate the initiation of dendritic Ca  spike and evoke BAC firing. These findings indicate that the proposed model is capable of reproducing in vitro experimental observations. By determining spike initiating dynamics, we have provided a fundamental link between dendritic Ca  spike and output APs, which could contribute to mechanically interpreting how dendritic Ca  activity participates in the simple computations of pyramidal neuron.},
  doi             = {10.1038/srep45684},
}

@Article{helmchen1999vivo,
  author          = {Helmchen, F. and Svoboda, K. and Denk, W. and Tank, D. W.},
  journal         = {Nature neuroscience},
  title           = {In vivo dendritic calcium dynamics in deep-layer cortical pyramidal neurons},
  year            = {1999},
  issn            = {1097-6256},
  month           = nov,
  pages           = {989--996},
  volume          = {2},
  abstract        = {Dendritic Ca2+ action potentials in neocortical pyramidal neurons have been characterized in brain slices, but their presence and role in the intact neocortex remain unclear. Here we used two-photon microscopy to demonstrate Ca2+ electrogenesis in apical dendrites of deep-layer pyramidal neurons of rat barrel cortex in vivo. During whisker stimulation, complex spikes recorded intracellularly from distal dendrites and sharp waves in the electrocorticogram were accompanied by large dendritic [Ca2+ ] transients; these also occurred during bursts of action potentials recorded from somata of identified layer 5 neurons. The amplitude of the [Ca 2+] transients was largest proximal to the main bifurcation, where sodium action potentials produced little Ca2+ influx. In some cases, synaptic stimulation evoked [Ca2+] transients without a concomitant action potential burst, suggesting variable coupling between dendrite and soma.},
  doi             = {10.1038/14788},
}

@Article{golding2002dendritic,
  author          = {Golding, Nace L. and Staff, Nathan P. and Spruston, Nelson},
  journal         = {Nature},
  title           = {Dendritic spikes as a mechanism for cooperative long-term potentiation},
  year            = {2002},
  issn            = {0028-0836},
  month           = jul,
  pages           = {326--331},
  volume          = {418},
  abstract        = {Strengthening of synaptic connections following coincident pre- and postsynaptic activity was proposed by Hebb as a cellular mechanism for learning. Contemporary models assume that multiple synapses must act cooperatively to induce the postsynaptic activity required for hebbian synaptic plasticity. One mechanism for the implementation of this cooperation is action potential firing, which begins in the axon, but which can influence synaptic potentiation following active backpropagation into dendrites. Backpropagation is limited, however, and action potentials often fail to invade the most distal dendrites. Here we show that long-term potentiation of synapses on the distal dendrites of hippocampal CA1 pyramidal neurons does require cooperative synaptic inputs, but does not require axonal action potential firing and backpropagation. Rather, locally generated and spatially restricted regenerative potentials (dendritic spikes) contribute to the postsynaptic depolarization and calcium entry necessary to trigger potentiation of distal synapses. We find that this mechanism can also function at proximal synapses, suggesting that dendritic spikes participate generally in a form of synaptic potentiation that does not require postsynaptic action potential firing in the axon.},
  doi             = {10.1038/nature00854},
}

@Article{larkum2004topdown,
  author          = {Larkum, Matthew E. and Senn, Walter and L{\"u}scher, Hans-R.},
  journal         = {Cerebral Cortex},
  title           = {Top-down dendritic input increases the gain of layer 5 pyramidal neurons},
  year            = {2004},
  issn            = {1047-3211},
  month           = oct,
  number          = {10},
  pages           = {1059--1070},
  volume          = {14},
  abstract        = {The cerebral cortex is organized so that an important component of feedback input from higher to lower cortical areas arrives at the distal apical tufts of pyramidal neurons. Yet, distal inputs are predicted to have much less impact on firing than proximal inputs. Here we show that even weak asynchronous dendritic input to the distal tuft region can significantly increase the gain of layer 5 pyramidal neurons and thereby the output of columns in the primary somatosensory cortex of the rat. Noisy currents injected in ramps at different dendritic locations showed that the initial slope of the frequency-current (f/I) relationship increases with the distance of the current injection from the soma. The increase was due to the interaction of dendritic depolarization with back-propagating APs which activated dendritic calcium conductances. Gain increases were accompanied by a change of firing mode from isolated spikes to bursting where the timing of bursts coded the presence of coincident somatic and dendritic inputs. We propose that this dendritic gain modulation and the timing of bursts may serve to associate top-down and bottom-up input on different time scales.},
  doi             = {10.1093/cercor/bhh065},
}

@Article{ramakrishnan2013neuron,
  author          = {Ramakrishnan, Shubha and Wunderlich, Richard and Hasler, Jennifer and George, Suma},
  journal         = {IEEE transactions on biomedical circuits and systems},
  title           = {Neuron array with plastic synapses and programmable dendrites},
  year            = {2013},
  issn            = {1940-9990},
  month           = oct,
  pages           = {631--642},
  volume          = {7},
  abstract        = {We describe a novel neuromorphic chip architecture that models neurons for efficient computation. Traditional architectures of neuron array chips consist of large scale systems that are interfaced with AER for implementing intra- or inter-chip connectivity. We present a chip that uses AER for inter-chip communication but uses fast, reconfigurable FPGA-style routing with local memory for intra-chip connectivity. We model neurons with biologically realistic channel models, synapses and dendrites. This chip is suitable for small-scale network simulations and can also be used for sequence detection, utilizing directional selectivity properties of dendrites, ultimately for use in word recognition.},
  doi             = {10.1109/TBCAS.2013.2282616},
}

@Article{bhaduri2018spiking,
  author          = {Bhaduri, Aritra and Banerjee, Amitava and Roy, Subhrajit and Kar, Sougata and Basu, Arindam},
  journal         = {Neural computation},
  title           = {Spiking neural classifier with lumped dendritic nonlinearity and binary synapses: a current mode {VLSI} implementation and analysis},
  year            = {2018},
  issn            = {1530-888X},
  month           = mar,
  pages           = {723--760},
  volume          = {30},
  abstract        = {We present a neuromorphic current mode implementation of a spiking neural classifier with lumped square law dendritic nonlinearity. It has been shown previously in software simulations that such a system with binary synapses can be trained with structural plasticity algorithms to achieve comparable classification accuracy with fewer synaptic resources than conventional algorithms. We show that even in real analog systems with manufacturing imperfections (CV of 23.5% and 14.4% for dendritic branch gains and leaks respectively), this network is able to produce comparable results with fewer synaptic resources. The chip fabricated in [Formula: see text]m complementary metal oxide semiconductor has eight dendrites per cell and uses two opposing cells per class to cancel common-mode inputs. The chip can operate down to a [Formula: see text] V and dissipates 19 nW of static power per neuronal cell and [Formula: see text] 125 pJ/spike. For two-class classification problems of high-dimensional rate encoded binary patterns, the hardware achieves comparable performance as software implementation of the same with only about a 0.5% reduction in accuracy. On two UCI data sets, the IC integrated circuit has classification accuracy comparable to standard machine learners like support vector machines and extreme learning machines while using two to five times binary synapses. We also show that the system can operate on mean rate encoded spike patterns, as well as short bursts of spikes. To the best of our knowledge, this is the first attempt in hardware to perform classification exploiting dendritic properties and binary synapses.},
  doi             = {10.1162/neco_a_01045},
}

@Article{billaudelle2021structural,
  author          = {Billaudelle, Sebastian and Cramer, Benjamin and Petrovici, Mihai A. and Schreiber, Korbinian and Kappel, David and Schemmel, Johannes and Meier, Karlheinz},
  journal         = {Neural networks : the official journal of the International Neural Network Society},
  title           = {Structural plasticity on an accelerated analog neuromorphic hardware system},
  year            = {2021},
  issn            = {1879-2782},
  month           = jan,
  pages           = {11--20},
  volume          = {133},
  abstract        = {In computational neuroscience, as well as in machine learning, neuromorphic devices promise an accelerated and scalable alternative to neural network simulations. Their neural connectivity and synaptic capacity depend on their specific design choices, but is always intrinsically limited. Here, we present a strategy to achieve structural plasticity that optimizes resource allocation under these constraints by constantly rewiring the pre- and postsynaptic partners while keeping the neuronal fan-in constant and the connectome sparse. In particular, we implemented this algorithm on the analog neuromorphic system BrainScaleS-2. It was executed on a custom embedded digital processor located on chip, accompanying the mixed-signal substrate of spiking neurons and synapse circuits. We evaluated our implementation in a simple supervised learning scenario, showing its ability to optimize the network topology with respect to the nature of its training data, as well as its overall computational efficiency.},
  doi             = {10.1016/j.neunet.2020.09.024},
}

@InProceedings{kousanakis2017architecture,
  author    = {Emmanouil Kousanakis and Apostolos Dollas and Euripides Sotiriades and Ioannis Papaefstathiou and Dionisios N. Pnevmatikatos and Athanasia Papoutsi and Panagiotis C. Petrantonakis and Panayiota Poirazi and Spyridon Chavlis and George Kastellakis},
  booktitle = {2017 {IEEE} 25th Annual International Symposium on Field-Programmable Custom Computing Machines ({FCCM})},
  title     = {An architecture for the acceleration of a hybrid leaky integrate and fire {SNN} on the convey {HC}-2ex {FPGA}-based processor},
  year      = {2017},
  month     = {apr},
  publisher = {{IEEE}},
  abstract  = {Neuromorphic computing is expanding by leaps and bounds through custom integrated circuits (digital and analog), and large scale platforms developed by industry or government-funded projects (e.g. TrueNorth and BrainScaleS, respectively). Whereas the trend is for massive parallelism and neuromorphic computation in order to solve problems, such as those that may appear in machine learning and deep learning algorithms, there is substantial work on brain-like highly accurate neuromorphic computing in order to model the human brain. In such a form of computing, spiking neural networks (SNN) such as the Hodgkin and Huxley model are mapped to various technologies, including FPGAs. In this work, we present a highly efficient FPGA-based architecture for the detailed hybrid Leaky Integrate and Fire SNN that can simulate generic characteristics of neurons of the cerebral cortex. This architecture supports arbitrary, sparse O(n2) interconnection of neurons without need to re-compile the design, and plasticity rules, yielding on a four-FPGA Convey 2ex hybrid computer a speedup of 923x for a non-trivial data set on 240 neurons vs. the same model in the software simulator BRAIN on a Intel(R) Xeon(R) CPU E5-2620 v2 @ 2.10GHz, i.e. the reference state-of-the-art software. Although the reference, official software is single core, the speedup demonstrates that the application scales well among multiple FPGAs, whereas this would not be the case in general-purpose computers due to the arbitrary interconnect requirements. The FPGA-based approach leads to highly detailed models of parts of the human brain up to a few hundred neurons vs. a dozen or fewer neurons on the reference system.},
  doi       = {10.1109/fccm.2017.51},
}

@INPROCEEDINGS{billaudelle2019versatile, crossref={billaudelle2020versatile}}
@InProceedings{billaudelle2020versatile,
  author    = {Billaudelle, Sebastian and Stradmann, Yannik and Schreiber, Korbinian and Cramer, Benjamin and Baumbach, Andreas and Dold, Dominik and G{\"o}ltz, Julian and Kungl, Akos F. and Wunderlich, Timo C. and Hartel, Andreas and M{\"u}ller, Eric and Breitwieser, Oliver and Mauch, Christian and Kleider, Mitja and Gr{\"u}bl, Andreas and St{\"o}ckel, David and Pehle, Christian and Heimbrecht, Arthur and Spilger, Philipp and Kriene, Gerd and Karasenko, Vitali and Senn, Walter and Petrovici, Mihai A. and Schemmel, Johannes and Meier, Karlheinz},
  booktitle = {2020 {IEEE} International Symposium on Circuits and Systems ({ISCAS})},
  title     = {Versatile emulation of spiking neural networks on an accelerated neuromorphic substrate},
  year      = 2020,
  month     = oct,
  publisher = {{IEEE}},
  abstract  = {We present first experimental results on the novel BrainScaleS-2 neuromorphic architecture based on an analog neuro-synaptic core and augmented by embedded microprocessors for complex plasticity and experiment control. The high acceleration factor of 1000 compared to biological dynamics enables the execution of computationally expensive tasks, by allowing the fast emulation of long-duration experiments or rapid iteration over many consecutive trials. The flexibility of our architecture is demonstrated in a suite of five distinct experiments, which emphasize different aspects of the BrainScaleS-2 system.},
  doi       = {10.1109/iscas45731.2020.9180741},
}

@Article{cramer2020control,
  author          = {Cramer, Benjamin and Stöckel, David and Kreft, Markus and Wibral, Michael and Schemmel, Johannes and Meier, Karlheinz and Priesemann, Viola},
  journal         = {Nature communications},
  title           = {Control of criticality and computation in spiking neuromorphic networks with plasticity},
  year            = {2020},
  issn            = {2041-1723},
  month           = jun,
  pages           = {2853},
  volume          = {11},
  abstract        = {The critical state is assumed to be optimal for any computation in recurrent neural networks, because criticality maximizes a number of abstract computational properties. We challenge this assumption by evaluating the performance of a spiking recurrent neural network on a set of tasks of varying complexity at - and away from critical network dynamics. To that end, we developed a plastic spiking network on a neuromorphic chip. We show that the distance to criticality can be easily adapted by changing the input strength, and then demonstrate a clear relation between criticality, task-performance and information-theoretic fingerprint. Whereas the information-theoretic measures all show that network capacity is maximal at criticality, only the complex tasks profit from criticality, whereas simple tasks suffer. Thereby, we challenge the general assumption that criticality would be beneficial for any task, and provide instead an understanding of how the collective network state should be tuned to task requirement.},
  doi             = {10.1038/s41467-020-16548-3},
}

@InProceedings{akar2019arbor,
  author    = {Akar, Nora Abi and Cumming, Ben and Karakasis, Vasileios and K{\"u}sters, Anne and Klijn, Wouter and Peyser, Alexander and Yates, Stuart},
  booktitle = {2019 27th Euromicro International Conference on Parallel, Distributed and Network-Based Processing (PDP)},
  title     = {Arbor --- A Morphologically-Detailed Neural Network Simulation Library for Contemporary High-Performance Computing Architectures},
  year      = {2019},
  month     = {feb},
  pages     = {274--282},
  doi       = {10.1109/EMPDP.2019.8671560},
}

@article{stimberg2019brian,
  author          = {Stimberg, Marcel and Brette, Romain and Goodman, Dan Fm},
  journal         = {eLife},
  title           = {Brian 2, an intuitive and efficient neural simulator},
  year            = {2019},
  month           = {aug},
  volume          = {8},
  abstract        = {Brian 2 allows scientists to simply and efficiently simulate spiking neural network models. These models can feature novel dynamical equations, their interactions with the environment, and experimental protocols. To preserve high performance when defining new models, most simulators offer two options: low-level programming or description languages. The first option requires expertise, is prone to errors, and is problematic for reproducibility. The second option cannot describe all aspects of a computational experiment, such as the potentially complex logic of a stimulation protocol. Brian addresses these issues using runtime code generation. Scientists write code with simple and concise high-level descriptions, and Brian transforms them into efficient low-level code that can run interleaved with their code. We illustrate this with several challenging examples: a plastic model of the pyloric network, a closed-loop sensorimotor model, a programmatic exploration of a neuron model, and an auditory model with real-time input.},
  doi             = {10.7554/eLife.47314},
}

@article{kaiser2021emulating, crossref={kaiser2022emulating}}
@article{kaiser2022emulating,
	author   = {Kaiser, Jakob and Billaudelle, Sebastian and M{\"u}ller, Eric and Tetzlaff, Christian and Schemmel, Johannes and Schmitt, Sebastian},
	title    = {Emulating dendritic computing paradigms on analog neuromorphic hardware},
	year     = 2022,
	volume   = 489,
	pages    = {290--300},
	journal  = {Neuroscience},
	issn     = {0306-4522},
	doi      = {10.1016/j.neuroscience.2021.08.013},
	url      = {https://www.sciencedirect.com/science/article/pii/S0306452221004218},
	abstract = {BrainScaleS-2 is an accelerated and highly configurable neuromorphic system with physical models of neurons and synapses. Beyond networks of spiking point neurons, it allows for the implementation of user-defined neuron morphologies. Both passive propagation of electric signals between compartments as well as dendritic spikes and plateau potentials can be emulated. In this paper, three multi-compartment neuron morphologies are chosen to demonstrate passive propagation of postsynaptic potentials, spatio-temporal coincidence detection of synaptic inputs in a dendritic branch, and the replication of the BAC burst firing mechanism found in layer 5 pyramidal neurons of the neocortex.}
}

@misc{plank2021long,
      title={A Long Short-Term Memory for {AI} Applications in Spike-based Neuromorphic Hardware},
      author={Philipp Plank and Arjun Rao and Andreas Wild and Wolfgang Maass},
      year={2021},
      eprint={2107.03992},
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}

@inproceedings{kriener2021yin,
  title = {The Yin-Yang Dataset},
  author = {Kriener, Laura and G\"{o}ltz, Julian and Petrovici, Mihai A.},
  year = 2022,
  isbn = 9781450395595,
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3517343.3517380},
  booktitle = {Neuro-Inspired Computational Elements Conference},
  pages = {107--111},
  numpages = 5,
  location = {Virtual Event, USA},
  series = {NICE 2022}
}

@ARTICLE{mason1953feedback,
  author={Mason, Samuel J.},
  journal={Proceedings of the {IRE}},
  title={Feedback Theory-Some Properties of Signal Flow Graphs},
  year=1953,
  volume=41,
  number=9,
  pages={1144-1156},
  doi={10.1109/JRPROC.1953.274449}
}

@manual{githubtorchjit,
  author = {{Facebook, Inc.}},
  title = {{PyTorch} {JIT} Overview},
  publisher = {GitHub},
  year = 2021,
  journal = {Github repository},
  url = {https://github.com/pytorch/pytorch/blob/v1.10.0/torch/csrc/jit/OVERVIEW.md}
}

@manual{torchxla,
  author = {{Facebook, Inc.}},
  title = {{PyTorch} on {XLA} Devices},
  publisher = {Facebook},
  year = 2021,
  url = {https://pytorch.org/xla/release/1.9/index.html}
}

@article{gansner2000visualization,
author = {Gansner, Emden R. and North, Stephen C.},
title = {An open graph visualization system and its applications to software engineering},
journal = {Software: Practice and Experience},
volume = 30,
number = 11,
pages = {1203--1233},
keywords = {graph visualization, software engineering, open systems},
doi = {10.1002/1097-024X(200009)30:11<1203::AID-SPE338>3.0.CO;2-N},
year = 2000
}

@article{klassert2021variational,
title={Variational learning of quantum ground states on spiking neuromorphic hardware},
author={Robert Klassert and Andreas Baumbach and Mihai A. Petrovici and Martin Gärttner},
year=2021,
eprint={2109.15169},
archivePrefix={arXiv},
primaryClass={quant-ph}
}

@article{suhan2021lazytensor,
      title={LazyTensor: combining eager execution with domain-specific compilers},
      author={Alex Suhan and Davide Libenzi and Ailing Zhang and Parker Schuh and Brennan Saeta and Jie Young Sohn and Denys Shabalin},
      year=2021,
      eprint={2102.13267},
      archivePrefix={arXiv},
      primaryClass={cs.PL}
}

@inproceedings{ostrau2020benchmarkingICANN,
  abstract     = {With more and more event-based neuromorphic hardware systems being developed
at universities and in industry, there is a growing need for assessing their
performance with domain specific measures. In this work, we use the methodology
of converting pre-trained non-spiking to spiking neural networks to evaluate
the performance loss and measure the energy-per-inference for three
neuromorphic hardware systems (BrainScaleS, Spikey, SpiNNaker) and common
simulation frameworks for CPU (NEST) and CPU/GPU (GeNN). For analog hardware we
further apply a re-training technique known as hardware-in-the-loop training to
cope with device mismatch. This analysis is performed for five different
networks, including three networks that have been found by an automated
optimization with a neural architecture search framework. We demonstrate that
the conversion loss is usually below one percent for digital implementations,
and moderately higher for analog systems with the benefit of much lower
energy-per-inference costs.},
  author       = {Ostrau, Christoph and Homburg, Jonas Dominik and Klarhorst, Christian and Thies, Michael and Rückert, Ulrich},
  booktitle    = {Artificial Neural Networks and Machine Learning – ICANN 2020},
  isbn         = {978-3-030-61615-1 },
  location     = {Bratislava},
  publisher    = {Springer International Publishing},
  title        = {Benchmarking Deep Spiking Neural Networks on Neuromorphic Hardware},
  url          = {https://pub.uni-bielefeld.de/record/2942322},
  doi          = {10.1007/978-3-030-61616-8_49},
  year         = 2020,
}

@inproceedings{ostrau2020benchmarkingNICE,
  abstract     = {With more and more neuromorphic hardware systems for the accel-
eration of spiking neural networks available in science and industry,
there is a demand for platform comparison and performance esti-
mation of such systems. This work describes selected benchmarks
implemented in a framework with exactly this target: independent
black-box benchmarking and comparison of platforms suitable for
the simulation/emulation of spiking neural networks.},
  author       = {Ostrau, Christoph and Klarhorst, Christian and Thies, Michael and Rückert, Ulrich},
  booktitle    = {Neuro-inspired Computational Elements Workshop (NICE ’20), March 17–20, 2020, Heidelberg, Germany},
  isbn         = {978-1-4503-7718-8},
  keywords     = {neuromorphic computing, spiking neural networks, benchmark},
  location     = {Heidelberg, Germany},
  publisher    = {Association for Computing Machinery (ACM)},
  title        = {Benchmarking of Neuromorphic Hardware Systems},
  url          = {https://nbn-resolving.org/urn:nbn:de:0070-pub-29418319, https://pub.uni-bielefeld.de/record/2941831},
  doi          = {10.1145/3381755.3381772},
  year         = 2020,
}

@inproceedings{ostrau2019comparing,
  abstract     = {In the field of neuromorphic computing several hardware accelerators for spiking neural networks have been introduced, but few studies actually compare different systems. These comparative studies reveal difficulties in porting an existing network to a specific system and in predicting its performance indicators. Finding a common network architecture that is suited for all target platforms and at the same time yields decent results is a major challenge. In this contribution, we show that a winner-takes-all inspired network structure can be employed to solve Sudoku puzzles on three diverse hardware accelerators. By exploring several network implementations, we measured the number of solved puzzles in a set of 100 assorted Sudokus, as well as time and energy to solution. Concerning the last two indicators, our measurements indicate that it can be beneficial to port a network to an analogue hardware system.},
  author       = {Ostrau, Christoph and Klarhorst, Christian and Thies, Michael and Rückert, Ulrich},
  booktitle    = {Conference Proceedings: 2019 International Conference on High Performance Computing \& Simulation (HPCS)},
  keywords     = {neuromorphic hardware, spiking neural networks, Sudoku, winner-takes-all},
  location     = {Dublin, Ireland},
  publisher    = {IEEE},
  title        = {Comparing Neuromorphic Systems by Solving Sudoku Problems},
  url          = {https://nbn-resolving.org/urn:nbn:de:0070-pub-29412077, https://pub.uni-bielefeld.de/record/2941207},
  doi          = {10.1109/HPCS48598.2019.9188207},
  year         = 2019,
}

@inproceedings{ostrau2019benchmarking,
  abstract     = {We present the modular framework SNABSuite (Spiking Neural Architecture Benchmark Suite) for black-box benchmarking of neuromorphic hardware systems and spiking neural network software simulators. The motivation for having a coherent collection of benchmarks is twofold: first, benchmarks evaluated on different platforms provide measures for direct comparison of performance indicators (e.g. resource efficiency, quality of the result, robustness). By using the platforms as they are provided for possible end-users and evaluating selected performance indicators, benchmarks support the decision for or against a system based on use-case requirements. Second, benchmarks may reveal opportunities for effective improvements of a system and can contribute to future development.
Systems like the Heidelberg BrainScaleS project, IBM TrueNorth, the Manchester SpiNNaker project or the Intel Loihi platform drive the evolution of neuromorphic hardware implementations, while comparable benchmarks and corresponding measures are still rare.
We show our methodology for comparing such diverse systems by applying a modular framework, with a user- centric view based on configurable spiking neural network descriptions.},
  author       = {Ostrau, Christoph and Klarhorst, Christian and Thies, Michael and Rückert, Ulrich},
  location     = {Madison, Wisconsin, USA},
  title        = {Benchmarking and Characterization of event-based Neuromorphic Hardware},
  url          = {https://nbn-resolving.org/urn:nbn:de:0070-pub-29353281, https://pub.uni-bielefeld.de/record/2935328},
  year         = 2019,
}

@article{stoeckel2017binary,
  abstract     = {Large-scale neuromorphic hardware platforms, specialized computer systems for energy efficient simulation of spiking neural networks, are being developed around the world, for example as part of the European Human Brain Project (HBP). Due to conceptual differences, a universal performance analysis of these systems in terms of runtime, accuracy and energy efficiency is non-trivial, yet indispensable for further hard- and software development. In this paper we describe a scalable benchmark based on a spiking neural network implementation of the binary neural associative memory. We treat neuromorphic hardware and software simulators as black-boxes and execute exactly the same network description across all devices. Experiments on the HBP platforms under varying configurations of the associative memory show that the presented method allows to test the quality of the neuron model implementation, and to explain significant deviations from the expected reference output.},
  author       = {Stöckel, Andreas and Jenzen, Christoph and Thies, Michael and Rückert, Ulrich},
  issn         = {1662-5188},
  journal      = {Frontiers in Computational Neuroscience},
  publisher    = {Frontiers Media SA},
  title        = {Binary Associative Memories as a Benchmark for Spiking Neuromorphic Hardware},
  url          = {https://nbn-resolving.org/urn:nbn:de:0070-pub-29139689, https://pub.uni-bielefeld.de/record/2913968},
  doi          = {10.3389/fncom.2017.00071},
  volume       = 11,
  year         = 2017,
}

@MASTERSTHESIS{kaiser2020masterthesis,
  author = {Kaiser, Jakob},
  title = {Implementation of Large Scale Neural Networks on the Neuromorphic BrainScaleS-1 System},
  year = 2020,
  type = {Master thesis},
  school = {Ruprecht-Karls-Universit{\"a}t Heidelberg}
}

@article{zenke2018superspike,
  title={{SuperSpike}: Supervised Learning in Multilayer Spiking Neural Networks},
  volume=30,
  ISSN={1530-888X},
  number=6,
  journal={Neural Computation},
  DOI={10.1162/neco_a_01086},
  publisher={{MIT} Press --- Journals},
  author={Zenke, Friedemann and Ganguli, Surya},
  year=2018,
  month=jun,
  pages={1514--1541}
}

@inproceedings{shrestha2018slayer,
  author = {Shrestha, Sumit Bam and Orchard, Garrick},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
  publisher = {Curran Associates, Inc.},
  title = {{SLAYER}: Spike Layer Error Reassignment in Time},
  url = {https://proceedings.neurips.cc/paper/2018/file/82f2b308c3b01637c607ce05f52a2fed-Paper.pdf},
  volume = 31,
  year = 2018
}

@software{pehle2021norse,
  author       = {Pehle, Christian and
                  Pedersen, Jens Egholm},
  title        = {{Norse --- A deep learning library for spiking neural networks}},
  month        = jan,
  year         = 2021,
  note         = {Documentation: https://norse.ai/docs/},
  publisher    = {Zenodo},
  version      = {0.0.7},
  doi          = {10.5281/zenodo.4422025},
  url          = {https://doi.org/10.5281/zenodo.4422025}
}

@article{pehle2022brainscales2,
  author  = {Christian Pehle and Sebastian Billaudelle and Benjamin Cramer and Jakob Kaiser and Korbinian Schreiber and Yannik Stradmann and Johannes Weis and Aron Leibfried and Eric M{\"u}ller and Johannes Schemmel},
  title   = {The {BrainScaleS-2} Accelerated Neuromorphic System with Hybrid Plasticity},
  journal = {Frontiers in Neuroscience},
  volume  = 16,
  year    = 2022,
  doi     = {10.3389/fnins.2022.795876},
  issn    = {1662-453X},
  url     = {https://www.frontiersin.org/articles/10.3389/fnins.2022.795876},
  eprint  = {2201.11063},
  archivePrefix = {arXiv},
  primaryClass = {cs.NE},
  abstract= {Since the beginning of information processing by electronic components, the nervous system has served as a metaphor for the organization of computational primitives. Brain-inspired computing today encompasses a class of approaches ranging from using novel nano-devices for computation to research into large-scale neuromorphic architectures, such as TrueNorth, SpiNNaker, BrainScaleS, Tianjic, and Loihi. While implementation details differ, spiking neural networks—sometimes referred to as the third generation of neural networks—are the common abstraction used to model computation with such systems. Here we describe the second generation of the BrainScaleS neuromorphic architecture, emphasizing applications enabled by this architecture. It combines a custom analog accelerator core supporting the accelerated physical emulation of bio-inspired spiking neural network primitives with a tightly coupled digital processor and a digital event-routing network.}
}

@article{pehle2022brainscales2_nopreprint,
  author  = {Christian Pehle and Sebastian Billaudelle and Benjamin Cramer and Jakob Kaiser and Korbinian Schreiber and Yannik Stradmann and Johannes Weis and Aron Leibfried and Eric M{\"u}ller and Johannes Schemmel},
  title   = {The {BrainScaleS-2} Accelerated Neuromorphic System with Hybrid Plasticity},
  journal = {Frontiers in Neuroscience},
  volume  = 16,
  year    = 2022,
  doi     = {10.3389/fnins.2022.795876},
  issn    = {1662-453X},
  url     = {https://www.frontiersin.org/articles/10.3389/fnins.2022.795876},
  abstract= {Since the beginning of information processing by electronic components, the nervous system has served as a metaphor for the organization of computational primitives. Brain-inspired computing today encompasses a class of approaches ranging from using novel nano-devices for computation to research into large-scale neuromorphic architectures, such as TrueNorth, SpiNNaker, BrainScaleS, Tianjic, and Loihi. While implementation details differ, spiking neural networks—sometimes referred to as the third generation of neural networks—are the common abstraction used to model computation with such systems. Here we describe the second generation of the BrainScaleS neuromorphic architecture, emphasizing applications enabled by this architecture. It combines a custom analog accelerator core supporting the accelerated physical emulation of bio-inspired spiking neural network primitives with a tightly coupled digital processor and a digital event-routing network.}
}

@article{pehle2022brainscales2_nopreprint_nourl,
  author  = {Christian Pehle and Sebastian Billaudelle and Benjamin Cramer and Jakob Kaiser and Korbinian Schreiber and Yannik Stradmann and Johannes Weis and Aron Leibfried and Eric M{\"u}ller and Johannes Schemmel},
  title   = {The {BrainScaleS-2} Accelerated Neuromorphic System with Hybrid Plasticity},
  journal = {Frontiers in Neuroscience},
  volume  = 16,
  year    = 2022,
  doi     = {10.3389/fnins.2022.795876},
  issn    = {1662-453X},
  abstract= {Since the beginning of information processing by electronic components, the nervous system has served as a metaphor for the organization of computational primitives. Brain-inspired computing today encompasses a class of approaches ranging from using novel nano-devices for computation to research into large-scale neuromorphic architectures, such as TrueNorth, SpiNNaker, BrainScaleS, Tianjic, and Loihi. While implementation details differ, spiking neural networks—sometimes referred to as the third generation of neural networks—are the common abstraction used to model computation with such systems. Here we describe the second generation of the BrainScaleS neuromorphic architecture, emphasizing applications enabled by this architecture. It combines a custom analog accelerator core supporting the accelerated physical emulation of bio-inspired spiking neural network primitives with a tightly coupled digital processor and a digital event-routing network.}
}

@inproceedings{peyser2015nest,
	title={The {NEST} neuronal network simulator: Performance optimization techniques for high performance computing platforms},
	author={Peyser, Alexander and Schenck, Wolfram},
	booktitle={Society for Neuroscience Annual Meeting, J{\"u}lich Supercomputing Center, FZJ-2015},
	volume=6261,
	year=2015
}

@article{yang2021critical,
  author={Yang, Shuangming and Gao, Tian and Wang, Jiang and Deng, Bin and Lansdell, Benjamin and Linares-Barranco, Bernabe},
  title={Efficient Spike-Driven Learning With Dendritic Event-Based Processing},
  journal={Frontiers in Neuroscience},
  volume=15,
  year=2021,
  url={https://www.frontiersin.org/article/10.3389/fnins.2021.601109},
  doi={10.3389/fnins.2021.601109},
  issn={1662-453X},
abstract={A critical challenge in neuromorphic computing is to present computationally efficient algorithms of learning. When implementing gradient-based learning, error information must be routed through the network, such that each neuron knows its contribution to output, and thus how to adjust its weight. This is known as the credit assignment problem. Exactly implementing a solution like backpropagation involves weight sharing, which requires additional bandwidth and computations in a neuromorphic system. Instead, models of learning from neuroscience can provide inspiration for how to communicate error information efficiently, without weight sharing. Here we present a novel dendritic event-based processing (DEP) algorithm, using a two-compartment leaky integrate-and-fire neuron with partially segregated dendrites that effectively solves the credit assignment problem. In order to optimize the proposed algorithm, a dynamic fixed-point representation method and piecewise linear approximation approach are presented, while the synaptic events are binarized during learning. The presented optimization makes the proposed DEP algorithm very suitable for implementation in digital or mixed-signal neuromorphic hardware. The experimental results show that spiking representations can rapidly learn, achieving high performance by using the proposed DEP algorithm. We find the learning capability is affected by the degree of dendritic segregation, and the form of synaptic feedback connections. This study provides a bridge between the biological learning and neuromorphic learning, and is meaningful for the real-time applications in the field of artificial intelligence.}
}

@article{yang2021neuromorphic,
  doi = {10.1109/tnnls.2021.3084250},
  url = {https://doi.org/10.1109/tnnls.2021.3084250},
  year = 2021,
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  pages = {1--15},
  author = {Shuangming Yang and Jiang Wang and Bin Deng and Mostafa Rahimi Azghadi and Bernabe Linares-Barranco},
  title = {Neuromorphic Context-Dependent Learning Framework With Fault-Tolerant Spike Routing},
  journal = {{IEEE} Transactions on Neural Networks and Learning Systems}
}

@article{yang2021cerebellumorphic,
  doi = {10.1109/tnnls.2021.3057070},
  url = {https://doi.org/10.1109/tnnls.2021.3057070},
  year = 2021,
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  pages = {1--15},
  author = {Shuangming Yang and Jiang Wang and Nan Zhang and Bin Deng and Yanwei Pang and Mostafa Rahimi Azghadi},
  title = {{CerebelluMorphic}: Large-Scale Neuromorphic Model and Architecture for Supervised Motor Learning},
  journal = {{IEEE} Transactions on Neural Networks and Learning Systems}
}

@article{yang2021bicoss,
  doi = {10.1109/tnnls.2020.3045492},
  url = {https://doi.org/10.1109/tnnls.2020.3045492},
  year = 2021,
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  pages = {1--15},
  author = {Shuangming Yang and Jiang Wang and Xinyu Hao and Huiyan Li and Xile Wei and Bin Deng and Kenneth A. Loparo},
  title = {{BiCoSS}: Toward Large-Scale Cognition Brain With Multigranular Neuromorphic Architecture},
  journal = {{IEEE} Transactions on Neural Networks and Learning Systems}
}

@article{yang2020scalable,
  doi = {10.1109/tnnls.2019.2899936},
  url = {https://doi.org/10.1109/tnnls.2019.2899936},
  year = 2020,
  month = jan,
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  volume = 31,
  number = 1,
  pages = {148--162},
  author = {Shuangming Yang and Bin Deng and Jiang Wang and Huiyan Li and Meili Lu and Yanqiu Che and Xile Wei and Kenneth A. Loparo},
  title = {Scalable Digital Neuromorphic Architecture for Large-Scale Biophysically Meaningful Neural Network With Multi-Compartment Neurons},
  journal = {{IEEE} Transactions on Neural Networks and Learning Systems}
}

@article{yang2018efficient,
  doi = {10.1016/j.physa.2017.11.155},
  url = {https://doi.org/10.1016/j.physa.2017.11.155},
  year = 2018,
  month = mar,
  publisher = {Elsevier {BV}},
  volume = 494,
  pages = {484--502},
  author = {Shuangming Yang and Xile Wei and Bin Deng and Chen Liu and Huiyan Li and Jiang Wang},
  title = {Efficient digital implementation of a conductance-based globus pallidus neuron and the dynamics analysis},
  journal = {Physica A: Statistical Mechanics and its Applications}
}

@article{yang2019realtime,
  doi = {10.1109/tcyb.2018.2823730},
  url = {https://doi.org/10.1109/tcyb.2018.2823730},
  year = 2019,
  month = jul,
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  volume = 49,
  number = 7,
  pages = {2490--2503},
  author = {Shuangming Yang and Jiang Wang and Bin Deng and Chen Liu and Huiyan Li and Chris Fietkiewicz and Kenneth A. Loparo},
  title = {Real-Time Neuromorphic System for Large-Scale Conductance-Based Spiking Neural Networks},
  journal = {{IEEE} Transactions on Cybernetics}
}

@InProceedings{indiveri2002computation,
  author = {Indiveri, Giacomo},
  editor = {Tagliaferri, Roberto and Marinaro, Maria},
  title = {Computation in Neuromorphic Analog {VLSI} Systems},
  booktitle = {Neural Nets WIRN Vietri-01},
  year = 2002,
  publisher = {Springer London},
  address = {London},
  pages = {3--20},
  isbn = {978-1-4471-0219-9}
}

@software{grant2017cereal,
  author={Grant, W. Shane and Voorhies, Randolph},
  year=2017,
  title={cereal - A {C}++11 library for serialization},
  url={http://uscilab.github.io/cereal}
}

@inproceedings{nussle2009fpga,
  title = {An {FPGA}-based custom high performance interconnection network},
  author = {N\"ussle, Mondrian and Geib, Benjamin and Fr\"oning, Holger and Br\"uning, Ulrich},
  booktitle = {2009 International Conference on Reconfigurable Computing and {FPGA}s},
  pages = {113--118},
  year = 2009,
  month = dec,
  doi = {10.1109/ReConFig.2009.23},
  organization = {IEEE}
}

@inproceedings{nussle2009rma,
  title = {A resource optimized remote-memory-access architecture for low-latency communication},
  author = {N\"ussle, Mondrian and Scherer, Martin and Br\"uning, Ulrich},
  booktitle = {2009 International Conference on Parallel Processing},
  pages = {220--227},
  year = 2009,
  month = sep,
  doi = {10.1109/ICPP.2009.62},
  organization = {IEEE}
}

@inproceedings{froning2013rates,
  title = {On achieving high message rates},
  author = {Fr\"oning, Holger and N\"ussle, Mondrian and Litz, Heiner and Leber, Christian and Br\"uning, Ulrich},
  doi = {10.1109/CCGrid.2013.43},
  isbn = {9780769549965},
  booktitle = {2013 13th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
  keywords = {Computer communications,High performance networking,Performance analysis,Performance prediction},
  pages = {498--505},
  year = 2013
}

@ARTICLE{li2021micro,
  title = {Micro-ring resonator based photonic reservoir computing for {PAM} equalization},
  author = {Li, Shi and Dev, Sourav and K{\"u}hl, Sebastian and Jamshidi, Kambiz and Pachnicke, Stephan},
  journal = {IEEE Photonics Technology Letters},
  volume = 33,
  number = 18,
  pages = {978--981},
  year = 2021,
  publisher = {IEEE},
  doi={10.1109/LPT.2021.3087323}
}

@inproceedings{litz2008velo,
  title = {{VELO}: A novel communication engine for ultra-low latency message transfers},
  author = {Litz, Heiner and Fr\"oning, Holger and N\"ussle, Mondrian and Br\"uning, Ulrich},
  doi = {10.1109/ICPP.2008.85},
  isbn = {9780769533742},
  issn = {01903918},
  booktitle = {2008 37th International Conference on Parallel Processing},
  keywords = {Device virtualization,Fine-grain communication,High-performance computing,Interconnection networks,Low-latency message passing},
  pages = {238--245},
  year = 2008,
  organisation = {IEEE}
}

@misc{rajkumar2008packet,
  title = {Packet aggregation for real time services on packet data networks},
  author = {Rajkumar, Ajay and Turner, Michael D},
  year = 2008,
  month = jun,
  publisher = {Google Patents},
  note = {US Patent 7,391,769}
}

@misc{cocke1991instruction,
  title = {Instruction control mechanism for a computing system with register renaming, map table and queues indicating available registers},
  author = {Cocke, John and Grohoski, Gregory F and Oklobdzija, Vojin G},
  year = 1991,
  month = feb,
  publisher = {Google Patents},
  note = {US Patent 4,992,938}
}

@inproceedings{thommes2021bssextoll,
  title = {{BrainScaleS} Large Scale Spike Communication using Extoll},
  author = {Tobias Thommes and Niels Buwen and Andreas Gr\"ubl and Eric M\"uller and Ulrich Br\"uning and Johannes Schemmel},
  booktitle = {2021 8th Neuro Inspired Computational Elements Workshop (NICE'2020)},
  year = 2021,
  eprint = {2111.15296},
  archivePrefix = {arXiv},
  primaryClass = {cs.AR},
  note = {peer-reviewed extended abstract incl.\ paper presentation}
}

@article{culurciello2003,
  title = {A Comparative Study of Access Topologies for Chip-Level Address-Event Communication Channels},
  author = {Culurciello, Eugenio and Andreou, Andreas G},
  year = 2003,
  month = sep,
  journal = {IEEE Transactions on Neural Networks},
  number = 5,
  volume = 14,
  doi = {10.1109/TNN.2003.816385}
}

@inproceedings{park2012,
  title = {Live demonstration: Hierarchical Address-Event Routing architecture for reconfigurable large scale neuromorphic systems},
  author = {Park, Jongkil and Yu, Theodore and Maier, Christoph and Joshi, Siddharth and Cauwenberghs, Gert},
  year = 2012,
  month = may,
  booktitle = {2012 IEEE International Symposium on Circuits and Systems (ISCAS)},
  pages = {707--711},
  publisher = {IEEE},
  isbn = {9781467302197},
  doi = {10.1109/ISCAS.2012.6272133},
}

@book{cormen2009algorithms,
  author = {Cormen, Thomas H. and Leiserson, Charles Eric and Rivest, Ronald Linn and Stein, Clifford},
  title = {Introduction to algorithms},
  edition = {Third edition},
  pages = {xix, 1292 pages},
  publisher = {MIT Press},
  address = {Cambridge, Massachusetts ; London, England},
  year = 2009,
  language = {eng},
  isbn = {978-0-262-03384-8 and 978-0-262-53305-8},
}

@book{privault2018markovChains,
  author = {Privault, Nicolas},
  title = {Understanding Markov Chains},
  edition = {2nd ed. 2018},
  series = {Springer Undergraduate Mathematics Series},
  pages = {XVII, 372 pages},
  publisher = {Springer Singapore},
  address = {Singapore},
  year = 2018,
  isbn = {978-981-13-0658-7},
  doi = {10.1007/978-981-13-0659-4},
}

@incollection{Peterson2022StPetersburgParadox,
  author = {Peterson, Martin},
  title = {The St. Petersburg Paradox},
  booktitle = {The Standford Encyclopedia of Philosophy},
  edition = {Summer 2022},
  year = 2022,
  editor = {Zalta, Edward N.},
  month = jun,
  publisher = {Metaphysics Research Lab, Stanford University},
  url = {https://plato.stanford.edu/archives/sum2022/entries/paradox-stpetersburg/},
}

@misc{Bischoff2022StPetersburgParadox,
  author = {Bischoff, Manon},
  title = {Sankt-Petersburg-Paradoxon: Spielen um jeden Preis?},
  booktitle = {Spektrum der Wissenschaft - Die Fabelhafte Welt der Mathematik},
  year = 2022,
  month = jun,
  url = {https://www.spektrum.de/kolumne/sankt-petersburg-paradoxon-spielen-um-jeden-preis/2024761},
  urldate = {2022-06-28},
}

@INPROCEEDINGS{vandesompele2016neuro,
	author={Vandesompele, Alexander and Walter, Florian and R{\"o}hrbein, Florian},
	booktitle={2016 {IEEE} Symposium Series on Computational Intelligence ({SSCI})},
	title={Neuro-evolution of spiking neural networks on {SpiNNaker} neuromorphic hardware},
	year=2016,
	pages={1--6},
	doi={10.1109/SSCI.2016.7850250}
}

@inproceedings{elias1992generic,
	title={A generic algorithm for training networks with artificial dendritic trees},
	author={Elias, John G and Chang, Ben},
	booktitle={[Proceedings 1992] {IJCNN} International Joint Conference on Neural Networks},
	volume=1,
	pages={652--657},
	year=1992,
	organization={IEEE}
}

@article{fatt1951analysis,
	title={An analysis of the end-plate potential recorded with an intra-cellular electrode},
	author={Fatt, Paul and Katz, Bernard},
	journal={The Journal of Physiology},
	volume=115,
	number=3,
	pages={320--370},
	doi={10.1113/jphysiol.1951.sp004675},
	year=1951,
	publisher={Wiley-Blackwell}
}

@article{fortin2012deap,
	author={F\'elix-Antoine Fortin and Fran\c{c}ois-Michel De Rainville and Marc-Andr\'e Gardner and Marc Parizeau and Christian Gagn\'e},
	title={{DEAP}: Evolutionary Algorithms Made Easy},
	pages={2171--2175},
	volume=13,
	month=jul,
	year=2012,
	journal={Journal of Machine Learning Research}
}

@article{henze1996dendritic,
	author={Henze, Darrell A. and Cameron, William E. and Barrionuevo, German},
	title={Dendritic morphology and its effects on the amplitude and rise-time of synaptic signals in hippocampal {CA3} pyramidal cells},
	journal={J. Comp. Neurol.},
	volume=369,
	number=3,
	pages={331--334},
	keywords={dendrites, compartmental model, afferent input, synaptic kinetics, passive neuron},
	doi={10.1002/(SICI)1096-9861(19960603)369:3<331::AID-CNE1>3.0.CO;2-6},
	abstract={Abstract Detailed anatomical analysis and compartmental modeling techniques were used to study the impact of CA3b pyramidal cell dendritic morphology and hippocampal anatomy on the amplitude and time course of dendritic synaptic signals. We have used computer-aided tracing methods to obtain accurate three-dimensional representations of 8 CA3b pyramidal cells. The average total dendritic length was 6,332 ± 1,029 μm and 5,062 ± 1,397 μm for the apical and basilar arbors, respectively. These cells also exhibited a rough symmetry in their maximal transverse and septotemporal extents (311 ± 84 μm and 269 ± 106 μm). From the calculated volume of influence (the volume of the neuropil from which the dendritic structures can receive input), it was found that these cells show a limited symmetry between their proximal apical and basilar dendrites (2.1 ± 1.2 × 106 μm3 and 3.5 ± 1.1 × 106 μm3, respectively). Based upon these data, we propose that the geometry of these cells can be approximated by a combination of two cones for the apical arbor and a single cone for the basilar arbor. The reconstructed cells were used to build compartmental models and investigate the extent to which the cellular anatomy determines the efficiency with which dendritic synaptic signals are transferred to the soma. We found that slow, long lasting signals show only approximately a 50\% attenuation when they occur in the most distal apical dendrites. However, synaptic transients similar to those seen in fast glutamatergic transmission are transferred much less efficiently, showing up to a 95\% attenuation. The relationship between the distance along the dendrites and the observed attenuation for a transient is described simply by single exponential functions with parameters of 195 and 147 μm for the apical and basilar arbors respectively. In contrast, there is no simple relation that describes how a transient is attenuated with respect to these cells' stratified inputs. This lack of a simple relationship arises from the radial orientation of the proximal apical and basilar dendrites. When combined, the anatomical and modeling data suggest that a CA3b cell can be approximated in three dimensions as the combination of three cones. The amplitude and time-course for a synaptic transient can then be predicted using two simple equations. © 1996 Wiley-Liss, Inc.},
	year=1996
}

@article {goncalves2020training,
	title={Training deep neural density estimators to identify mechanistic models of neural dynamics},
	author={Gon\c{c}alves, Pedro J and Lueckmann, Jan-Matthis and Deistler, Michael and Nonnenmacher, Marcel and O\"cal, Kaan and Bassetto, Giacomo and Chintaluri, Chaitanya and Podlaski, William F and Haddad, Sara A and Vogels, Tim P and Greenberg, David S and Macke, Jakob H},
	volume=9,
	year=2020,
	month={sep},
	doi={10.7554/eLife.56261},
	journal={eLife},
	issn={2050-084X},
	publisher={eLife Sciences Publications, Ltd},
}

@article{cranmer2020frontier,
	title={The frontier of simulation-based inference},
	author={Cranmer, Kyle and Brehmer, Johann and Louppe, Gilles},
	journal={Proceedings of the National Academy of Sciences},
	year=2020,
	volume=117,
	number=48,
	pages={30055--30062},
	doi={10.1073/pnas.1912789117},
	publisher={National Acad Sciences}
}

@InProceedings{greenberg2019automatic,
	title={Automatic Posterior Transformation for Likelihood-Free Inference},
	author={Greenberg, David and Nonnenmacher, Marcel and Macke, Jakob},
	booktitle={Proceedings of the 36th International Conference on Machine Learning},
	pages={2404--2414},
	year=2019,
	volume=97,
	publisher={PMLR}
}

@article{tejero2020sbi,
	title={sbi: A toolkit for simulation-based inference},
	author={Alvaro Tejero-Cantero and Jan Boelts and Michael Deistler and Jan-Matthis Lueckmann and Conor Durkan and Pedro J. Gon\c{c}alves and David S. Greenberg and Jakob H. Macke},
	journal={Journal of Open Source Software},
	year=2020,
	volume=5,
	number=52,
	pages=2505,
	doi={10.21105/joss.02505},
	publisher={The Open Journal}
}

@article{keren2005constraining,
	title={Constraining compartmental models using multiple voltage recordings and genetic algorithms},
	author={Keren, Naomi and Peled, Noam and Korngreen, Alon},
	journal={Journal of Neurophysiology},
	year=2005,
	publisher={American Physiological Society},
	doi={10.1152/jn.00408.2005}
}

@book{sisson2018handbook,
	title={Handbook of approximate Bayesian computation},
	author={Sisson, Scott A and Fan, Yanan and Beaumont, Mark},
	year=2018,
	publisher={CRC Press},
	isbn={978-1-4398-8150-7}
}

@book{wirsansky2020genetic,
	author={Wirsansky, Eyal},
	title={Hands-On Genetic Algorithms with Python},
	edition={1st edition},
	publisher={Packt Publishing Ltd.},
	year=2020,
	language={eng},
	isbn={978-1-83855-774-4},
	url={https://learning.oreilly.com/library/view/-/9781838557744/?ar}
}

@book{baeck2000evolutionary,
	author={B\"ack, Thomas and Fogel, David B and Michalewicz, Zbigniew},
	title={Evolutionary Computation 1: Basic Algorithms and Operators},
	volume=1,
	year=2000,
	month=dec,
	isbn={0-7503-0664-5},
	publisher={CRC press}
}

@article{vanier1999comparative,
	title={A Comparative Survey of Automated Parameter-Search Methods for Compartmental Neural Models},
	author={Vanier, Michael C and Bower, James M},
	journal={J Comput Neurosci},
	volume=7,
	number=2,
	pages={149--171},
	year=1999,
	publisher={Springer},
	doi={10.1023/a:1008972005316}
}

@article{papamakarios2021normalizing,
	author={Papamakarios, George and Nalisnick, Eric and Rezende, Danilo Jimenez and Mohamed, Shakir and Lakshminarayanan, Balaji},
	title={Normalizing Flows for Probabilistic Modeling and Inference},
	journal={Journal of Machine Learning Research},
	year=2021,
	volume=22,
	number=57,
	pages={1--64}
}

@inproceedings{lueckmann2021benchmarking,
	title={Benchmarking Simulation-Based Inference},
	author={Lueckmann, Jan-Matthis and Boelts, Jan and Greenberg, David and Goncalves, Pedro and Macke, Jakob},
	booktitle={Proceedings of the 24th International Conference on Artificial Intelligence and Statistics (AISTATS)},
	year=2021,
	volume=130,
	pages={343--351},
	publisher={PMLR},
}

@inproceedings{papamakarios2017masked,
	title={Masked Autoregressive Flow for Density Estimation},
	author={Papamakarios, George and Pavlakou, Theo and Murray, Iain},
	booktitle={Advances in Neural Information Processing Systems},
	year=2017,
	volume=30,
	isbn={9781510860964}
}

@inproceedings{lueckmann2017flexible,
	author={Lueckmann, Jan-Matthis and Goncalves, Pedro and Bassetto, Giacomo and {\"O}cal, Kaan and Nonnenmacher, Marcel and Macke, Jakob},
	booktitle={Advances in Neural Information Processing Systems},
	title={Flexible statistical inference for mechanistic models of neural dynamics},
	year=2017,
	volume=30,
	isbn={9781510860964}
}

@inproceedings{papamakarios2016fast,
	author={Papamakarios, George and Murray, Iain},
	title={Fast $\varepsilon$-Free Inference of Simulation Models with Bayesian Conditional Density Estimation},
	booktitle={Advances in Neural Information Processing Systems},
	year=2016,
	volume=29,
	isbn={9781510838819},
	publisher={Curran Associates Inc.},
	pages={1036--1044},
}

@article{billaudelle2022accurate,
	title={An accurate and flexible analog emulation of {AdEx} neuron dynamics in silicon},
	author={Billaudelle, Sebastian and Weis, Johannes and Dauer, Philipp and Schemmel, Johannes},
	journal={arXiv preprint},
	archivePrefix={arXiv},
	year=2022,
	eprint={2209.09280},
	primaryClass={cs.NE},
	doi={10.48550/arXiv.2209.09280}
}

@article{vangeit2008automated,
	title={Automated neuron model optimization techniques: a review},
	author={Van Geit, Werner and De Schutter, Erik and Achard, Pablo},
	journal={Biol. Cybern.},
	volume=99,
	number=4,
	pages={241--251},
	year=2008,
	publisher={Springer},
	doi={10.1007/s00422-008-0257-6}
}

@article{vangeit2016bluepyopt,
	title={{BluePyOpt}: Leveraging Open Source Software and Cloud Infrastructure to Optimise Model Parameters in Neuroscience},
	author={Van Geit, Werner and Gevaert, Michael and Chindemi, Giuseppe and R{\"o}ssert, Christian and Courcol, Jean-Denis and Muller, Eilif B and Sch{\"u}rmann, Felix and Segev, Idan and Markram, Henry},
	year=2016,
	journal={Front. Neuroinform.},
	volume=10,
	publisher={Frontiers Media SA},
	doi={10.3389/fninf.2016.00017}
}
